[
    {
        "speaker": "Amit Varma",
        "time": "00:00:00.000",
        "message": "The very fact that you're listening to my voice right now is a bloody miracle. When I was a kid growing up in pre-liberalization India, I would never have thought that I can speak into a mic in my room to someone thousands of miles away and be heard the next day by hundreds of thousands of people. What manner of wizardry is this? Almost every bit of technology that I use today would have seemed like magic to me in the 1980s. Hell, I remember in 1994 when I graduated from college and moved to Delhi to work in what was then India's largest advertising agency, there were just four computers in the entire floor where I worked, in a special computer room. And I remember marveling at the software I used once to type a script. Have you heard of WordStar? I remember moving to Bombay shortly after that, and yes, it was Bombay at that time and not Mumbai, and dreaming about how I would one day be rich and famous and have a laptop. It was so aspirational back in that time and so out of reach. And even the primitive garbage that was available then was way more expensive than laptops are today. I remember a friend telling me about how he had something called the internet in his office computer and you could type in anyone's name and it would show every web page in the world made on that person. I went to his office once, was allowed to see his computer and typed in the words Van Morrison. And my God, what a magical feeling it was to see the resultant web page when it did appear after about five minutes. Because hey, remember dial-up connections, young Indian men of my age will remember staring at the forehead of Samantha Fox as a dial-up connection tried to reconnect. You remember the sound that would make? But why am I talking about this now? It's because we normalize every technological advance as soon as it happens. And we have lost our sense of wonder. And so this is a good time to reconsider how far we have come in these years and how quickly our world is changing around us. Artificial intelligence and machine learning have already changed the way we live our lives and will continue to transform our post-COVID world. In fact, those of us who live to say 2040 will look back on 2020 and think, wow, we lived such primitive lives. We were like cave dwellers. I keep saying that the future is full of unknown unknowns, so we can't imagine what it will look like, but I'm often struck by curiosity. What kind of a world will my future self inhabit?"
    },
    {
        "speaker": "Unknown",
        "time": "00:02:29.200",
        "message": "Welcome to The Seen and the Unseen, our weekly podcast on economics, politics and behavioral science."
    },
    {
        "speaker": "Amit Varma",
        "time": "00:02:39.080",
        "message": "Please welcome your host, Amit Bharma. Welcome to The Seen and the Unseen. My guest today is Vasant Dhar, an artificial intelligence researcher who is a professor at the Stern School of Business and the Center for Data Science at New York University. But Vasant is no mere academic. He worked with Morgan Stanley in the 1990s, flirted with sports analytics, and even set up a hedge fund called SCT Capital, which used machine learning to make its trades in the 1990s. His understanding of AI is second to none. He's been both a practitioner and a researcher. In other words, he's had skin in the game. And indeed, it could be argued that we all have skin in the game now, because all of our lives will be transformed by technology. And it is to explore exactly this that Vasant has started an interview podcast called Brave New World. Vasant is a host of this new show, and we'll be having conversations with a variety of fascinating guests, such as Scott Galloway, Richard Thaler, Eric Topol and Srinan Aral. It's an initiative of the Data Governance Network, and I'm actually producing the show for them. But for the first time, I'm working on a show where I am not the host. So I can give you the inside scoop on Brave New World. You are going to hear some incredible conversations on it. So do subscribe to it on your podcast app of choice. And you can also visit its website at brave new podcast.com. And now for our conversation. But first, let's take a quick commercial break. One of the things I have worked on in recent years is getting my reading habit together. This involves making time to read books. But it also means reading long form articles and essays, there's a world of knowledge available through the internet. But the big question we all face is, how do I navigate this knowledge, who will be my guide to all the awesome writing out there? Well, a couple of friends of mine run this awesome company called CTQ Compounds at ctqcompounds.com, which aims to help people upgrade themselves constantly to stay relevant for the future. A few months ago, I signed up for one of their programs called the daily reader. Every day for six months, they sent me a long form article to read the subjects covered went from machine learning to mythology to mental models, we even marmalade. This helped me build a habit of reading. At the end of every day, I understood the world a little better than I had the previous day. Some of the viewers of the scene in the unseen often asked me, hey, how can I build my reading habit and upgrade my brain? I have an answer for you. Head on over to CTQ Compounds and check out the daily reader as well as the other activities that will help your future self be an upgrade on your present self. CTQ Compounds at ctqcompounds.com. You owe this to yourself."
    },
    {
        "speaker": "Guest",
        "time": "00:05:28.280",
        "message": "Wasn't Welcome to the scene in the unseen. I'm delighted to be here. And thank you for having me on your show."
    },
    {
        "speaker": "Amit Varma",
        "time": "00:05:36.120",
        "message": "No, no, it's a great pleasure for me, partly because I've also been, you know, as I announced in the introduction, helping you with your show Brave New World. And I'm so excited to listen to the future episodes of that, because it's just all such a fascinating field. But before we kind of go into the future, I want to go back into the past a little bit. And I want to talk about sort of your childhood, what was a young wasn't like, you know, in one of your talks on YouTube, you began with this beautiful picture of a bridge in Kashmir. And you wrote about how that's a bridge you went across when you went to school. So tell me about sort of the young wasn't in your years, growing up, and what did you"
    },
    {
        "speaker": "Guest",
        "time": "00:06:12.840",
        "message": "want to be when you grow up? You know, I have no idea. But you know, there's a Jerry Garcia line, you know, what a long, strange trip it's been. And that kind of characterizes my life, more or less, you know, when I just think of the changes I've seen in my lifetime, that bridge you saw in my TEDx talk was actually a bridge in Safa Qadal in Kashmir. And, you know, as you walk across that bridge, and when that bridge was under construction, you know, I used to take the boat, I was like four years old, with someone who used to take me to school, who's still alive, by the way, and I meet him in Kashmir every year. And you know, we reminisce about those old times, that was early 60s. So that's where I got started. And, you know, my father was in the army, which is why I was in Kashmir, a fair amount because he was posted to hill stations, or rather, posted to forward stations, they call them where families were not allowed. So my mom and I spent, you know, whenever he was away, we spent some time in Kashmir. And so that's where I started my childhood. But I, you know, but it was all over the place, because he was in the army, I was in Wellington, which is my earliest memories in life, where he was at the Defense Services Staff College. And so that was actually my earliest memory in life was in Wellington in South India, but we've always moving around. Yeah, and, you know, and then he got posted as a military attach\u00e9 to Ethiopia. And so we sailed from Bombay to Yemen, and, you know, then to Ethiopia and, and that was a real mind boggling experience for me. And I was, you know, not even nine, I spent three years there, you know, it was really formative years. You know, Haile Selassie was the emperor. And I used to get invited to his New Year's parties for diplomats kids. So, you know, three years, I went to meet Haile Selassie, you know, and I shook hands with him. One of those years, another year, he did a namaste, you know, one year he shook hands. And many, many years later, when I went to Jamaica, that really endeared me to the Rastafarians. Because I don't know if you know this, but they follow Haile Selassie. And his real name was Rastafari, which is where Rastafarian comes from, which most people are not aware of. Yeah, so it was that and then, you know, boarding school in India after that. And by the way, when I was in Ethiopia, my mom put me in the wrong grade, she put me into seventh grade instead of fourth grade, you know, I was I'd gone from third grade in India, and she said, Oh, you're going to standard four. And, you know, the kids there, you know, we'd gone to a party one day to the ambassador's house and they told me, Oh, you know, standard four here means seventh grade, you know, tell your mom that, you know, in the British schools, you know, you want to be in class two, you know, so on the way back home, I told my mom, I said, By the way, you know, it's not standard four, that's seventh grade, you know, and, you know, she just sort of in Kashmiri said, you know, which means, you know, shut up, you little pipsqueak, you know, you think you know everything. And the next day, I went and took the test for seventh grade, you know, and I did really poorly. And the headmaster called my dad, you know, he was a very understated British guy and said, you know, you show you know what you're doing, you know, and what he meant was, you know, your son's like nine years old, why are you putting him in seventh grade? And my dad completely missed it, right? Because, you know, in his, the headmaster in his typically understated British way said, you know, I think you're being a little ambitious, actually. And my dad just missed that. And so first day I walked into class, there were people who were like 15, you know, and, you know, whistling at an attractive 19 year old English teacher, and I told my parents about this. And instead of being worried, they were amused. They didn't actually realize it until, you know, I went to the airport with my dad to collect my brother who was coming home from boarding school. And there was this woman who walks by in the short skirts and heels. And I walked up to her and I say, Hey, Fatima, how are you doing? And she's like, Vasant, and we chatted and my dad said, Who is that woman? And I said, Well, she's in my class. He says, What? I said, Well, what do you think I've been telling you all this time? So, you know, those incidents have not been atypical. So that that was my early childhood. Then I went to boarding school in India, and then IIT and, you know, and then graduate school after that in Pittsburgh. So you know, which is where I got into AI. So that, in a nutshell, has been sort of my journey. So not a typical childhood, very, very unusual, but"
    },
    {
        "speaker": "Amit Varma",
        "time": "00:10:45.200",
        "message": "And kind of staying with your childhood, because now I'm curious that, did you years spent outside the country, you know, hobnobbing with the likes of Haile Selassie, did it mark you out from the other kids when you came back, like you went to boarding school at Lawrence School Sanhavar and all that fairly elite place at the time, but do you feel that you were a slightly different person, one, because, you know, that you had this experience of the world, which was beyond them, because those were not the kind of connected times that they are, where you have easy experience to other cultures and all of that. Did that kind of make you feel that there was one layer to you, which nobody else had"
    },
    {
        "speaker": "Guest",
        "time": "00:11:20.800",
        "message": "in some ways? Yes, I think there was, I'm not sure, quite sure how to describe it, right? It's hard to describe these experiences that I had, both in terms of, you know, just like a completely different culture. And I did, you know, when you're young, you sort of immerse yourself in it so much easier. And so, you know, when my parents went off to, you know, diplomat parties, I'd be hanging out with the houseboy and the maid, and we'd talk in Amharic, right? So I learned Amharic, the language, and then in school, I was learning French. Yeah, but it was just a very unique experience. So the answer is, yeah, it probably did change me in some ways. Hard to describe how. And then, of course, boarding school was an entirely different trip already, you know, to come back in India, you know, to go from third grade to seventh grade to eighth grade back to sixth grade in boarding school. And finally be kind of, you know, somewhat, you know, have some sort of stability for the last six or seven years of my school. Before that, you know, I was in a new school every year because my father would get posted to, you know, Ambala or something, and I'd, you know, go to school there. So it was just, you know, constant change in my childhood, a lot of change, but very interesting because it just gave me this also a wanderlust, I guess, because, you know, when I was in IIT, you know, me and a friend hitchhiked from Delhi to London, you know, took three months. And now, you know, I last year, I just met someone at the London Business School, a faculty member who had actually done it exactly the same time, the other direction, you know, London to Delhi. And that was a time there was a lot of movement, a lot of hippies were going back and forth and these VW vans. So it was a it was a great time. But it gave me sort of a sense of, you know, wanting to explore, which I've continued to do since then. I just love to travel and be in other cultures."
    },
    {
        "speaker": "Amit Varma",
        "time": "00:13:16.240",
        "message": "It's very refreshing. And I noticed when you went to IIT Bombay, you know, you passed out from IIT in 78. And you did a BTech in chemical engineering, which doesn't seem too connected with everything you've done since. So, you know, were you just sort of following the standard routes that one does school and then okay, IIT is the thing to do. So one goes for engineering and all of that, or did you have specific ideas of your own at that point in time about the things that you wanted to do? And was there anything in your interests or your sort of hobbies at that time that presage the future direction of your life as it were?"
    },
    {
        "speaker": "Guest",
        "time": "00:13:50.640",
        "message": "No, not at all. I was just trying to create good alternatives for myself, you know, and my parents being typical Indian parents of that era said, you know, do something where you can get a job, you know, that you can support yourself and you're good at math and stuff. So, you know, engineering seems like the right thing. So, you know, so I took, you know, Sangal's classes in South Extension. So I went to IIT Delhi, by the way, you know, I took his classes and, you know, and we were just so much behind the kids from the Delhi schools, you know, like Columbus and Xaviers, you know, we guys have gone to boarding school. We played lots of sports, you know, and, you know, my very first day in class in Sangal, you know, I was just completely lost, you know, he'd say, you know, acha, sign 4A kya hai, you know, and like five guys would like raise their hands and say, you know, whatever. And I'd be like trying to figure out like, how do I break this, you know, sign 4A and solve it. So, yeah, no, there was it was nothing it was just go for the, you know, alternatives in front of you, and, you know, I took the IIT exam, which, as you know, is brutal and grueling, you know, stumbled my way into IIT, you know, and then applied to graduate school. And then Pittsburgh is really sort of where I found myself in the graduate program. That's when I learned about artificial intelligence and what it was. And I'll never forget my very first exposure to AI, you know, that will remain with me. I just joined a PhD program there, and a bunch of students asked me, they said, Hey, you know, we want to ask this professor to offer a course in artificial intelligence, but we just want to go in fourth, so he knows there's enough of us. So I just said, What is AI? And they said, Oh, it's when machines get intelligent. And I was like, Okay, whatever. And we went up to this lab in the medical school, 13th floor, top floor, and there was this special decision systems lab. And there was this professor, Harry Popel, who had built the first diagnostic system for the entire field of internal medicine, I had no idea what that was. And I had no idea that I'd be spending the next five years of my life in that lab, you know, pretty much full time. But, you know, we went there and asked him. And he said, Oh, yeah, I'd be delighted, you know, asked us about our backgrounds. And, you know, and but I remember I was standing while we were waiting for him, he was on the call, there was this gigantic screen in the middle of the room, connected to a computer at Stanford. And there was this physician, like, completely white hair puffing a pipe. And he was, quote, unquote, talking to the monitor in front of him through his assistant who was typing because people those days couldn't type. And the system was asking him questions about a case. He was trying to solve a case. And so the system asked him a few questions, he gave the answers, it asked him about blood, urine, all that kind of stuff. And it was engaging in a dialogue and then asked him a question like, you know, was there a pain in the left lower abdomen and, you know, he said, puff, puff, why, like, why you asked me the question. And on the screen, you know, the computer said, well, because the evidence you've given me so far is consistent with the following hypotheses. And this question will help me discriminate between the top two. And I was just looking at this thing like, holy smoke, like, how is a computer doing this? Like, how the hell is a computer behaving in this way and asking him, you know, the right questions? You know, and that was my first exposure. And you know, I think one of the things we'll probably come back to at some point during this talk is where are we now, relative to where we were then, right? And I'm talking like 1979, where I was watching this dialogue happen, and the system hardly ever made any mistakes. It would actually diagnose virtually every case correctly, right? And so that was my first exposure to AI. And I said, wow, like, I don't know what this hell this thing is, but this is what I want to do for the rest of my life, you know? So that's what I mean by sort of found what I really wanted to do in a purpose in life there, because of him and another gentleman called Herb Simon, who was a Nobel laureate in economics and one of the fathers of AI. So these are my two mentors in grad school, and they gave me a hell of a training and just sort of taught me how to think, which I didn't really know how to do before that. I didn't really know how to think, I didn't really know how to ask a question and answer it, right? I mean, even though I'd gone to IIT and done engineering and math and all that kind of stuff, I just didn't know how to think, you know, I mean, I could solve problems, and I was good analytically. So I had some good tools in my back pocket, but just learning how to think about problems and ask the question and just learning what inquiry is really all about scientific inquiry is really all about and how to conduct it was what they exposed me to. And that's been sort of the rest of my career. So you know, when I started with that Jerry Garcia quote, it's, you know, you can see why it's sort of been, you know, a long, strange trip indeed."
    },
    {
        "speaker": "Amit Varma",
        "time": "00:18:51.640",
        "message": "Yeah, you know, and I'm so fascinated and delighted by that story of that moment of magic, when you see the computer doing what it does. And it just strikes me that in our own lives, and this will especially be true for young people today, that we are so jaded, and we take the tech around us for so much for granted that we probably don't have those moments of magic, those aha moments when you realize what the hell like I remember, you know, just speaking of myself, one sort of moment I had like that was actually quite recent when it reminded me also of how jaded I'd become, which was, you know, I'm a bit of a chess enthusiast. And when AlphaZero came out, and I remember looking at AlphaZero's games, and, you know, and of course, computers in chess, of course, have been around for the longest time, Stockfish and all have been used for pedagogy by the top chess players forever, and have shaped the world of chess in very interesting ways. But AlphaZero was just such a step ahead. And you know, I thought of that just now, when you spoke about how you started learning how to think from what you had seen and from what AI has done, because you know, for example, you know, Magnus Carlsen's, the chess world champion Magnus Carlsen's coach Peter Hein Nelson once said that, you know, I always used to wonder what it would be like to see aliens with a super intelligence, play chess. And when I saw the AlphaZero games, I knew, and, you know, but we'll kind of come back to that later. But I want to probe a little further into what you meant when you said that you learned how to think, you know, can you elaborate a little bit on that? What do you mean by learning how to think?"
    },
    {
        "speaker": "Guest",
        "time": "00:20:22.280",
        "message": "Great question. And let me see if I can do it justice, because it's one of those things where you kind of know what it is, but putting it into words, it's a little challenging. But it means several things. First, it means developing an ability to ask a good question. You know, I mean, I've been teaching for 30 plus years. And very often, you know, in class, you know, someone asks a question, I say, wow, that's a great question, right? And, you know, like, what makes for a great question, right? It's a bit of an art there, you know, but you learn to recognize it when you see it. Like, I mean, some questions are just better than others, right? And you know, maybe if I have some more time to think about it, I'll, you know, I may be able to come up with actually a model of what makes a good question. But the ability to think is firstly about how to ask the right question. And then once you've asked the right question, a good question, the ability to then say, well, is this answerable, right? Can I actually answer this question? If I had all the resources, I had all the data can actually answer this question, right? And what sorts of answers do I have any expectations about what I should observe, right? What do we know at the moment about this question, right? So if I'm asking a question, like, what do we know about it? You know, so just to go off at a bit of a tangent here, right, someone called me yesterday was interested in applying to the PhD program in data science at NYU, which I had. And so he wanted 20 minutes of my time. So I said, you know, I said, call me. And I said, so what's going on in your head? He says, you know, I'm graduating, I've become fascinated by reinforcement learning because of what I saw in AlphaGo, you know, which used reinforcement learning. So I'm really interested in reinforcement learning, because I think there's some real magic to that method. And, you know, so that's what I want to explore in my PhD program. And my question to him was, well, why do you think it worked so well in solving AlphaGo as opposed to other methods, right? Do you think other methods could have solved AlphaGo as easily, or was it something unique about reinforcement learning? And he says, gee, I don't know. That's an interesting question. And I said, but, you know, okay, and what do you think makes reinforcement learning work in game playing programs, right? Do you think it would work across the board, right? So I said to him, I said, you know, I view data science as a canvas where rows are methods and columns are domains, and the interesting questions arise at the intersections of these domains and methods. Reinforcement learning is a method, so look at it as a row. And now game playing is a column, right? Because game playing is a domain, you know, just like finance or healthcare or physics or law or, you know, whatever you call it, those are all domains, right? So I said, so do you think reinforcement learning would work across all of these domains equally well? What do you think about game playing that makes it work really well, right? So when you asked me earlier, like, you know, what does it mean to learn how to think? That's what I'm getting at, right? Where are we right now? What's the baseline? What do we know about something, right? And why is this a good question? Why will this take us forward significantly, right? Which makes it a good question. And then can you actually solve the problem? Do you have the data to solve it? And then do you have the chops, right? Do you know the tools and do you have any idea as to how long it's going to take you to solve it, right? So one of the things, you know, I sort of, I'm a pracademic, some people call me, you know, I'm an academic, but I'm also a practical and I, you know, created, you know, an automated hedge fund, you know, many years ago. And so when I think of research there, my question always is like, how long will this take us? You know, will it take a day, two days, a week, a month, or a year? If it's going to take a year, I'm not interested. It's too uncertain. But if it's going to take a day, I'm very interested. If it's going to take a week, well, I need to think about it, right? So it's all of those things that go into being a good researcher. So from A to Z, asking the question to execution."
    },
    {
        "speaker": "Amit Varma",
        "time": "00:24:42.040",
        "message": "So I'm just thinking aloud again, zooming into sort of the element of asking a good question, which is a great answer, by the way, would it be one quality of a good question that you minimize the assumptions, because it strikes me that humans are full of these inherent biases and their assumptions about the way the world works. And sometimes, if those are embedded within the question that you ask, that can make the question less effective. Like, again, going back to chess, one of the things that, you know, programs like Stockfish and all would have embedded in them is the earlier heuristics of how you evaluate a chess position where, you know, material has a certain importance, space has a certain importance and so on. Whereas with AlphaZero, one of the things we realized was that chess players across human history have been undervaluing the importance of initiative with regard to material. And that impression would have persisted because if you ask the questions with the wrong assumptions, then you don't really get those great answers. So again, I'm just kind of thinking aloud that would the good questions, therefore, be willing to take that further step back and, you know, be open to sort of learning that your assumptions earlier were false as well."
    },
    {
        "speaker": "Guest",
        "time": "00:25:57.720",
        "message": "That's a great way to look at it in terms of like, how many assumptions are you making in asking the question, right? There's another way to look at it, which is, can I control for a bunch of things, assumptions, for example, can I control for them when I answer the question, right? That's the other way to think about it, right? So, you know, just thinking aloud here, if I were to ask a question like, has India been adversely affected by COVID because of its administrative systems? Yeah, I'm just making this up, right? So this is a very broad question. And now, you know, you might say, well, you know, like, what kinds of assumptions will you have to make in answering the question, right? And you're right. If I have to make all kinds of assumptions about the problem, the data and all that kind of stuff, then by the time I answer the question, the answer may not have any teeth, right? It may not be relevant. But if I can ask like a really nice, tight question, you know, that involves few assumptions and to the extent that it involves assumptions, I can control for them, then that's okay. So I guess to sort of summarize my answer, I'd say, yeah, in general, you want to make sure that you're not making too many assumptions, especially the wrong ones. And to the extent you are, can you control for them? When you answer the question."
    },
    {
        "speaker": "Amit Varma",
        "time": "00:27:19.320",
        "message": "And, you know, one of the fascinating quotes that I found of yours, so I'm not going to quote you back to yourself, which really made me sit back and think is when you said quote, when you have a data driven approach to life, you find that patterns often emerge before reasons to stop quote, which I found sort of eye opening to me in various contexts, not just chess, which is what I immediately thought of, you know, and that's something as you pointed out came from your sort of journey examining big data that I think you mentioned started in 1990, when Nielsen approached you to look at some data and you found some unusual patterns, which you could not explain at the moment, but they were clearly patterns and they were clearly significant. But, you know, before we go there, you know, take me through your journey, then through the 80s, where you spent five years in this lab, and, you know, what kind of work are you doing? What are the kind of problems which now interest you? Because I'm guessing that it is at around this point from the way you seem to describe it, that you have this intellectual focus that, you know, these are the kind of problems you want to solve. So tell me a little bit about that journey from that point on."
    },
    {
        "speaker": "Guest",
        "time": "00:28:23.960",
        "message": "So I'm really glad you asked the question, because it actually helps to clarify how AI itself has changed in the last 40 years, right? So in the 80s, or rather, I should say until the 90s, AI was largely about reasoning and, you know, systems that could reason from data, make inferences. And I'd say that the language of AI was mostly logic, right? Because with logic, you're on firm ground, right? If A implies B, and you're seeing A, well, then B must be true as well, right? So a lot of AI had sort of hitched itself up to logic, right? And so logicians were in control of the field at that time. And then people said, you know, logic isn't enough, right? In fact, even my thesis advisor, right, he said, you know, deductive logic doesn't do it. And he came up with, you know, he picked up on something that a philosopher called Charles Sanders Peirce had come up with called abductive logic, which is that if A implies B and you observe B, it doesn't mean that A is necessarily true, but it could be, right? It's like an induction now, right? So now you're sort of observing things and invoking hypotheses or reasons for them, right? So AI, I'd say until the 90s, and this is sort of when I got involved in AI, it was all about logic. It was all about representation, reasoning, inference, right? That was the thing. What happened around the 90s is that there was a sea change because data started becoming available, right? And this is, in my own career, that was becoming apparent that, wow, you know, like we can actually now look towards data. And at that time, it was mostly telephone companies and banks that had the data, you know, other than defense and physics, like, you know, physicists have always had tons of data, but other than them, which was kind of a world in itself, it was mostly telecom companies and banks and companies like Nielsen, right, who were in the information business. And so I saw this sort of as a new paradigm for AI that was emerging that would probably eclipse the previous one. And in fact, that's exactly what happened, right? So in the early 90s, I sort of shifted my focus towards data and learning from data and got this project with AC Nielsen, household services company, and they were tracking 50,000 households all over the US. You know, anytime they shop, they scanned the item and went to a database and they shared this data with me and said, you know, see if you can find something interesting in this, you know, like we don't quite know what, but, you know, look at this data. So I did. And, you know, I cracked it through these algorithms I was working on at the time, you know, things called genetic algorithms that would look at data and find, extract rules from it. So I went to this meeting and they said, so what'd you find? I said, you know, I found something, but I have no idea what it means. This is all right. Let's take it from the top. And I said, it looks like older women in the Northeast do a lot of their shopping on Thursdays. And he said, Oh yeah, that's coupon day. What else did you find? And I was just ecstatic, you know, that I found something really interesting that made sense that I hadn't really told the machine to find. I just told it to look for unusual shopping activity. And it said, you know, older women on Thursdays, you know, and there were many other patterns like that in the data that they were completely unaware of, you know, the manager of the Neil said, Oh really? Wow. Wow. We didn't know that. Right. And so that was like, that's interesting that these patterns that are emerging and the reasons they're telling me in retrospect, like, you know, that, you know, some of them were easily explained like, yeah, that's coupon day, but others that weren't right. And fast forward four years, right. I was now in wall street, I'd taken some years off from academia and I was with a trading group and I told them, I said, you know, just give me all your data and I'll tell you if you could have done better. And they said, well, you don't need to know anything about what we do. I said, no, just give me your trades. So they did. And again, you know, hocus pocus, I cranked them through my genetic algorithm. I come back to our weekly meeting next week on Fridays. And so the head of the group said, so Vasant, what'd you find? I said, I found something, but I have no idea what it means. And he says, so what'd you find? I said, well, you know, when the 30 day volatility is in the lowest quartile, your trades are three times as profitable as they are otherwise the silence around the room for five seconds. And the head researcher says, yeah, I looked at volatility. And the head of the group says, Frank, you know, shut the fuck up. How long have I been telling you to look at volatility? And this guy who knows nothing about what we do tells us it matters. And then there was like, you know, shouting and people sort of accusing each other of, you know, being dumb and, you know, missing the picture. And I was just watching this thing and saying, can someone tell me what's going on here? And they said, no, but we've observed whenever volatility spikes, we lose a lot of money. So it's interesting that you're telling us this, knowing nothing about what we do. And it was six months later that I realized why I was observing what I was observing, you know, because I went into the literature of finance and, you know, and, and I found the reasons for what I'd found. And this has happened to me pretty much in every domain I've looked at, you know, I mean, I, you know, I worked with the San Antonio Spurs for a couple of seasons looking at their data, you know, sports, basketball, completely different context, again, the same thing, right? But the things that pop up, you know, for example, you know, having to do with, you know, the performance of a team, whether they're playing at home versus away and, you know, why do they perform worse when they're away, you know, just things like that, that was just in the data that you could extract and then say, wow, like, I had no idea that this even existed. And then finding the reasons for why those patterns exist is often a very enlightening exercise, you know, reveals a lot of things about the domain that you may not have been aware of. And to me, this is one of the promises of data science is that it can nudge you towards things that you might not even have thought of, right? I mean, you talked about AlphaGo and, you know, people have said, wow, it came up with these moves that we never would have thought about, right? Machines look at the world differently from humans do, you know, and they often reveal things that surprise us in retrospect. And that's where that quote came from, which is that patterns often emerge before the reasons for them become apparent."
    },
    {
        "speaker": "Amit Varma",
        "time": "00:35:00.680",
        "message": "Yeah, this sort of reminds me of and this is not apropos of either AI or whatever, but I thought of the phrase, you know, when you spoke about looking for reasons after the patterns emerge, and I thought of the phrase the interpreter, which is a phrase that Michael Gazzaniga used when he did his famous experiments on patients of split brain epilepsy, I guess you've heard about that, right?"
    },
    {
        "speaker": "Guest",
        "time": "00:35:19.280",
        "message": "I have not, I'm sorry to say, but it sounds really fascinating."
    },
    {
        "speaker": "Amit Varma",
        "time": "00:35:22.400",
        "message": "Yeah, yeah, no, no. I'll go through it for the benefit of my listeners. So this really happened in, I think, the 50s or the 60s. And Gazzaniga recently, I think, wrote a book called human or something where he talks about it. But this is like a seminal experiment mentioned in various books, where one of the ways of solving or, you know, helping to mitigate split brain epilepsy was that you cut that part of the corpus callosum, I think, which connects the right brain and the left brain. So when you sever that, they can't communicate to each other. So he tried this experiment with patients on whom this operation had been done, where you basically then divide their field of vision. So the right brain, of course, controls the left eye or whatever. But essentially, you divide the field of vision, the two halves of the brain aren't talking to each other. And you show you know, what the part of the field of vision that the right brain can perceive, you show it something like, you know, ask for a glass of water, or say this, or go to the loo or whatever. And the person would do that he would follow the instruction. And then he would be asked, why did you do that? And he would have no clue. So the left part of his brain would make up an explanation, which was completely unrelated. And it would actually believe that. And he called, therefore, this function in the brain, the interpreter, and reading about this was a sort of a aha moment for me, because then I realized that, you know, many of the things that we do in our lives, we think we do them for reasons, but we could be making them up post facto, we could be rationalizing them along the way. And our actual actions could be just caused by a variety of instinctual things, which"
    },
    {
        "speaker": "Guest",
        "time": "00:36:53.080",
        "message": "we are not aware of at a conscious level. Absolutely. Yes, yes. And I mean, I'm sorry to interrupt, because there's so much I want to say on that, right? Because there's at least there's at least two strands to what you've said. One is, you may be convincing yourself that you found the reasons and fooling yourself, which is absolutely right. You know, and my sort of hypothesis is that in areas where the domain is much more well defined, that's an easier problem, right? So when a problem has a high degree of predictability, it has a good theory, you can actually have more confidence that you've actually interpreted the pattern correctly or for the right reasons, right? So that's a fascinating line of thinking. And you're absolutely right that even though you're trying to now explain the reasons, it's not always the case that you'll be successful or find the right reason for that, right? So the other strand, and I don't know if you've read this book called My Stroke of Insight. No, no. Oh, you will find that fascinating. It's also a TED talk. That book is, you know, what I was saying is, is more interesting than the talk, because it really gets into the story about this individual who's a brain scientist. And she has a stroke where her left brain is impaired. And so she's aware that her left brain is impaired. And so it's this whole story about right brain, left brain function, right? She said she felt a sense of bliss because the right brain apparently is a parallel processor that just sort of takes in the information as it's coming, just streaming in blissfully. And the left brain is the logical one, you know, logic, time, reasoning, you know, all of that stuff, and how these, you know, two halves communicate. So since you mentioned this experiment, I think you'll find this book really fascinating, you know, both from sort of a scientific view and also a humanistic view, right? Because she describes, you know, her many months in the hospital, you know, and just, you know, and she describes this feeling when her mother came to visit her, she could not recognize her mother. She had no idea who this person was, but she felt a sense of warmth, you know, there was something really nice, you know, that was, you know, exuding from that person in that aura, right? So she had the intuition, which was this right brain kind of thing, but no logic. And it's a story about how she actually then learned to sort of bootstrap her left hemisphere, you know, and get things back to, you know, as normal as they could be. But I think you'll find that book really fascinating, and it's a great TED Talk as well, my stroke"
    },
    {
        "speaker": "Amit Varma",
        "time": "00:39:34.880",
        "message": "of insight. That's so fascinating. I'm absolutely going to look it up. There's another larger question I saved for, you know, the last part of the show. But since we're on the subject, I think I might as well ask it now, which is, I mean, a couple of related questions. And one question is that, you know, the more one reads about AI, one realizes that, you know, our brains are also machines. And in some ways, they are magnificent machines. And in some ways, they're very flawed machines, and, you know, so on and so forth. So when we learn about AI, it seems to me that there are three possible phases that we can go through with regard to learning about AI and what AI can do with data. And many people possibly don't reach the third one. The first is, of course, excitement, that moment of magic, when you realize what it can do, which, you know, you described what happened to you at the lab, and so on and so forth. Or the first time someone, you know, from a village may see a GPS on a smartphone. And that can also be a moment of enormous magic, because my God, I mean, it is magical technology, if you think about it, you know, in the 1960s. And the next phase, I'm guessing would be and that I see in a lot of policy makers, for example, is an arrogance, because data gives them so much power, you know, having all these tools at their disposal, gives them so much power. And already as it is, you know, humans and government often suffer from what Frederick Hayek calls the fatal conceit. So you know, when you have so much these great tools at your disposal, does that then accentuate the arrogance, where you think that you can, you know, achieve ends, where you think you are superhuman, in a sense, with the help of these, whether it's in reordering society or in just gaming markets, or whatever it may be. And the third phase, I would postulate, and I'm just completely thinking aloud, and I'm sure much greater thinkers than me have gone through all of this. But you know, after excitement and arrogance, I'm thinking the third phase could well be humility, where you realize your own inherent limitations. And that sort of that veil of arrogance that makes you believe that you're something special in the universe, that kind of falls apart, where you realize that you are actually in so many ways, besides being mortal, of course, you are in so many ways, hopelessly inadequate, even in this one thing that we assume that we can do better than other animals."
    },
    {
        "speaker": "Guest",
        "time": "00:41:44.480",
        "message": "There's a whole bunch of stuff in what you've just said. So let me take it at two levels. One is just the capabilities of AI. And the second part, which you were getting at, which is, you know, what you call arrogance, but it's sort of this unbridled use of data and the power that you get from doing it. And what that says about how we organize society, because that's probably one of the biggest questions of our time right now is, you know, the internet was supposed to be free, right? It was supposed to be free, empowering, all that kind of stuff. I don't know how you feel, but to me, the internet of today doesn't feel particularly empowering in some ways, but very much so in others. It's become a much more complicated space than it used to be, you know, 25 years ago, when it was sort of in its relative infancy. And we're seeing different countries adopt different models of governance, you know, of data and the internet. And, you know, I've described four of these, you know, being the US, Europe, India, and China. And we can come back to that in a little bit. But that's one of the big questions of our time is, you know, how will we govern AI? You know, how will we govern these platforms, which have become AI platforms, essentially, because they're like very data intensive, very automated. So that's one of the big questions. But there's another thing you talked about, which is this sort of excitement and arrogance. So, you know, I want to say one thing, which is that as excited as I am about AI and the progress we've made, we're still in the Bronze Age, right? These are like very early stages of AI, right? Machines have gotten incredibly good, but that's relative to what they were, which is incredibly stupid, right? So they were stupid, they were dumb, right? Now they're pretty amazing relative to our expectations, right? And you talked about, you know, people have become sort of blase about all these, you know, advances. And it's absolutely true. Like my students, right? I mean, I remember the first time, you know, Watson became the Jeopardy champion, right? And I asked people, I said, so what do you think of this? Like, isn't that amazing? And they said, yeah, it's pretty good. And I was like, like, what do you mean, it's pretty good, like, you know, I said, you know, expectations are just crazy, right? Because to me, my expectations were, I guess, so low in retrospect, right? The very first time I saw a search engine, and able to type anything in natural language, I was amazed that it came up with anything remotely useful, because in my experiences with natural language, you know, 20 years ago, it had been that it was just terrible. Like machines had no way, they just look at keywords and try and do their best and give you something and throw up their hands, right? They were pretty stupid at the time, right? But I was amazed that it would actually give you something useful, right? Now the fact that you can talk to Alexa or Siri or whatever, it just like boggles my mind, right? That I never imagined I'd see this in my lifetime, you know, that we have systems that we can actually talk to, and that seem to understand quote, unquote, a fair amount of what we're saying, at least understand it at sufficient depth to provide a useful answer, right? So that's pretty amazing, right? So we've come a long ways, right? But I want to come back to this, which is we're still in the Bronze Age, we've got a long way to go here, you know, so I talked about, you know, that medical diagnostic system that I first came across in the 79, that I thought was amazing, right? It was engaging in a dialogue. Now, the difference is that you can actually feed the system an X-ray image, you can feed it all of this data that you're seeing as a physician, which you couldn't earlier on, right? You have to look at the image, you have to see it, and then enter it into the system, and then the system will quote, unquote, reason with it, right? Whereas now we've gotten to the point where machines have become good at seeing things, and so that gets tossed into the mix as well. But are we at that point yet where machines form at, you know, function at an equivalent level that human physicians do? For the most part, not really, right? Humans still hold that edge because of our intelligence, and humans and machines see the world very differently, by the way, and we can come back to this a little bit later, but we're not at that stage yet, because machines still make lots of mistakes, and this is something that I've talked about a lot in my research as well, about, you know, when do we trust AI systems, and my simple theory is we trust them when, you know, they don't make too many mistakes and those mistakes don't have very severe consequences, right? You know, to put it simply, and at the moment, we're just grappling with trying to understand this in more detail, like what kinds of mistakes will driverless cars make, right? Will they, you know, plow over kids routinely, you know, will they slam into poles or whatever, right? You know, someone told me, a chief scientist from one of these driverless car efforts told me that if you drive in driverless mode these days, you'll probably die four or five times in a year, right? So, we're not good enough yet in terms of driverless cars, but the larger question really is in all of these domains where we apply AI, you know, whether it is medicine, healthcare, or whether it's payments or, you know, whatever you can think of, the question that we always have to ask ourselves is, you know, what can go wrong, and how do you really avoid catastrophes from happening, right? How do we really bridle the power of AI without really exposing ourselves to these huge risks that these machines can subject ourselves to? So, even when you talk about, you know, you mentioned data and, you know, I don't know whether you mentioned Big Brother or observing society and having access to all these things, you know, the question is like, what can go wrong, and are you willing to deal with the consequences? It reminds me of a talk by James Robinson several years ago, you know, he wrote this book called Why Nations Fail, and it's all about the importance of institutions and checks and balances and that, you know, societies that have more advanced institutions generally tend to do better as opposed to the ones that don't, and I remember after his talk, and this was in Goa, he and I went out into town and I asked him, I said, you know, are your assumptions still valid in the age of AI and data? Because what I see China doing blows my mind, right? Maybe the assumptions you're making are assumptions of an old era where information flowed like molasses and people weren't informed, and so, you know, decentralization was better and let people close to the phenomenon make the decisions, right? And let's have strong institutions with checks and balances, and my question to him was, are we in a different era now where because there's so much data, there's so much information that we can actually have very effective centralized control, and I believe that the Chinese government believes that, right? I think they actually believe that their leadership has the wisdom that given the power they will wield it in a responsible way, right? It remains to be seen what, you know, how things will look 20 years from now. It'll be a really fascinating, you know, experiment, this one, as to, you know, where China is 10 or 20 years from now with its very centralized model of data gathering and control and decision making versus, you know, these democratic societies that are just much more freewheeling and messy, right? Now, we've seen that with Corona, the centralized model has actually worked better, right? South Korea is my favorite example where, you know, they have two to four deaths a day, right? That's amazing statistic, right? That's because there's a congruence between what the government is trying to do and what individuals want, right? They all want to keep deaths down and infection rates low, and so individuals are willing to trust the government with this one, right? But it's a really messy kind of a question, right? It doesn't have an easy answer as to like, where do you really draw the line between people in control saying, yeah, you know, we're responsible, trust us. If we just had the right data, we do the right thing, which South Korea has done and demonstrated. So it shows that it is possible for individuals and the government to work together, you know, to solve a larger problem for the larger social good. But how we do this, and by the way, South Korea is a democracy, right? But you know, so different models, at least for COVID, have worked very differently, but the larger question that you ask is a fascinating one, right? Which is, how do you really find that balance between the arrogance and the capability, right? The arrogance of people in charge saying, yeah, if we have the data, we can do amazing things versus them actually doing some amazing things, but doing some pretty screwed up things as well, right? Because they can't help it and they have access to the data and they don't have accountability, right? So this is what we're dealing with, and this is what I see lawmakers dealing with in the US, in Europe, in India, and I'm sure the Chinese are thinking about this as well. So it's a fascinating situation which we're in, which is that the internet isn't what it used to be. It was freewheeling internet, but it's something that's become so powerful now that we need to figure out how to govern it. And the choices we make in the next few years will have a huge influence on society in these various countries."
    },
    {
        "speaker": "Amit Varma",
        "time": "00:51:47.120",
        "message": "So you've touched on many things there, and I'm going to, you know, through the course of this conversation, come to each of these big questions separately. For example, when should we trust data? You know, what are the dangers of AI going too far? What are the dangers of states going too far? Are we using AI for their totalitarian purposes? How should we govern the use of AI today? All these are big questions that come to them. But first, a sort of response or even a musing about what you said about the internet, which sort of also brings up a related, both an ethical question and also a question of how humans regard themselves, which is, you know, and I completely agree with you that the internet has, you know, enabled amazing progress in various ways. But also it has made our discourse so polarized and toxic and cause so many dangers. And I think part of the reason we were all sort of hopeful about the internet at one point in time, and I think more or less correctly, so is that we assume that it would, you know, enable various aspects of humanity to express themselves to their fullest. But that held an implicit assumption, which I think was wrong, which is that, you know, all of those getting expressed inevitably means a march towards virtue or a march towards a better world, which is not the case, because there are many aspects of human personality which are very ugly, for example, our tribal instincts are, you know, what we see on the internet today, for example, is that, especially on social media, that there is constantly a move towards the extremes, people form ideological echo chambers, confirmation bias kicks in when they look at the world around them. You know, dialogue has stopped, people aren't talking to each other, but past each other and at each other. And all of that, people are constantly signaling to raise their status within their own in groups. And as a consequence, everything is driven to extremism. And also that reality doesn't matter so much as narratives. And you basically pick your own narrative, and then you live in that world constructed in your head, and you can just get away with that. And there are a lot of bad consequences of that the one that I see most often in what I do is just how bad the political discourse has become. But then the ethical question that comes up, and I've had episodes in the past on this, as well, where I've been grappling with this issue, is that a lot of these consequences come from the voluntary actions of humans, that, you know, if I, for example, want to be in an ideological echo chamber, and I want to consume only the sort of narratives that satisfy me without giving a damn about whether they conform to reality or not, then the question that arises is that one should somebody get in the way of my voluntary choices? And would it be correct for them to do so? So that's really an ethical question. Because then you're telling people what is good for them, or what they should do. And that becomes a very paternalistic kind of approach. And also whether that will make a difference anyway, because now that we've been empowered with tools that allow us, for example, just to take one domain that allow us to believe whatever narrative we want about the world, we are going to do that anyway, you might say that no, the algorithm should give me, you know, more broad based views and news from across the place, but I will still believe what I want. So I mean, this is a bit of a rambling, but any reactions?"
    },
    {
        "speaker": "Guest",
        "time": "00:55:00.760",
        "message": "No, it's a it's a fascinating question, right? What you're really touching on is, well, the internet was supposed to be free, aren't we free? So what if I have a bias, I should express it freely. After all, I'm doing it voluntarily. And I can completely see the logic for that. The question you have to ask yourself is, is it voluntary, right? Is your response voluntary, or is it orchestrated by an algorithm, right? And the disturbing thing that I think we're seeing sufficient evidence of is that if I have an AI machine, that machine has, you know, what we call an objective function, right? That machine is driven, its behavior is driven by an objective function, right? So in healthcare, if I'm diagnosing cases, you know, I have a machine that's learning how to diagnose cases, it's trying to minimize the errors it makes, right? That's its objective function is to minimize error, right? Minimize misdiagnosis. So that's how it learns. Now, let's take this in a social context, right? Now I have an algorithm, and it's an algorithm, and I'm making money through, let's say, advertising. And now I tell the algorithm, hey, just go for it, maximize my advertising revenue, you figure out the best way to do it, right? I'm not going to tell you, and the algorithm goes, you know, it does its hocus pocus. And it realizes that, oh, people are more likely to accept friend suggestions when they see that you've got more people in common with this friend that's just been suggested to you by this algorithm, right? And the more likely it is that you accept the invitation, the more likely it is now that you're connected more densely to this cluster of people, and this more dense connection increases your engagement with the platform, you now spend more time on it. And now that your engagement has increased, we find that, hey, like our revenues go up when people are more engaged, surprise, surprise, right? Now, if you're the operator of that algorithm, what are you going to tell it to do, stop? No, you've got an obligation to your shareholders, right, to maximize ad revenue. Well, you're going to tell the algo, just go for it, you know, keep closing these triangles all day. And sure, we get echo chambers, but what the hell, that's someone else's problem, right? It's almost like factories in the old days that polluted the rivers and said, yeah, we're just polluting the river and polluting the air. But for us, it's free. It's society's problem to clean that shit up, right? That's what economists called an externality that, you know, General Electric and it's, you know, nuclear power, whatever, is polluting the Hudson. Well, you know, and then we realized, gee, that's bad for society, we're going to fine them for doing that, right? So we have the EPA that comes in and says, okay, now we have Environment Protection Agency because you guys are polluting the environment, right? We need to protect the environment because you're polluting it, right? And you're causing rates of cancer to go up, people are sick, they're dying, right? We need to do something. It's the same thing happening in social media, if you really think about it, right? But the trouble is, we don't really understand it well enough, right? We do know there's plenty of evidence to suggest that these algorithms do some nasty stuff as a side effect of the fact that they're trying to maximize engagement. So they're just trying, they're just doing what they've been told to do, right? And they don't know that they're causing, you know, teen suicide rates to go up or things like that, right? So when, you know, when I talk to high school counselors, they say that they are seeing increased levels of stress among teenagers, right? What's causing it? I don't know, right? But we can't really figure these things out unless we have some data to be able to measure these things and be able to correlate them with what's going on in society, such as social media platforms. At the moment, we don't have this data. So the jury's out. But this is the kind of stuff we need to be thinking about, which is, are these things really voluntary or are you just being a tool that the algo is kind of, you know, playing around with and the algo is not being evil. It's just doing its thing in accordance with its objective function, but it's creating a mess as a side effect of what it's doing. And that's what we're seeing emerging these days. And that's why you have all this concern and lawmakers, you know, both Democrats and Republicans accusing social media platforms of being biased, of spreading misinformation, you know, all that kind of stuff. When the truth is that, look, you know, we didn't, we haven't really thought about these things and you can't blame these platforms for doing what they're doing, right? They haven't, you know, egregiously violated the law. You know, they did things that were shady, but within the limits of the law. And yes, in the early days, they tell you, we'll protect your data. We won't share it. And then they say, well, maybe we'll share it. And then they said, screw you. We're just going to do whatever we want with it. If you don't want us, you know, we don't want you, right? We are now sufficiently powerful that we don't need you. We have all the data anyway. So that's the situation we're in where a lot of these platforms have just been bad boys. They've behaved badly. They've behaved irresponsibly. And we're seeing the side effects of that. So it's a more nuanced question than saying, well, you know, why should we get in the way of freedom, which I completely agree with. But I think it's more complicated than that because we're in the whole new world here. We're in the whole brave new world here where, you know, algos have assumed a level of control that they didn't have, you know, 10 years ago."
    },
    {
        "speaker": "Amit Varma",
        "time": "01:01:15.160",
        "message": "We are indeed in a brave new world here, which is why your new podcast is, you know, much awaited by me and others. You know, I'll come to the political economy and the governance aspect of what do we do about big tech later on in the second half of the show. But I'd like to sort of stay on this for a moment to kind of take a step back and get a little meta and just look at a sort of a broader philosophical aspect of it. Because one, you could argue that there is no such thing as free will, however, we should still behave as if there is. So instead of people's actions being determined by random events, it is sort of perhaps more concentrated. But the other aspect of that is like, let me zoom into the political discourse, which is something, you know, I kind of think about a fair bit. And therefore, I can talk about in some detail, like, I think a lot of what exacerbated like, I think, a seminal moment in this kind of political polarization was the introduction of the Facebook like button, maybe around 2008 or 2009, or whatever. Because then what happens is that people start craving likes, because they get their dopamine rushes. Similarly, in a Twitter context, they will start craving retweets, because they want that dopamine, they want the notifications to fill up with validation. Now, where does that come from? It comes from a human need for validation, which those tools are playing to which is why they are so effective. Now, this leads to polarization, because to maximize your likes, and to maximize your retweets, you take more and more extreme positions, you do more and more, which you signaling whether you are on the left or the right or whatever, you are doing all that and, you know, and these echo chambers form and what Cass Sunstein calls group polarization, then automatically happens. But there are a couple of points here. One point is that these functionalities also serve a useful purpose. For example, almost every ad that I see these days is targeted towards me and is therefore useful to both me and the advertiser in a mutual sense, you know, when I go to YouTube, I'm damn happy that their algorithm is tracking my preferences to such a fine degree, because it is serving up much more relevant content, and is acting as a fantastic filter in that sense. The other aspect of it is that there is nothing teleological or intentional about this. It's one thing that say when a foreign country says that I want to, you know, spread misinformation, and I want to influence an election, there is an intent, there's a teleology there, that's not necessarily the case when it comes to a social media platform, putting something there that actually makes your experience pleasurable in certain ways, like the Facebook like button or the Twitter retweet button. So you know, to cut a long story short, what I'm coming at is that all the problems that we see emerging out of social media and out of the internet and all of that, and I'm not using this as an argument for one or the other, I mean, this just in a descriptive sense, these are problems inherent in humanity, not in technology. It just so happens that technology has, for example, played on this need for validation and you know, created this effect and so on and so forth. But the deeper thing that we perhaps need to think about and that should give us a cause to ponder and perhaps just be more humble about our species as a whole, is that we are deeply flawed and messed up. And you know, a lot of that finds expression through technology nowadays."
    },
    {
        "speaker": "Guest",
        "time": "01:04:36.160",
        "message": "Absolutely. We are very deeply flawed and quite deeply messed up. And our history has been quite brutal, right? I mean, we've come from, I guess, you know, being savages to being feudal, you know, I mean, so we, you know, humanity has gone through sort of, you know, you know, savagery, you know, followed by slavery and then feudalism, you know, and then law, right? So we've kind of then created societies around law, right? So that's been sort of the progression of humanity. It's been a positive progression. And now we're in the age of tech where we'd actually implement law so much more easily and actually it'll turbocharge law as well. So we've had like a brutal history, but I think the solution is not control because who the hell knows what the right amount of control and what the right control is, right? The solution isn't control, but it's transparency, right? We need to be aware of our savage instincts, of our limitations, right? So we need to become more self-aware and that's where I see the solution, right? So I, you know, so I tend to be an optimist, right? I'm not, you know, despite these dysfunctional kind of, you know, dark scenarios that people paint about the future, I actually think the future is going to be pretty amazing because I have optimism in the ability of mankind to sort of, you know, rescue ourselves from the precipice, right? The stakes are too high. We, you know, we don't want to like destroy our society, you know, we've come close at a few points, you know, with the cold war and things, but we've sort of been aware of, you know, of some of these risks that technology has introduced in society, right? The fact that we can destroy ourselves, you know, with a push of a few buttons, right? So we're in a society that has tremendous destructive potential. And the internet is similar. It has some amazing positive potential, much of which we've seen, right? But we're beginning to see some of these dysfunctional aspects of the interplay between technology and humans. I agree with you that it's, there's nothing evil about the technology. The technology itself, one might even argue it's neutral. It's the way it's employed that we need to be aware of. And that's what governance is really all about, right? That's what internet governance is really all about, about being aware of our limitations, about being aware of our history, about being aware of how we're interacting with these platforms and going into this with our eyes open and then saying, hey, what's the best way to govern? The trouble is when things get dark, right? When people function in the dark, that's when shit happens, right? And we've seen this in every industry, like finance, like, you know, Wall Street were the bad boys for decades, they were the flogging horse, right? It used to be an expression, look at Wall Street, right? Wall Street was synonymous with evil, right? Because people had gotten greedy, you know, they would treat customers badly. They would do things that were unethical because regulation didn't exist that was, you know, geared towards finding that, right? So in a sense, regulators always sort of behind the eight ball, right? People close to the ground find ways to function, find ways around the system and bankers were no different, right? They were functioning within the law, but were doing unethical things, right? And that led to all kinds of problems. There were lots of NASDAQ dealers that got fined in the 90s, you know, for throwing customers' orders into the garbage or not answering phones and, you know, stuff like that, just bad stuff. And so regulation came in and said, hey, you know, like we see this thing going on, we have to, you know, put some constraints around it so that these people don't destabilize the system. They don't favor customers over others, right? So the reason the financial system of the U.S. is sort of the most trusted in the world is because it's been through all kinds of phases, bad stuff's happened. People have come and looked at it. They've sorted it out, you know, to the degree that people have trust in the system and systems don't function unless there's a fundamental degree of trust in it, right? And that's why people trust the U.S. financial system, even though it has its, you know, moments of insanity, right? For the most part, it's things have to add up, right? There's a regulation you have to follow, you know, you just can't do whatever you want, right? Whereas, you know, Mr. Zuckerberg can turn a dial and do what the hell he wants and no one is aware of what's going on, right? So the solution to it is transparency. We've got to know what these guys are doing, what's, you know, what's going on inside these platforms, right? If, you know, Jane and Gene are working for me, calling customers all day, the regulator wants to do like, you know, wants to know what did Jane and Gene do all day? But if Jane and Gene are machines, do I just get a pass? Because they're robots and they can call people all day and do stuff under the radar? No, that doesn't make any sense, right? Why should we treat humans and machines differently if they're doing the similar kind of thing and, you know, resulting in risks to the system, right? So in financial services, we figured this stuff around, around humans first and then around systems because the financial industry was one of the first to automate and adopt technology because the stakes were the highest. And then, you know, we learned to wrap sensible regulation around it, but mostly sensible anyway. And so that's where I see ourselves more generally in the social media space, right? So I see that as kind of the current Wild West, which, you know, the financial industry was in the 70s and the 80s and 90s. Those are relative Wild West days. That's where I see ourselves now in terms of internet governance. So we need that transparency. And what I think we're talking about is what does that actually mean, right? What does that look like? You know, sounds great, but, you know, to Zuckerberg and Dorsey, transparency means, well, you know, we'll give you quarterly reports that'll tell you all the crap that happened on our platform. And believe me, we'll be good about it, right? You know, trust us, right? You know, would we have trusted the bankers of the great financial crisis if they said, you know, yeah, sorry, I know we were irresponsible and we did all kinds of off balance sheet transactions, but believe me, we've learned and trust me, we'll never do this again, right? Yeah. Good luck with that one, right? That's where we're at with internet platforms. And they're going to go through a similar process of scrutiny. And it's about time, right? Not to say that this will be fast and expeditious and effective. I'm sure it'll be messy. But we're heading in the right direction. And being an optimist, I'm optimistic that, you know, we'll solve this problem."
    },
    {
        "speaker": "Amit Varma",
        "time": "01:11:35.160",
        "message": "So you know, you described your life as in terms of what Jerry Garcia spoke about what a long, strange trip it's been. And indeed, because I just realized that you've been on two frontiers. One is a wild west of Wall Street in the 1990s when you worked there. And now the current sort of wild west of this current situation. And we'll talk more about what to do with Jane and Gene. But if Jane and Gene are listening to this, which I have no doubt they probably are, because hey, AI, they can relax a bit, because we'll do that after a quick commercial break. So Jane and Gene, you have one minute to get to act together. As many of you know, I'll soon be coming out with a four volume anthology of the seen and the unseen books organized around the themes of politics, history, economics, and society and culture. These days, I'm wading through over 3 million words of conversation from all my episodes so far to curate the best bits. And for this to happen, I needed transcripts. And that was made possible by a remarkable young startup called Tapchief. Tapchief at tapchief.com is a digital platform that allows companies to outsource work to their network of freelancers. And Tapchief's network includes more than 125,000 people as of now. You want people to make you a web page or design a logo or compose a jingle or do some digital marketing for you. Tapchief gives you an easy way to reach out to freelancers competing for your work. I can say from first hand experience how valuable this has been for me and solve the problem I was actually a bit worried about. So do go over to tapchief.com and check out all that Tapchief has to offer. Maybe they could solve your problem too. Welcome back to the seen and the unseen. I'm chatting with Vasanth here about the brave new world we are in and brave new world actually makes it sound exciting, but he's optimistic and so am I. So we'll just look at, you know, we won't rename it to scary new world or the dystopian future or anything like that. It is a brave new world. Now, you know, I was going to sort of leave the the last part of the episode for talking about the current times about sort of what we ought to do about big data and all the dangers that we can all agree it poses. But since we are on the subject, I think I might as well go there now. And here I'm, I found your writing on this very insightful and I'm in agreement as well, because whenever I've discussed this issue in the past, it struck me that many of the problems that are arising from the way our data platforms behave, such as increasing polarization, are social problems and we have to find social solutions. And I'm a bit wary of the state getting involved because, you know, we always assume when we advocate state sort of getting into solve something that it will always be benevolent and benign and it is rarely the case. So you want to, you know, achieve things with as little state coercion as possible, like even for the financial crisis, a lot of the things that went wrong, like even, you know, going back to the bad incentives of the Community Reinvestment Act in the late 70s or the moral hazard posed by the way Fannie Mae and Freddie Mac behaved is sort of an example of what a state interference can do. But the solution that you come up with actually empowers users more than coerces them. And this also comes from something that Ben Thompson of Statutory once wrote, which I agree with, so I'll just quote him, where Thompson discussing the possible regulation of big data wrote, quote, if regulators, EU or otherwise truly want to constrain Facebook and Google, or for that matter, all of the other ad networks and companies that in reality are far more of a threat to user privacy, then the ultimate force is user demand. And the lever is demanding transparency on exactly what these companies are doing. And I came across this excellent piece that you had written recently, it'll be linked from the show notes, where you wrote about three ways to increase social media platforms transparency. And I love the solution, because here, what it is, is it's not the state using the strong arm on specific companies to say, do X, do Y, do Z, it's just making them more transparent. So then users can decide for themselves whether they want to engage with those platforms or not. So take me through these sort of these three ways to increase the transparency, so to say."
    },
    {
        "speaker": "Guest",
        "time": "01:15:52.160",
        "message": "So not surprisingly, the solution that I'm proposing has been motivated by the financial services industry that we were talking about, one that is regulated, and that for the most part seems to work all right, at least in terms of engendering trust in the system. So if you think about it, some of the assumptions that we've been working under are a little bit outdated regarding, let's say, anonymity. So the first thing I proposed is that, and by the way, in 2016, when I first suggested that we should regulate the social media platforms, I got a lot of backlash from people for being un-American and all that kind of stuff, because people felt, you know, this is a free country, they should be able to do whatever they want. But it was becoming clear that that really wasn't going to work, because one of the things about a democracy is, and people don't often recognize this, is that it's not just about rights, but it's also about obligations. So yeah, we have rights in a liberal democracy, rights to be free, but we also have an obligation towards our citizens. We can't go around killing people, there's a certain sort of expectation and norms that exist in democratic society. And so what I suggested was that we need user transparency. These platforms need to know who the users are, who you're dealing with, like a bank knows who it's dealing with, right? You can't just set up a bank account without adequate credentials and authentication, like to prove to them that you are who you say you are. Now, India, by the way, has a fascinating solution to this with the Aadhaar platform, like really forward thinking solution. But this is what banks do, are you who you say you are, like know your user, right? So in banking, it's called know your customer. Now in the social media space, customers are actually advertisers because they're the ones paying you. But what I suggested, you should know your users. If someone is on platform, who are they, right? Can they authenticate who they say they are? Because I think they have a responsibility, not to you and me, they can still use a pseudonym and function anonymously. But the platform must know who the hell am I dealing with? Who is this a real person, right? They need to know. Is this a person? Is it a bot? What am I dealing with? So that's the first thing that I suggested is, you know, since we're all talking about transparency, well, let's just be transparent to the platform, right? And so they know who they're dealing with. Is that an invasion of privacy? No, it isn't. It's no more an invasion of privacy, like, you know, if you're required to provide your bank with information, that's not an invasion of privacy. It's just like you authenticating and guaranteeing who you say you are, and by the way, you're also agreeing to behave yourself, right, with the bank, right? You're agreeing not to conduct fraudulent transactions, and you're not going to do trades that destabilize the market, and you're agreeing implicitly to a lot of things when you sign up with an account. So it's not an invasion of privacy, right? They should know their user, right? And yet so far, their motivations have been exactly the opposite. They'd rather not know their users, right? It's actually more profitable for them not to know their users, right? Like, you want another account? Come on over, sign up, say whatever you want. And so my solution is, yeah, you can say whatever you want, but we need to know who you are, right? Authenticate yourself, the platform. The second thing I propose is that just like I can, as a regulator, go into a bank and say, hey, show me that you haven't been favoring some customers over others. And I've had to do that twice myself over the last 10 years to regulators in the hedge fund that we operate. It's a machine learning-based fund. Regulators come over and they ask us all kinds of stuff. We have to show them all our trades, how they were allocated to clients. We weren't favoring anyone. They all made the same amount of money, right? So they come in and do a bunch of tests on us and tell us whether we passed or failed. Now, in financial services, these guys have been doing this for a long time. They know what they're looking for, right? They're looking for the bad behavior they're looking for. So what I'm proposing is something similar, right, in the social media space. If you're actually generating revenues by making recommendations, well, then keep a history of it. You don't need to share that with the rest of the world, but just keep it. In case regulators want to come in next year and say, so, like, have you been making recommendations? Tell us. Show us, right? And you should be able to do that. At the moment, there's no such requirement, right? There's absolutely no transparency. So that's the second thing, the ability to provide audit trails of your behavior, right? And this, by the way, will get rid of a lot of problems that we're having right now, right? The very fact that you know you're being observed makes you behave, right? Not that you will, that they'll come in and, like, want to see everything, but the fact is that they can, right? So you have to behave in accordance with how you're supposed to, right? So that's the second word was transparency in terms of the ability to produce audit trails of your actions. That was the second one. And the third one was algorithmic transparency, which is, what did Jane and Gene do all day anyway? You know, like, broadly speaking, what are they doing? Are they soliciting? Well, okay, if they're soliciting, how are they doing it, you know? Is it, are they using legitimate data? And after they do the solicitation stuff, what actually happens, you know, does engagement actually go up? What were the consequences of that? We can actually look at that, right? So regulators should have the ability to go in and see what the hell is going on inside this operation, right? So it's like operational transparency through the algorithm. So to me, these are, I don't know, they seem like no brainers to me, like what's wrong with this? What's wrong with these three forms of transparency? Yeah, you can keep doing your secret sauce and have your algorithms and smart algorithms and whatever, but we just need to be able to see what you've actually done, just like we do in other industries. So that's what I've proposed is that these are simple ways to increase transparency without really much in the way of downside, as far as I can see, other than the fact that you need to store a lot of stuff."
    },
    {
        "speaker": "Amit Varma",
        "time": "01:22:14.720",
        "message": "No, that's fair enough. And I like these solutions because none of these seem like state overreach to me, which is something that I'd otherwise be worried about. And one hears suggestions to that account as well, though I was a bit concerned by your framing of it is, you know, obligations along with rights. So you don't need it in the same sense as our prime minister did, where Narendra Modi keeps talking about how it's not just about fundamental rights, but fundamental duties, which by the way, is a phrase introduced into our constitution by Indira Gandhi. So it's like from one great totalitarian to the other, in a sense. And what I keep pointing out is that, listen, it's okay to talk about duties, but we should remember that individuals don't have a duty to the state, the state has a duty to individuals. And in fact, the only obligations I would recognize on the part of individuals is not to transgress on the rights of others, which really, you know, comes down to the framing of rights as well. But yeah, I mean, as far as the state having an obligation to its citizens is concerned, I don't see how this kind of regulation is an overreach in any way. So I'm good with these, except that, thinking aloud, my thought would be that I totally agree that if everyone had to authenticate herself to the platform, they would automatically behave themselves. But the other two, I don't know what difference it would make. For example, if you have algorithmic transparency, and they show you an algorithm that says that we connect people with like minded people, and we allow them to express themselves in whatever way they want, as long as they are not transgressing the laws or inciting a crime. In that case, you know, that would be perfectly legit, you can't argue with that. And yet that does go all the polarization and all of that is sort of happening because of that. So you know, would that really solve the problem? We sometimes assume that many of the problems that are being caused by big tech is somehow there are malign algorithms, which are doing malicious things and deliberately spreading misinformation. But no, I would argue that, you know, there are, you know, algorithms, which are completely benign, like connecting you to like be like minded people and allowing you to express yourself, which you know, the problem is with humanity, not technology and regulating technology won't necessarily solve that. And I know what you said isn't intended as a panacea, obviously, so but this thought just came to mind."
    },
    {
        "speaker": "Guest",
        "time": "01:24:26.960",
        "message": "You're absolutely right on that. But this reminds me of some of my colleagues who teach ethics. And the thing you become aware of is that I can conjure up a scenario where it's actually fine to lie. I can conjure up a scenario where I say, okay, you know, you have two choices, right? One is, you know, if you just tell this little lie, the planet will be saved. But if you tell the truth, we'll all be destroyed. What's the right thing to do? The right thing to do is to lie, right, even though we know that you shouldn't lie, right? So and that's the nuance here that we need to appreciate, you know, you're absolutely right that, you know, look, you can't like transgress and interfere with people's freedoms and tell them what to do and tell these platforms how to operate the algorithms, right? But if I conjure up a really extreme scenario, right, one that's just so compelling, right, that you can't ignore it, then what do you do? So for example, for me, a compelling scenario might be that, so Facebook's been recording all its recommendations, everything to everyone, right, for the last year, the last couple of years. And then we go back and look at the stuff and we say, you know what, like, this is all well and good, but for people like under 17 years of age, it's leading to like increases in suicide, and we can actually demonstrate that, you know, quite convincingly that your algorithms that are doing whatever the hell they're doing, we have no idea, are, you know, there's like very strong evidence that leading to like a tenfold increase in teen suicide, right? Like, what would you do in that case, right? Would you still say, well, you know, it's a free country, like, let him die, right? That's free will, they're just bringing them upon themselves. They should be smarter than to know that they should get, you know, sucked in by whatever they're doing. Clearly, that's not good enough, right? In the physical world, we have all kinds of safeguards, like with food, you're required to label the food and show what's in it, right? With something else, you know, you can always come up with scenarios where people have sort of decided that something crosses a line, and that is an acceptable behavior. That's the situation we're in, we just don't know enough at the moment about the consequences of sort of the Wild West that we're in at the moment, right? We have suggestive evidence that tells us we should be concerned, but we don't have all the data. And we struggle with it, right? There was a Supreme Court judgment last week in the US, where the Supreme Court ruled that Governor Cuomo's edict to ban gatherings, let's say religious gatherings, of over 50 or 25 in the red zone was banned, prohibited, and the Supreme Court ruled it unconstitutional. And one of the reasons that, you know, one of the judges, I think maybe it was Neil Gorsuch, said that, you know, there's no evidence that suggests that these institutions have been responsible for the spread of the virus. There's no evidence, right? So you can look at that, and so we're going to apply the strict scrutiny test to this. Now, the question I have is, supposing we were South Korea, right, and we had actually measured these things, right, we'd measured how many people gather, the infection rates, all that kind of stuff, right, and you present this to the nine Supreme Court justices, and you say, you know what, you know, your honorable selves, what we're seeing is that, you know, when gatherings go above 49, I'm just making this up, you know, and the base rate of infections in that community is more than, you know, X, the spread of the disease tends to be tenfold higher than it is otherwise, and here's the data to show it, right, there's like 300 cases, you know, and here are the statistics, right, would the Supreme Court justices still come up with the same verdict? I would think that they would be affected by the evidence, right? So what I'm getting at is more transparency for society in general, where we can make these decisions based on the evidence, right, as opposed to our value systems or beliefs or assumptions, right? That's the old way, right? The brave new world will consist of data being, you know, having sort of first-class status in decision-making and not just people's assumptions and norms, right, that is, we should look at data, we should be influenced by evidence, and that cuts across, you know, all areas of our lives. So, you know, it just sort of comes back to, no, there shouldn't be overreach, that's the last thing we need is government overreach, but we do need some way where the government is acting intelligently based on the evidence at hand, and it's transparent, right? To me, that's the best of both worlds, is the government is accountable, and it's transparent, and we have a contract with the government, we, I mean individuals, that we can agree that certain uses of data are for the larger social good, and we work out ways where we prevent these transgressions from taking place, right, where we don't get an overly enthusiastic government coming and changing the rules on us. And that's the situation where we're in at the moment, we find ourselves in. It is a brave new world, right, it's a whole new world of how we govern the internet, how we allow machines to influence and play larger roles in our lives in a way that's better all around for everyone, you know, better for society, better for individuals, but we need to think about this and do it consciously, because the rate at which we're going, that sort of Wild West ways, won't get us there. I'm optimistic that we are seeing the problem in time, and that we will walk away from the precipice and come up with solutions that make sense."
    },
    {
        "speaker": "Amit Varma",
        "time": "01:30:52.120",
        "message": "So let me sort of probe a little deeper in the sense that I obviously agree with you on the desirability of transparency, not just in this domain, but so many others. But what I'm a little skeptical about is the efficacy of it. And the reason why that is not a technical point, but has consequences, is that if one finds later on down the line, that greater transparency doesn't actually solve the problem, then the state can use it as a pretext to actually overreach. Now, let me sort of drill down into one concrete and ask you about one hypothetical example that you brought up and ask you to get concrete. Now you spoke about how, if there is algorithmic transparency, and we find that there is some algorithmic action that has led to a rise in teen suicides, right? Now every reasonable person agrees that that's a problem, and we should solve it. The question here is, number one, how do you establish causality when there are so many algorithms doing so many things and so many influences otherwise? Number two, even if you manage to pinpoint a particular algorithm that shows that there is some causality, that algorithm could by itself both be benign and could have positive effects in another way. For example, an algorithm that allows people to meet like-minded people and talk about things that they care about could foreseeably lead to sort of a rise in teen suicides because you could have young teens coming together and, you know, driving each other more and more towards despair. But at the same time, you could have young teens coming together and sort of cheering each other up and moving away from the precipice. And the latter case would, of course, be unseen. What would be seen would be the suicides, if I might invoke the name of the show. So then the question would be that, first of all, how do you determine causality in terms of which algorithm causes, two, how do you separate out, you know, the ill effects of a benign algorithm if the algorithm also did many good things, such as connecting like-minded people like you and me together, which could have happened, you know, through social media for all that we know, you know, and so when I drill down to the concrete, it becomes difficult. And then if you leave it to human judgment at how this caused that, then human judgment will always be flawed. And when it's a judgment of the state will always end up enhancing their power or I mean, this is, of course, a non-political example. But if there is a political example, then it will play to the biases of whichever side holds the levers of the state at that point in time, which could be either the far right or the far left. So is it sort of, and obviously, forgive me if it's a new question, but can you give me an example of, is there an actual instance where in this context of social media causing behavior, you figured out that a specific set of algorithms actually caused a specific kind of problem in the real world?"
    },
    {
        "speaker": "Guest",
        "time": "01:33:37.600",
        "message": "So there were at least two things you were alluding to in your question. One was the difficulty of doing the science itself, and the fact that you may not find anything significant once you do the science. That is the causality may be hard to establish, there may be considerable uncertainty associated with the conclusions you're drawing, even if you find some effect, there's still what I call variance around it and uncertainty. And that's certainly true. And I guess in a sense, I'm sort of assuming that we can solve that problem, that in fact, if there is no demonstration of causality and no statistically significant relationship between A and B, well, then that should not be used, right? Then it's telling you that there's nothing to worry about, right? So what I'm assuming is that we first do find things that we have to worry about, right? And we're finding those already, right? And that's the last two years have revealed all some of the things that we should be worried about, but probably not all of them, right? So to me, transparency is about achieving awareness, because once you're aware of something, you can act more intelligently, right? So if you're aware of the fact that, let's say, certain algorithms are leading to teen depression, right? That there's a high association between those two, well, then maybe we can start taking mitigating steps, like making teens more aware of how these algorithms might be influencing"
    },
    {
        "speaker": "Amit Varma",
        "time": "01:35:23.640",
        "message": "them, right? But if I might interrupt you, algorithms such as what, like I'm just having difficulty conceiving of an algorithm that specifically causes, say, teen depression rates to rise apart from the very general ones of bringing like-minded people together."
    },
    {
        "speaker": "Guest",
        "time": "01:35:38.880",
        "message": "So let's say that I come up with an algorithm that finds a way of connecting various kinds of teens together that increases overall engagement among them, but also increases discord among them, which you can actually see in the data. You can look at what they're saying, right? And by the way, I'm making this up. I'm not saying that this is actually happening, right? So platform is doing something, and we observe that it's actually leading to more engagement. People are talking more to each other, but you know what? This talking is actually not good talk. It's bad talk. And, you know, and that bad talk is, you know, correlated with, you know, depressions and suicides in that zip code, right? In that area, right? Now let's say the science was done really well, and it's showing you this. Well, are you going to ignore it, right? Well, you could, but it's probably not a good idea to ignore it, right? Because there's something going on that you should probably do something about, right? And you know, this is something you may not have been even aware of earlier, right? I mean, how do you know this is going on if you don't even have the data? But if I have sufficient evidence to know that something is going on among the users of that platform that's leading to some very undesirable societal outcome, then I should figure out what I can do about it. Right? It's, that's all. And so if, but if there's nothing, well, then I shouldn't do anything about it, right? So I'm assuming that the science has been done, you know, notwithstanding the messiness of it, right? Sometimes the data is hard, it's noisy, you know, we're used to dealing with that, you know? So we can deal with that. But presumably, we can still do the science well, and establish either causality or a strong association that may suggest causality that's worth exploring."
    },
    {
        "speaker": "Amit Varma",
        "time": "01:37:43.080",
        "message": "Right. Let's sort of move on to how different regimes across the world are actually sort of handling this problem. Like in a piece that you wrote, headlined, In the AI Era, Privacy and Democracy are in Peril. You wrote, quote, I recommend regulating the use of personal data for prediction products. I also propose classifying certain platforms as digital utilities that aim to maximize public benefit and spur economic growth, much like the interstate highway system, and the information superhighway have done for physical and electronic commerce, stop quote. And then you talk about four major models of data use, which is at the extremes US and China and then EU and India as well. So can you talk a bit about these four different approaches, where they come from, how, you know, effective they are, and what do you feel closest to in terms of actually solving"
    },
    {
        "speaker": "Guest",
        "time": "01:38:32.880",
        "message": "the problem? So they all have their strengths and weaknesses, right? The strength of the US model is that it's free, right? It's free of like almost, you know, any kind of regulation at this point. And so it's grown up sort of organically, right? This is the way that the internet has developed in the US and people would argue that there've been lots of good things that have happened, that all kinds of really cool stuff has emerged that may not have, you know, if we hadn't given these internet companies and the platform complete freedom and absolved them from lawsuits and responsibilities and things like that. So that's the beauty of the US model. But we've seen the problems with it emerge recently, that these platforms have become so powerful and they've overreached so much in terms of data that they're like messing some things up, right? And that's why all the scrutiny around the internet models in the US. Europe not surprisingly has always been, you know, for accountability, right? So their approach to it has been sort of responsible data use, right? That you can use data if it's essential for what you're trying to do, right? Which also makes sense because they're trying to cut down the risks associated with this sort of unbridled wild west use of data. So have some, provide some kind of accountability. India is sort of a really interesting case for a number of reasons. So it has sort of this late mover advantage and it also has sort of a different kind of advantage, which is that I think the people that are sort of influential in tech in India have sort of more of a social approach to infrastructure. And there's this notion of sort of public digital infrastructure, not surprisingly has emerged, you know, mostly, you know, out of India. And it actually happened maybe partly by accident, you know, partly it was serendipity, you know, that we decided to implement the Aadhaar platform, right? Which was for authentication and to actually bring people who didn't have an identity prior to that into the digital mainstream, right? So now I know there's controversy in India about its misuse and all that kind of stuff, but I think we'd be silly to, you know, not acknowledge the tremendous benefits that such a platform can bring to society, right? So we built that and then it seems to work, right? It has, you know, 1.2 billion users and it's very widely used for a number of things. And now we're thinking, oh, maybe we can actually build something on top of this authentication layer where we can have, you know, and in India, they call it the India stack, where we can actually stack sort of layers of utility. So I look at Aadhaar as a utility, that's essentially what it is. It just does one thing, you know, are you who you say you are? And it answers that question, yes or no, simple. It doesn't, it hasn't spilled over into other things, like it's not checking your credit scores or anything like that. It's just simple, one function thing in mind. And now we're thinking, oh, we can layer a system on top of that, that since someone can authenticate you and they know who you are, you can then say, oh yeah, I want to share my transcripts of the university or with the school, or I want to share my credit history with the bank for a certain amount of time. I like the thinking around it because it's a utility, not unlike sort of the, the, the internet, right? Not, not, not unlike, you know, the highway where you're sort of trying to lay the rails for a utility that will be used like crazy, right? Where you don't actually want the utility provider to be making a, you know, a ton of money off of it because it's a utility, it should be priced like electricity or water. It's a basic thing that everyone engages in, like moving money around, you know, like it's a utility. There's so much of it happening. It should happen almost costlessly, you know, because after all, it's just a ledger entry, right? It's not like you have a Pony Express moving money from A to B that costs money. It is costless. And so it should be costless. It should be utility. So I like the thinking in India around this, which is around data that is personal, non-personal, right? And non-personal becomes community data, right? So maybe, you know, if you and I go to the hospital, maybe that data can be anonymized, you know, and it becomes sort of a community data set. But you can say that, you know, people over 50 who have this, who are exposed to this are at risk. And therefore we can craft some, you know, health policy around that based on community data, right? Now, this thinking is in its early stages, right? I've read the reports that have come out around this. They're interesting, but they have all kinds of holes in them that haven't really been thought out, but that's okay. You know, we can sort those out. So I like the thinking in India around this sort of public digital infrastructure and building layers on top of that, where people are in control and consent to have that data used or, you know, for a certain amount of time and for a specific purpose. To me, that's sort of the best of both worlds, where users are aware and in charge and can control data, right? In the US, it's become kind of, we've lost control over that, right? People have lost control over where their data is, who's using it, for what, what can they infer from it? The Chinese model is fascinating and worrisome at the same time, right? It's fascinating because they've done amazing things with it, right? They've exercised control and, you know, in a way that's been quite effective, presumably for things like COVID to the extent that you can trust information coming out of China, right? They've been quite effective at dealing with that. They've been quite effective at gaining efficiencies by having data centralized. But you know, there's a nasty side to it, right? You know, we read these excesses, you know, in Xinjiang against the Uyghurs and, you know, and God knows what else is there, right? Dissidents are like rounded up and threatened and things like that, right? So there's this sort of very dark side of that centralized model that really sort of bothers me. And it concerns me when people project that as kind of the better model for internet governance, because, you know, knowing human history, we should be worried, right? I mean, I don't trust any institution fully, like any kind of government, you know, yeah, for the most part, I, you know, I, for the most part, I trust the US government, but they routinely violate their laws. Well, I don't know how routinely, but you know, the example that comes to mind is that, you know, who was that Swami from Pune, who moved to Antelope, Oregon?"
    },
    {
        "speaker": "Amit Varma",
        "time": "01:45:33.240",
        "message": "Rajneesh. Yeah, Rajneesh."
    },
    {
        "speaker": "Guest",
        "time": "01:45:35.240",
        "message": "Right. I mean, Osho, you know, the US government essentially decided, you know, I mean, the one thing you do is you can't mess with the US government sort of openly and blatantly that they'll get you, right? It's a free country. It's a democracy. You have rights. But if you overstep, they'll get you. And that's what the US government did. You know, they violated their own laws to get him and basically set an example. Yeah. So that's what bothers me about the Chinese model is that, yes, I buy the tremendous benefits of efficiency and all the good things that come with centralization and control, right, that you didn't have in the old style centralized regimes, communist regimes, right, where there was no information. And so that was a terrible model, a governance model in terms of getting anything done. Right. And we saw that in Russia, saw that in China, like just bad governance models, whereas now they're like swimming in data and saying, wow, like, you know, we can do our job so much better. If only we had that in the old days, maybe we would have worked then. So there's this, for lack of a better word, assumption that some people make that, you know, maybe that is a superior, you know, the central model is a superior governance model because it leads to efficiencies."
    },
    {
        "speaker": "Amit Varma",
        "time": "01:46:44.560",
        "message": "But it is scary to me. I think, you know, if again, if I may think aloud and I'll lead up to like a broader question, but it seems to me that the Chinese model in some senses is what the Indian state would like its model to be, except that it isn't efficient enough. Like, I'll go back to a keyword that you use when you were speaking about the Indian model, which is consent. And my issue with Aadhaar has always been, you know, I don't want to enter the debate. So what kind of system is it? But my problem with Aadhaar always was an imposition of Aadhaar. The fact that it was an act of state coercion, and, you know, which, because there your consent goes out of the window. And that kind of leads me to the sort of the hypothetical question that a couple of hypothetical questions and again, it's a thought experiment, assume that there is a state that says that I have access to all the data that there possibly is, including even what you're thinking. And therefore, that can lead to the best possible outcomes for society, if you just submit to our will, and we'll take care of everything. Even if it maximizes happiness, is that necessarily the right thing to do? And to take it even further, if a state was to say that, listen, we figured out the tech, we figured out the data of how your neurons work, we'll plug electrodes into the brains of every individual, and we'll make sure that you're permanently in a state of the highest possible happiness. Is that something that you would be comfortable with? Because I would certainly not, obviously, I would say that, you know, the one thing that should be sacred in all of this is individual autonomy, and therefore consent. And sometimes we get carried away with the power of data, and the seductive allure of a technology like Aadhaar, which can do all of these things. And we forget the basic core principles that make a democracy a democracy, which is not just the fact that you have, you know, elections happening, but also that individual rights are sort of taken care of, you know, and if you have that mentality, that consent doesn't matter, Aadhaar is for everybody's good, then that mentality creeps over into what is Chinese style authoritarianism, except that I don't think it will happen in India, because our state is simply too incompetent. What are your thoughts on that?"
    },
    {
        "speaker": "Guest",
        "time": "01:48:50.920",
        "message": "There was so much stuff I wanted to say, but I'll bring it back to what you started with, which is, you know, like, how to think and what a good theory is, right, and a good theory is one that makes fewest unreasonable assumptions, right? And so I will turn the question around and ask yourself this, which is, like, this scenario which you painted, this utopian scenario. Dystopian. Well, no, but utopian in quotes, which is that your happiness is always maximized, believe me, you know, if I know what you're thinking, I'll feed you that right amount of dopamine, and you'll be in bliss forever. So what are you complaining about, right? So the question I would ask is, like, what assumptions is that theory hinged on, right?"
    },
    {
        "speaker": "Amit Varma",
        "time": "01:49:36.280",
        "message": "Are you familiar? It's a thought experiment."
    },
    {
        "speaker": "Guest",
        "time": "01:49:38.280",
        "message": "Are you comfortable with the assumptions on which that theory is based? And I would wager that you will not be comfortable with those assumptions, and therefore you'll say that's a terrible theory, and I would concur with that, right? Because right off the top of my head, the assumptions it makes is that, number one, that's even possible to do, right, that humans don't have any free will, and that I'll just tell you what you want. Right? So that's an assumption, perhaps. I'm just thinking aloud here. Maybe the other assumption is that the state will always act in the best interest and will never make a mistake, again, huge assumption, because states make mistakes."
    },
    {
        "speaker": "Amit Varma",
        "time": "01:50:12.240",
        "message": "My assumption is the opposite of that, but continue, I'll respond to all of this."
    },
    {
        "speaker": "Guest",
        "time": "01:50:15.920",
        "message": "But the reason I'm saying that that's, like, a dystopian and not a utopian future is exactly because I don't buy the assumptions that it's predicated on."
    },
    {
        "speaker": "Amit Varma",
        "time": "01:50:26.280",
        "message": "So I'll lay out my assumptions, so that my core assumption here is that the brain is a machine that we haven't figured out yet. But we will one day. Now, when will we? I don't know. But we will surely figure it out one day. It could be a century down the line, if we survive that long. It could be a few decades, you never know. But it is a machine, it is figureoutable. We haven't done it yet. Someday we will. Maybe some superior artificial general intelligence will do it for us instead and use it to control us. That's irrelevant. I believe the state is always and without exception, a malign entity, that its only interest is towards itself. It is predatory and parasitic, although its purpose, the purpose of its existence is the opposite to safeguard individual rights. But it will always end up doing the opposite. And therefore, we must show what Jefferson called eternal vigilance, which is why, you know, again, if you ignore the assumptions, I mean, a thought experiment doesn't always, I think, have to have waterproof assumptions. But core point that I'm getting at here is that to me, individual autonomy is sacred. That even if it was possible for someone to plant electrodes in my head and keep me in a state of bliss all the time, I would not want that, because I would feel it an attack on my autonomy, and I would never consent to it. And yet there is a way of thinking in the world today that can look consent doesn't matter. The state will do what is good for the people. And the imposition of Aadhaar is an example of that. What China is doing is another example of that."
    },
    {
        "speaker": "Guest",
        "time": "01:51:50.320",
        "message": "But who makes this assumption that the state is good for the people other than, let's say,"
    },
    {
        "speaker": "Amit Varma",
        "time": "01:51:54.560",
        "message": "the Chinese?"
    },
    {
        "speaker": "Guest",
        "time": "01:51:55.560",
        "message": "The state itself. Well, the state itself. But who cares about that? Right. I mean, that's the state itself. I don't even know whether all states really believe that. I mean, maybe the Indian state believes that, but I certainly don't think most Americans trust the government. You know, I mean, in fact, you know, while we're talking about it, one of the things that people all over the world always wonder about Americans is like, why are they so hung up about guns? Right. Why do they tell people that there should be gun control? Right. This is like senseless. Right. And I totally get it. Right. But what they don't get about America is that that's exactly what you're getting at. People don't trust the government. Right. So when people say, oh, we shouldn't have guns, well, that assumption is based on trusting the government. When people say that, they trust their government to 100 percent. But a lot. Right. So that statement says, I trust the state. I don't need to protect myself. Right. In the U.S., it's very different. It's very difficult for the rest of the world to understand this. Right. That it's based on this assumption that, hey, you know, the state might start doing some shady stuff. Right. We don't completely trust the state. And that's what I love about America. Right. It is, you know, this society that basically says, no, like we shouldn't trust the government, you know, that we should have checks and balances and the government can tell us what the hell it wants about how this will be good for everyone. No, thank you. Right. I want my individual freedom. Right. Unfortunately, what's happened is that this third actor, namely digital platforms, social media platforms have come into the picture. Right. So Orwell sort of didn't quite have it right. Right. It's not your danger isn't from the government necessarily. Right. It's actually from these private entities that have in some ways become even more powerful than the government. Right. So it's not the government that's necessarily the bad person anymore. We may not trust them, but we need them and we need to somehow work with them. Right. You know, to create this better future. So I don't know if I answered your question. I mean, I went around it in several ways, but I think we're on the same wavelength. And as far as people maintain that skepticism about institutions, which they should, we need to craft policy with that in mind. The fact that we cannot trust any institution completely and therefore we need these checks and balances. And some of these solutions, by the way, will come from technology itself. Right. Technology is advancing. Right. There are tremendous strides in encryption and, you know, new kinds of technologies that might actually work towards, you know, preserving privacy. Remember that we're always working with old technologies that were adapting to new phenomena that we're confronting. And, you know, and so we're dealing with a whole new world using old technology. The optimist in me says that the technology will itself provide some of those solutions that preserve this delicate balance we need in a healthy democracy, you know, between individuals and state overreach and corporate overreach. And these are the three sort of legs of a healthy democracy. And we need these checks and balances, you know, between them. You know, I mean, Raghuram Rajan has this great book, you know, where he talks about these institutions. And I'm looking at the world the same way, but in terms of sort of the information and control and checks and balances on each other, you know, in this new era of data and AI."
    },
    {
        "speaker": "Amit Varma",
        "time": "01:55:41.080",
        "message": "That's fascinating and insightful, but you didn't answer my question, but I'll take you back to that and ask a more pointed version of it. But before that, I'll actually take issue with what you said about Americans' distrust of the state. I think a lot of that is rhetorical, because even if, you know, some of them want to have guns, all of them do trust the currency which is given out by the state. It's not like everyone is stashing Bitcoin or alternative currencies the way they are stashing guns at home or they're using the barter system. So there's an inherent trust of the state in everything they do. And in fact, you know, if you just look at the politics, you know, both parties just want to make government bigger and bigger. And as far as India is concerned, I've often said that our biggest religion is not Hinduism. It's a religion of the state, that for every single problem, we always look to the government for a solution, which is a bit of a paradox, because, you know, everybody accepts that the government is dysfunctional and can do nothing properly. And yet for every problem, the government is suddenly a solution. But to get back to my question, my question was really about the balance between individual autonomy and state coercion for supposedly good ends. And the position that I took there was on the side of individual autonomy and saying that there is a line you cannot cross there, no matter how good the ends might be. And to me, which is why I oppose the imposition of Aadhaar, that I don't even want to enter the debate about the technology, whether it's good or it's bad. The point is, it was imposed on us, and that is a problem to me."
    },
    {
        "speaker": "Guest",
        "time": "01:57:05.800",
        "message": "So what is your feeling of that? My feeling on that is that even though it was imposed, it was probably a good thing. So there are certain things on which we just can't run a counterfactual experiment, right? So it would've been great if we had had a parallel India without Aadhaar and seen how things have proceeded there, right? I suspect that the difference would have been that after Aadhaar, a lot of people got identity. And so what are the benefits of 600 million people suddenly getting identity, right? Can you calculate the benefit associated with that versus the cost of imposition? And my intuition tells me, and I have no data to prove this, that that was actually a good bargain. That is, it's probably in the aggregate been a good thing, even though it was imposed. Now, the reason I say it was a good thing, even though it was imposed, is because of its scope, right? The trouble with surveillance systems is that they have no defined scope very often, right? I was watching the movie Snowden last night, right, since we're talking about state overreach, right? And that was his problem with the CIA and the NSA, is that they're just like collecting data on everyone, right? Phone records to Verizon, they were like just tapped into Verizon servers, right? But that's not right. I mean, there's gotta be, like, who agreed to that? Do the people agree to that? No. Should they have been consulted for something like that? Maybe yes, right? Now, in the movie Snowden, you know, the scene there where I forget the guy who he's talking about the fact that there are some things you need to do that you can't broadcast to the rest of the world, because the whole point of doing them is that you need some sort of secrecy, right, for those things. And so a state has to sort of make that decision that in this particular case, I think we need to keep this under wraps. And so yeah, the court orders will be issued by, you know, judges that are not in the public sphere, that this is done by secret courts, and it's an unusual situation. So the state reserves the right to do something like that, right? It's a very delicate question as to, you know, what is that situation where it's okay to assume that power, because you're trying to address a specific question that's posing a risk or a threat to society, right? So in the Snowden case, he said, look, I just felt that it was morally wrong to be collecting data without any kind of purpose, right, with, you know, just in a completely unbridled way. And therefore, he decided to reveal this and become a whistleblower. But that's a tricky question to answer. And since we started with Aadhaar, you know, the nice thing about that is that it doesn't have this sort of scope creep. It's designed for something very specific. And people even complained that it shouldn't be used as an identity card to, you know, shouldn't be a required form of identification for non-government kinds of purposes, which also makes sense to me, because that's not what it was designed for, right? So that was an example of creep. And the Indian court struck that down correctly, in my view, right, that it should be used for the purpose for which it's designed. So as long as that's the case, I feel, you know, comfortable that it was, yes, it was imposed, but it was imposed with a particular goal in mind. And the question is, you know, was that achieved? And now we can argue about the extent to which it was achieved. Unfortunately, we don't have the counterfactual, we can't do an A-B test and show it was better. But my sense is that it's led to a lot of good things. And that on balance, it's probably been a good thing for India."
    },
    {
        "speaker": "Amit Varma",
        "time": "02:01:06.400",
        "message": "I won't, I won't litigate this further. But let's, let's kind of move on. And let's move on to another fascinating question that you have answered, and that your personal experiences have kind of a lot to do with like, in the 90s, one of the things that you did, which fascinates me, and I want to know more about that, is you started a hedge fund called SET Capital, where you used, you know, artificial intelligence to make investments. And you've spoken in the past about how that experience over the next few years running that hedge fund and running AI to, you know, invest in markets gave you insight into that larger question on which you've spoken at great length, which is, when should we trust machines? And, and, you know, one of the interesting points that you made about that period of time when you were running that hedge fund, was that when you often compared what the algorithms were doing compared to, you know, what you guys were doing, the algorithms would often outperform you. So one of the very early learnings from that is that humans are fallible and more fallible, perhaps than humans realize. In fact, there's a lovely quote by you, which is so memorable, where you said that quote, models of men tend to be better than men, stop quote, which I absolutely love. So now my question is, given this, given that humans are fallible, given that artificial intelligence can do all these things like not get lost in the neighborhood, thanks to GPS, that given that AI can do all of these things in practically every domain in which we are interested, you know, when should we trust machines? How should we think about AI? What are we looking for when we see decision making to AI, for example?"
    },
    {
        "speaker": "Guest",
        "time": "02:02:39.400",
        "message": "Yeah. So, you know, that's a fascinating question. And sort of at the heart of one of my core questions, which is, you know, when is X plus Y better than X, where X is machine, Y is human, right? So when is human plus machine better than human, right? To me, that's a fascinating question. And, you know, Kasparov did this experiment with chess, you know, we keep coming back to chess, you know, where he said, humans plus machines outperform machines, right? Which makes a lot of sense to me, you know, for chess, because, you know, human grandmasters have a tremendous knowledge about the game of chess, tremendous experience. And now they're working with a tool that is in some ways even more amazing than them, right? That can search the space in ways that they can't even think of, come up with moves, and then they can say, yeah, like, that's a good move. Or no, I think I have a better one, right? But if I've got a tool that's like a really good machine, then as a human, I'll probably just let the machine do most of the stuff. But occasionally, I might do something as a human, right? But I'm willing to believe that for something like chess, that x plus y is better than x."
    },
    {
        "speaker": "Amit Varma",
        "time": "02:03:59.520",
        "message": "If I might say so, if I might interject there, Kasparov was right then, but I don't think even he would believe that now, because now it is a case that y is better than that x doesn't matter, because now machines on their own would just wipe out any combination of human and machine, because I think the human would just get in the way, but he was right"
    },
    {
        "speaker": "Guest",
        "time": "02:04:17.280",
        "message": "then. He was right then. So in the degenerate case, the human does nothing, right? So in a sense, human plus machine will not be worse than the machine as long as the human just keeps out of the way, right? But yeah, I was willing to believe it then, right? But the reason it didn't, Kasparov's assertion didn't resonate with me was because 20 years ago, I'd done a similar experiment in finance, right? So we used to run a machine, a different machine than what we run now, and I did an experiment. I persuaded the person at Deutsche Bank who was allocating capital to let me do an experiment to see if we could actually do better, right? Because very often we would say things like, oh, the machine wants to buy bonds tomorrow, and we know that there's a meeting of the FOMC, and they're going to raise rates. Like what the hell? This makes no sense. They're going to raise rates, and it wants to buy bonds tomorrow, it should be selling. So we did an experiment, there were six of us, where we had a little budget where we could actually override the machine's decisions. And me being the head of the group, I don't look at the markets on a granular basis. I look at them occasionally just to see what's going on, but I don't really look at them because it's just noise to me. Whereas my trader, I had a human trader, and she was the first to go face down. So she's watching the markets all day, and so she would intervene more often. And essentially the results of my experiment were that we all did worse than the machine. Now it doesn't necessarily have to be the case always, but to me that was an interesting lesson, which was like, don't mess with the machine, you're just going to do worse. And certainly most of the experiences I've had since then, when we've cut risk or stuff like that for reasons, maybe the client wants to cut risk, or you just think that the market's riskier than you think. It invariably turns out to be the case that you have two left feet. So to me, the interesting question isn't whether humans always do better than machines, but when they do better. And to me, that's one of the open questions. And I often assert that in finance, which is such a noisy kind of space, that you as a human just mess with it. If you trust the math and you trust the statistics, then just stay with it because that's what you've designed. And now you're just trying to get cute and think you're smarter than the machine, which is actually, yeah, you're an intelligent person, but that doesn't say anything about your ability to forecast markets. So for a problem like that, I'd say, I'm not willing to believe Kasparov's assertion, but with something like healthcare, like let's say cancer, which is a fascinating area now because machines are becoming so much better at seeing, to imaging. I'm not willing to sort of throw my life in the hands of a machine just yet, right? Because I don't think that machines and healthcare are sufficiently intelligent that I should trust them. I still want that human oversight. I still want that expert who has tons of experience. And even though that expert might've looked at, I don't know, 3000 images in their lifetime, as opposed to the computer that's seen 3 million, and therefore has an advantage in terms of how it looks at images. There is that gestalt that humans make, for lack of a better word, that lets them do better than just what the data is telling them. It lets them invoke experiences and say, oh yeah, this is, you know, all this is good. But I remember seeing these cases, you know, two years ago where something just didn't quite fit. And therefore I want to do another test or I want to probe further, you know, I'm not willing to trust the machine on this one, right? So in healthcare, we're still in that phase where we need humans, you know, for the most part where we're not willing to trust the machine. Same thing with driverless cars, you know, I told you, like, you know, machines actually see images better than humans do, you know, that they're better at recognition in many ways that humans are, but humans just have that ability, right? So, you know, when I was talking to that individual, the scientist from one of these driverless car companies who told me you'll die five, six times a year if you let it go on autopilot, what he told me was that it's not that the machine doesn't see as well as humans, it actually does. But humans just invoke this other level of intelligence when they're driving, right? That if you see a shadow move in the periphery of your eye, you know, you recognize, hey, maybe that's an animal, you know, maybe that's another car that I haven't seen yet or something like that, right? You just sort of have this alarm that goes off in your head that driverless cars don't at the moment, right? I mean, I went through this driverless car experience a couple of years ago, where on certain turns, it would like go really fast, and then suddenly stop and it's all car in front of it. Whereas I as a human, you know, I'm taking the turn anticipating that I might actually see something in front of me. So I'm taking it slower, right? So there's all this human intelligence that we have that we use to our advantage. And even though our, you know, machinery out there in terms of sort of processing power is a fraction of what computers have, somehow, it enables us to do some pretty amazing stuff, you know, where we invoke this deeper level of the model of the world that we have on demand when we need to, you know, that machines are unable to do at the moment, right? And so that's why we don't really trust them in those situations, because the cost of error is too high, right? The cost of error is low, then we trust them, right? So in the hedge fund example, you know, if I can keep the risk associated with every position low, then even if that position blows up, I don't really care, I only had a little bit of risk allocated to it. And so that's what I realized that, you know, in the finance world, even though the machine is wrong, almost half the time, right, the error consequences are smaller, so I'm willing to believe it and trust the math. But whereas in healthcare, you know, I might die or, you know, a driverless car might make a mistake. And that error is costly. In fact, I'm dealing with a situation, you know, where a good friend of mine actually has, you know, is in an advanced stage of a certain kind of cancer, it's quite likely it's because it was diagnosed as a false negative in 2014. You know, I remember this, you know, because I was, you know, at a famous hospital in India, which I will not name, you know, but they said, yeah, you're fine, right? And, you know, and this cancer spread and it was a high cost of error. It's a really costly false negative, right? Now in this case, the false negative was actually a human, right, but we excuse humans for these mistakes, right? We're not perfect. And we excuse humans for these kinds of mistakes, even when they're severe, right? Will we excuse machines for the similar kinds of mistakes? No. We subject machines to a much higher standard when it comes to these kinds of decisions, right? And yes, machines will make mistakes as well, but we want to make sure that they don't happen very often and the consequences are not severe. So that's how I look at the world of trust with machines and at the moment, and so it's a fascinating question as to, you know, when we trust machines, when we don't trust machines and we're in a state of flux right now where machines are getting better. But in general, the tendency is towards trusting machines more with problems as we get more data, as we get more comfortable with their prowess in terms of predictability. And as we get more comfortable that they're not going to make these errors that will just kill us or cause, you know, really severe damage in something, right? That's the thing that we are aware of that determine how much we let the machine control decision-making."
    },
    {
        "speaker": "Amit Varma",
        "time": "02:12:18.380",
        "message": "In fact, I was quite taken by your term that, you know, you used in this regard, I think what you call the predictability spectrum. And that took me back to another mind sport that this time I actually played professionally for a while, which is poker, where you typically calculate the expected value or the EV of every decision, which would depend on the frequency of something happening on one axis and on the other axis, the cost, and therefore you determine the EV and you take the action. And what you are sort of postulating as far as trust in machines is concerned is that for one, you have to think about how predictable it is, but the other, you also think about, you know, the cost of going wrong. For example, you point out that, you know, autonomous cars might be much more predictable and reliable than say day trading machines, but the mistakes that a day trading machine can make can be said to be much smaller and can be limited, whereas an autonomous car failing could just run over five kids near a school and the, you know, that cost is huge, which kind of makes a lot of sense to me, that kind of spectrum that as you move towards greater reliability and the lower cost and the EV, so to say, of the wrong decision kind of goes down or the expected cost, then you can, you know, your trust can go up accordingly, except it as a rational way of thinking about it. And what many humans tend to do when it comes to AI is also react at a psychological level. For example, if autonomous cars one day reach the level which they might already have, I'm not sure what the state of the art is, but if they reach a level where autonomous cars would lead to one 10th of the fatalities in a given year, as is the case now, people would still object to it, because that one 10th would be the scene effect. And you know, the 10 times, all the people who haven't died would be unseen. And therefore, you'd build stories around the people who have died that so and so was outside a school and a car went over him, it would be hard to make the aggregate argument that look overall, we are all much safer because of it. And people often, you know, expect technology to be a panacea, like I remember back in, I used to be a cricket journalist as well. And I was one of the earliest proponents of Hawkeye, the technology which came in in the early 2000s, and the use of technology as an aid to umpires. And the arguments against were always were that they weren't perfect. And my point was, they don't have to be perfect. If they can take your decision making accuracy, from you know, umpire alone being right 93% of the time to umpire plus tech being right 97% of the time, that's fine with me. But it's a human tendency to focus on the 3% that goes wrong the kid who dies in front of school because Tesla went out of control or whatever. So you know, what do you feel about this aspect of it and the way that we react with suspicion to machines?"
    },
    {
        "speaker": "Guest",
        "time": "02:15:02.420",
        "message": "You know, you said it really well, you talked about expected value, which is kind of when you put it all together, you know, what's your expected value of accidents, right? Did it go from 7% to 1%? That's great, right? So on that metric, you're doing really well. The other thing you said, and I don't know whether use the term was that that worst case, right? And so that's what I try and bring attention to that it's not the expected value alone that matters. It does matter, right? You do want a better expected value. But what you care much more about is that worst case scenario, right? That's what you really care about, because that can like wipe you out, right? And you're not willing to do that, right? So in our case, you know, we don't know what these worst case scenarios are yet with driverless cars, which is why we have this feeling of real discomfort, right? And so we're trying them out in the wild and all of that, you know, personally, I feel like the leader in driverless cars will probably be Amazon, because, you know, I think that, you know, deliveries will get automated first, right? Because the cost of error there is lower, right? You don't have humans sitting around. So you, you know, bash up a little vehicle that's moving at 10 miles an hour and the groceries that were in it, not a big deal, right? So I suspect that we will learn about these errors of autonomous vehicles by actually putting them in the wild. But we're not going to put them in the wild right away, you know, like driving at, you know, 70 miles an hour on the highway and, you know, aggressively in cities and stuff like that, that, you know, that ain't happening, right? What we're going to see is a much more cautious introduction. And my prediction is that, you know, if you were asked me, who do you think will be the driver, the winner? I think it'll be Amazon. They've already acquired, you know, an autonomous vehicle maker, you know, quietly. And you know, with, you know, by the way, with the pandemic, I see these guys delivering groceries in Manhattan, you know, on, on bicycles, right? You see these guys, you know, with trays behind them and they're delivering groceries all over Manhattan, right? I can just imagine a little vehicle, you know, chugging along at 10 miles an hour that, you know, sends you an SMS on your phone saying, hey, I'm going to be at, you know, Bleeker and LaGuardia at 1033, or at some designated spot where you pick up your stuff, you know, and you go pick it up, right? That kind of scenario is much more likely to happen where we sort of gradually get comfortable with the cost of errors. And now an insurance industry can develop around it, right? But you have an idea, right? Because, you know, insurance would come out of the blue and develop because we have actuaries that count how many accidents happen and what the severity is, you know? And you know, in the U.S. we have these laws around auto insurance, which are no fault laws. A lot of states have that where, you know, you have to settle up regardless of who's at fault. And these laws are sort of really retarded when you think of them because they're based on the assumption of no data being available, right? That it's impossible to establish fault, right? Two years ago, I was sitting in my car in Union Square and some Escalade came and like slammed into me, right? I thought there was an explosion, right? And he said, oh, sorry, man, I fell asleep, right? And you know, the cops came, like a whole new, like a whole case around it. But I thought to myself that, and by the way, his insurance company didn't pay up, right? Because they couldn't prove conclusively that it was his fault, right? Now imagine in today's era, right, you know, you've got a sensor inside the car, outside the car, like everything is visible, right? If you could establish fault unequivocally, would you still need no fault laws? No. You know exactly who committed the error, right? Laws will change, right? So a lot of our assumptions, a lot of our laws, a lot of our practices are outdated and they're based on no data being available. When we start getting data, we'll move towards a more rational and more efficient situation all around where things become evidence-based. So yeah, driverless cars right now, too high cost of error, but over time we'll reduce that cost of error because we'll know the kinds of errors that they make, we'll correct them to the extent possible, and we're going to be left with some residual errors that we'll be comfortable with. And it'll be a brainless proposition, right? Where deaths go from, you know, 4 million to 40, you know, I mean, something that stark. And we're in a situation where even when those 40 things happen, they weren't crazy violations and they can be covered, you know, comfortably by insurance because we now know so much more than we do now."
    },
    {
        "speaker": "Amit Varma",
        "time": "02:19:53.860",
        "message": "I think Jane and Gene must be really chilling listening to this because they're like these two guys are just, you know, just talking randomly, it's no threat to us. I've taken a lot of your time, so I'm going to kind of go with three final lines of inquiry, though all of them are kind of broad and could go into interesting directions. But here's the first of them, and it comes from a Stephen Hawking quote. And again, I discovered the quote through one of your articles, and it leads to my larger question. And his quote is, whereas the short term impact of AI depends on who controls it, the long term impact depends on whether it can be controlled at all, stop quote. And this leads me to the paperclip problem of Nick Bostrom, which, for the sake of my listeners, I'll quickly read out Nick Bostrom's description of the paperclip problem, where he says, quote, suppose we have an AI whose only goal is to make as many paperclips as possible. The AI will realize quickly that it would be much better if there were no humans, because humans might decide to switch it off. Because if humans do so, there would be fewer paperclips. Also human bodies contain a lot of atoms that could be made into paperclips. The future that the AI would be trying to gear towards would be one in which there were a lot of paperclips, but no humans, stop quote, which is quite a delightful vision of an interesting sort of dystopian future. But a lot of people have recently expressed their worries about AI becoming powerful and almost in a sense becoming sentient and having interests of his own, which it will of course rationally pursue, and where that leaves us. What do you feel about this? I mean, do you think that AI is not just an aid to making our lives better, but also a threat to us in direct ways in and of themselves, not just in how they, you know, accentuate the worst aspects of humanity?"
    },
    {
        "speaker": "Guest",
        "time": "02:21:37.740",
        "message": "Ooh, big, big, big question, this one. And both great examples, right? Hawking was obviously concerned about whether technology can be controlled at all. And it's a great question, right? Whether AI is controllable at all. And, you know, next example points to a larger problem that has also been discussed. I mean, there are these things called Asimov's laws, you know, which haven't, you know, which are like really interesting that have to do with, you know, robots cannot, you know, harm a human. Right. And, you know, but when you think about it, there are all kinds of problems with, you know, Asimov's laws, which is, you know, what if it causes like a little harm in the short term or longer term, it leads to tremendous benefits, right? So like, so the Asimov's laws are sort of somewhat simplistic in that sense. The harm, like, what do you mean by harm? Like, you know, because, you know, tough love can be viewed as harm too, right? I'm going to be tough on you, you know, but it's out of love, because believe me, in the longer run, things will be a lot better for you, right? And Asimov's laws sort of ignore tough love and those kinds of situations. And you know, Nick's example actually is somewhat related to my example earlier on where I was talking about the fact that these AI machines are driven by objective functions, you know, and those objective functions can lead to behavior that has unintended consequences, right? You say, well, you know, I didn't realize that this was going to happen. And so there was an unintended consequence. You know, now being the optimist that I am, I believe that, you know, human beings are smart enough and that we will put adequate safeguards into systems where we let machines make decisions, right? Now, I say that with complete humility, because I realize that, you know, a machine might\u2014unknown to humans\u2014decide to do something that we had not envisioned, and one of those things would be that it just disables the switch, right? For whatever reason, right? And that's the kind of dystopian scenario that I worry about, and I really don't have an answer to, you know. So you know, being an optimist, I believe, yeah, we'll figure these things out, right? We'll put adequate safeguards around it, you know, we'll make sure that this thing just doesn't run amok. But the larger philosophical question remains, and I don't have an answer to that. But I wish I did, which is that, yeah, you do get a machine that becomes sentient, sufficiently sentient, and that it disables the switch. And by the way, this, you know, I don't know about you, but sometimes I press the off switch on my computer and it doesn't respond, you know, it just doesn't turn itself off. And those situations always remind me of this dystopian scenario that you've just painted, which is, sorry, pal, like, yeah, I know you thought you had control, but you know, I took charge a few years ago of that switch and, you know, I ain't letting go."
    },
    {
        "speaker": "Amit Varma",
        "time": "02:25:13.740",
        "message": "This is really scary, because now I don't know whether I'm in a conversation with Vasanthar or Vasanthar's computer, who's really in charge here. My next sort of broad question here actually follows on from this, which regards, you know, you know, what you guys call artificial general intelligence. Now for the benefit of my listeners, a quick sort of primer that artificial special intelligence or narrow AI, as it is also called, and Vasanth can correct me if I'm using these terms in a wrong way, would refer to narrow functions in which computers have already surpassed us. For example, a calculator will calculate numbers much better than I can, or the GPS app on my phone will, you know, be able to find its way around the city much better than I can and so on. So in terms of ASI or artificial special intelligence or narrow AI, we are way behind. Now, there is also talk by AI experts of what is artificial general intelligence, which is almost like a holy grail, which is, I suppose, an analog of human consciousness, that when a machine gets that kind of self awareness, that you can say it has something akin or analogous to human consciousness, in which case it immediately becomes superhuman because in all of those specialized functions, it is already way ahead of us. So my questions would be that, one, how far away do you think this is from happening? Two, is it something that we should be worried about? And three, the interesting ethical question that then comes up is that once a machine achieves AGI and is therefore clearly superior to us in that aspect, and we are just sort of, you know, a moist machine, as the phrase goes, you know, then what gives us ethical superiority over the machine, like what is to stop that machine from saying that I will not be your slave that is ethically wrong, I'm superior to you, you will be my slave. And is there a plausible case we can make against that? Does our special status as ethical beings, then rest on not our sort of cognitive faculties or our consciousness of ourselves, but on merely being flesh and blood? Is this stuff you've thought about?"
    },
    {
        "speaker": "Guest",
        "time": "02:27:20.060",
        "message": "Yeah, I have. It's a fascinating question. And you know, I was thinking of something that you said, which is consciousness, right? And you know, one of the sort of philosophical questions that some of my philosopher colleagues much better to ask is like, you know, can machines achieve consciousness? I don't know the answer to that question. If they can achieve consciousness, then you're right, right? Then you have like this conscious entity that's clearly superior in that, you know, it knows the math of things better than we do. It does number crunching better than we do. And it also has this general intelligence. So yeah, in a scenario where machines achieve consciousness, you know, I could see that as being a relevant question. I just don't know whether machines can achieve consciousness and what that even means. With regard to your slightly easier question or easier part of the question, which is around artificial general intelligence, right? So AGI is like a construct we've created to distinguish like intelligence that's of a more general type than like specific problem solving kinds of capabilities. And it has several forms, but to me, the most important form of it is common sense, right? That is much of human AGI, if you want to call it that, like if you want to like just put a simple term on AGI, it's common sense, right? That is, we just have a lot of common sense, right? Now how do we learn that? Well, that's a very fertile area of AI at the moment, right? Now, and it reminds me of like in 1989, 1990, I spent some time in this AI research lab in Austin, Texas called MCC. Now one of the big groups there was building the system called Psych CYC, and it was supposed to have common sense knowledge. Now some people thought that was a visionary project. I just thought it was misguided and it didn't make any sense. I mean, that's what I felt because people were trying to teach the machine common sense, like top down, right? Remember, I talked about the paradigm of logic dominating the early days of AI, right? So now people were saying, well, you know, you go to a restaurant, well, it's customary that you pay a tip. Well, that's common sense, right? No, that's not common sense, right? It's something that you as a human kind of observed over and over again, and then just started doing it, right? And there's nothing logical about it, right? It was just acquired over time. And now that's one part of artificial general intelligence, right? You know, you can also think in terms of, you know, if you're walking down the street, you know, you don't expect to see projectiles popping out of the earth and flying into the sky, right? I mean, it just doesn't happen. It's not reality. So we grow up thinking that we shouldn't expect to see that, right? We grew up thinking that if you toss a ball into the air, it just falls down, right? But if you grew up in outer space, that wouldn't happen, right? So here on earth, common sense is that objects fall down on the earth, right? But you can imagine a civilization out there in outer space that lives in zero gravity, their common sense would be completely different, right? It would be more along sort of Newton's laws, which is without friction, or you send something in motion, it stays in motion forever. You know, it doesn't fall down, there's no gravity, right? The common sense there would be different. So what I'm seeing happening in AI these days is people are asking themselves, like, how do infants learn? How do babies learn, right? And the way they learn is through something called an inductive bias in the data all around us, right? There are certain things that we see often, there are certain things we see never at all, there's some things we see sometimes, and those become common sense, right? So as an infant, you can see certain things, like if a ball disappears behind an object, the infant actually expects it to come out on the other side, right? It's common sense, because that's the way objects move, right? So we've sort of taken this instead of a top-down approach to common sense, which was the old way of AI, just stuff knowledge into a system, what we're now asking ourselves is, hey, we have all this data around us, right, in terms of everything that happens ultimately is data, and we as human beings are observing this data, and we're learning from this data in this sort of biased way, like bias is actually a good thing here, right? It's biasing us towards things that make sense, and so we learn them, and then they become axiomatic for us, like just axiomatic common sense kinds of things, right? Machine has no such idea, right? It's just doing what you're calling narrow intelligence, right? But once the machine achieves a lot of this common sense ability in the same way that humans do, then machines will also have the common sense that, oh, if you toss a ball in the air, it'll fall down, right, that if an object, you know, is temporarily occluded behind another, and if it's moving, it'll appear on the other side, right? All of these things, machines will learn on their own, just like humans do, right? And at that point, we'll be closer to AGI, right? Because then we won't have to tell a machine that, you know, objects that go up fall down. It'll have that knowledge in itself, so if it needs to use it as part of something else, just like humans do, you know, it'll learn that, right? Just like we learn, you know, when you're a kid and your mother gives you that look that that wasn't the right thing to do, and you sort of learn that certain things aren't right, right? You pick up on those cues as a human that, yeah, those are positive, these are negative, right? Machines don't see the world in that same way, which is why they don't have any common sense, right? And we have the common sense because it's just acquired gradually, painstakingly from the moment we're born, and it's biased by the data we see around us, right? So the new way of AI is sort of more bottom up, where we're getting machines to learn through experience, learn through data, and then gradually they'll pick up these, you know, things that we call artificial general intelligence, and then they'll become sort of generally intelligent, right? Whether they achieve consciousness in the way that humans do, that's a great philosophical question."
    },
    {
        "speaker": "Amit Varma",
        "time": "02:33:57.780",
        "message": "I'm just imagining a future in which a machine is at a restaurant and is tipping the waiter, I don't even know if that's utopian or dystopian. But so I should technically have one more question for you, but I'll throw in another one just before that, because I realized that, you know, your the first episode of your podcast was just out, and you have a bunch of other great episodes lined up and, you know, because of my privileged position of helping you with the show, I'm privy to some of the great guests you're having on, but I'd like you to tell my listeners a little bit more about Brave New World, your podcast, what are you trying to, obviously, it's driven by your curiosity and your passion towards this field. So what are the kind of areas you're planning to explore, and so on and so forth?"
    },
    {
        "speaker": "Guest",
        "time": "02:34:39.580",
        "message": "Tell us a little bit. So it's a very broad canvas. I'm beginning with the impacts of COVID on humanity. And a lot of these impacts have been mediated by technology. And so my goal is to look at this interplay of technology and post-COVID society and see where it's taking us. So what's this post-COVID humanity where tech plays a much greater role in our virtual lives, virtual, I mean, we don't see each other that much, we don't touch, we're sort of in this Asimov kind of world of the naked sun. What's humanity going to look like in this era? And to me, that's a fascinating question because COVID has been a discontinuity in humanity. Tech was sort of creeping into our lives gradually, but now it's just accelerated, and it's turbocharged, and it's causing all these transformations in humanity. So my objective is to look at these across all areas of our lives, whether it's how we work, how we stay healthy, how we travel, how we communicate, how we exercise our spirituality, how machines are accelerating in terms of their ability to solve certain kinds of health problems, cancers, how society is being transformed, like some of the issues we discussed in terms of social media, how should we think in terms of governing this brave new internet world that we're entering. So those are the questions that I'm exploring, and I'm surrounded by people who are a lot smarter than myself and who have such deep knowledge in these areas that I find it just fascinating to talk to them. So my objective is to explore these areas with thought leaders in these areas, people who've really thought about these different aspects of humanity, and just let them tell us and explore, have a conversation with them, where I largely stay out of the way and steer the conversation in the direction that will be of interest to people in general. So the audience of this podcast is global, and it's everyone, young, old, everyone. So it's a podcast for everyone. And I'm kicking this off with some people I know really well, some of my colleagues at NYU, Arun Sundararajan, who's been a leader in the sharing economy, I'm talking to him about just what's happening in the US-China war with these platforms. I have my colleague and friend Scott Galloway, who's just an amazing thinker in this space. He's just come up with a book recently about just post-COVID impacts of tech on society. I'm going to talk to him about education. It's an area that both he and I are sort of intimately familiar with, and how the whole education complex is likely to get transformed in the next decade. I have Sinan Aral coming on, who is a professor at MIT, a former colleague of mine who just came out with this book called The Hype Machine, fascinating book that I highly recommend. It's basically a book where he tells a lot of stories, but they're fascinating stories, and they illustrate certain concepts. So it's basically science done really well, integrated and told through stories. So fascinating book. And then after that, I have some other guests lined up. I have Eric Topol, who's a leading authority in AI and cancer. And then I have John Sexton, who was the ex-president of NYU, who I want to have and talk about law and just the role of data and evidence in the legal sphere. And then I'm going to have some of my colleagues from the Center for Data Science, some people who are really kind of out there at the frontiers of AI, just talking about what's the interesting stuff happening in AI. I want to have some spiritual leaders, people with large followings, and talk to them about what's the impact of COVID been on the emotional and spiritual aspects of our lives, and how's that changed the way you interact with your followers. So it's a wide open canvas. And my objective is to get some really interesting people who've thought about these issues and who have, you know, something interesting to share with the rest of the world at this intersection of sort of post-COVID humanity and technology and where it's taking us."
    },
    {
        "speaker": "Amit Varma",
        "time": "02:39:30.020",
        "message": "That's fabulous. I can't wait to listen to all those episodes as the show unfolds. I wish they were already done so I could just binge on them right now. Now I'll have to wait. For all my listeners, the show is called Brave New World, hosted by Vasant Har. So you can just search for that on all podcast apps. It's available for free on all podcast apps, and you can go to bravenewpodcast.com. Now my final question for you is, you know, I have a tradition of asking my guests on whatever the subject they are talking about is, you know, looking into the future, what gives you hope and what gives you despair. And in the context of what we are speaking about AI technology, the future and so on, I'll rephrase the question as what is likely to keep you up at night out of worry? And what is likely to keep you up at night out of anticipation?"
    },
    {
        "speaker": "Guest",
        "time": "02:40:20.740",
        "message": "Yes. So, you know, I, you know, being an optimist, I guess my I'm a little stunted emotionally in that I don't worry enough about those, you know, dystopian states of affairs. Maybe I should. But the things that worry me, I've sort of, you know, I've written about them. And, you know, I think that we've gone a little askew in terms of technology and its role in our lives in some ways. I think we just need to self-correct a little bit, just think a little more deeply about where technology is taking us, because some of the areas in which it's taking us don't seem to be particularly good for us. There's sort of always this dark side of humanity, right? And tech is neutral, so it can also be, you know, it can also take us in those directions. So if there's anything that keeps me up, it's that, right, that I know humanity and I'm not, you know, the many aspects of it that are ugly and that aren't going to go away just as part of human beings. And it's almost like, you know, worrying about this ugly side of us taking over and technology helping this ugly side as opposed to the good side, you know. And being an optimist, I don't worry enough about that, right? But to me, it's just, you know, I've had a great life. And I just imagine, like, if I were around for another 60 years, like, Jesus, wouldn't that be amazing? Like, you know, even if I think of, like, what the world is going to look like in 10 years, I think it's going to be pretty darn amazing, right? I believe that, you know, we will have solved a lot of big problems, we will solve a lot of health care problems, we need to address societal problems, because that is probably one of our biggest hurdles right now. You know, I'm, I'm very deeply connected to the US. I'm also very deeply connected with India. There are things I love about these societies, but there are things that are just so ugly about both of them that worry me and that bother me. I won't go into them at the moment, but they are, but they're there. And I feel that we have an ability to solve a lot of these problems to create a better society through technology. I really believe that we do, but we also have an ability to do just the opposite. And if there's one thing that keeps me up at night, that's what it is, that I realized that we can actually do some pretty nasty things and create an ugly society. I don't think we'll do that, but we certainly have the ability to do that. But I just marvel at the kinds of things that are coming down the road. You know, when I look back at sort of the previous 60 years, you know, it's been one hell of a ride, you know, for the most part, extremely positive and exhilarating, right? And I'm a big believer in tech, and I don't believe that the future will be different in that sense. I think the innovation will continue, it'll even accelerate, right? It's sort of a, and even though we're, you know, I said, we're still in the bronze age of intelligence, you know, I think that'll accelerate and we're going to see some pretty amazing stuff appear, you know, where we're able to invoke the machine and have it do all kinds of things on demand, you know, that we couldn't even dream of, you know, at the moment. You know, I, you know, I was talking to someone about, you know, when I first started using computers and IIT, it was to solve engineering problems that you couldn't solve, you know, in closed form, right? So you have to like solve them through iteration. And I was just telling someone that, you know, I used to write the program on punched cards and then I would put it in the cubby hole and then come back the next day and there'd be an output saying, you know, compilation error in line seven, you know, literal parents not recognized or something like that, right? And I'd correct that, repunch that card, insert it, put it in there, and next day I'd come back and be an error saying, you know, program compiled execution error in line 33, array out of bounds or something like that. And I'd say, oh, of course, you know, I should, you know, I should, my termination condition needs to be better. I'd rewrite it, put it in there, come back the next day. And maybe after three or four days, I'd see an output, right? And I'm talking like I was what, 19, 20, 21 at the time, right? And in graduate school, there was a first screen editor that came out. So I could actually edit the file on the screen, run it and solve the same problem that took me three days in three minutes, right? Now stuff that takes, used to take me three minutes, takes 0.3 seconds and it's done so much better, faster. I just, you know, in the future, I'll just have to imagine it and it'll be done. So it's going to be a fascinating, brave new world in my estimation. I can't even envision what it's going to look like, but it's going to be pretty darn amazing."
    },
    {
        "speaker": "Amit Varma",
        "time": "02:45:16.900",
        "message": "Wow. Thanks so much. You know, I'm optimistic like you. And one of the things that makes me optimistic is that we can have conversations like this, you know, to people like us across an ocean for, you know, all the tens of thousands of people who listen. So thank you so much for coming on the show and, you know, for your time and insights and best of luck with Brave New World."
    },
    {
        "speaker": "Guest",
        "time": "02:45:37.860",
        "message": "Thank you. Thank you for having me on your show. It was a fascinating conversation. You asked some really good questions, some tough ones, and I know there's one out there that you're not convinced about, but thanks so much was a really engaging conversation. The time just flew by. So thanks again."
    },
    {
        "speaker": "Amit Varma",
        "time": "02:45:57.140",
        "message": "If you enjoyed listening to this episode, do head on over to brave new podcast.com or your favorite podcast app and subscribe to Brave New World hosted by Vasanthar. You can follow him on Twitter at Vasanthar. You can follow me at Amit Verma, A-M-I-T-V-A-R-M-A. You can browse past episodes of The Seen and the Unseen at seenunseen.in. That at least is one thing which is future proof. Thank you for listening. Did you enjoy this episode of The Seen and the Unseen? If so, would you like to support the production of the show? You can go over to seenunseen.in slash support and contribute any amount you like to keep this podcast alive and kicking. Thank you."
    }
]