WEBVTT

00:00:00.000 --> 00:00:10.240
[Amit Varma]: The very fact that you're listening to my voice right now is a bloody miracle.

00:00:10.240 --> 00:00:15.080
[Amit Varma]: When I was a kid growing up in pre-liberalization India, I would never have thought that I can

00:00:15.080 --> 00:00:20.280
[Amit Varma]: speak into a mic in my room to someone thousands of miles away and be heard the next day by

00:00:20.280 --> 00:00:22.320
[Amit Varma]: hundreds of thousands of people.

00:00:22.320 --> 00:00:24.400
[Amit Varma]: What manner of wizardry is this?

00:00:24.400 --> 00:00:28.760
[Amit Varma]: Almost every bit of technology that I use today would have seemed like magic to me in

00:00:28.760 --> 00:00:30.080
[Amit Varma]: the 1980s.

00:00:30.080 --> 00:00:34.440
[Amit Varma]: Hell, I remember in 1994 when I graduated from college and moved to Delhi to work in

00:00:34.440 --> 00:00:39.000
[Amit Varma]: what was then India's largest advertising agency, there were just four computers in

00:00:39.000 --> 00:00:42.560
[Amit Varma]: the entire floor where I worked, in a special computer room.

00:00:42.560 --> 00:00:46.880
[Amit Varma]: And I remember marveling at the software I used once to type a script.

00:00:46.880 --> 00:00:48.200
[Amit Varma]: Have you heard of WordStar?

00:00:48.200 --> 00:00:52.040
[Amit Varma]: I remember moving to Bombay shortly after that, and yes, it was Bombay at that time

00:00:52.040 --> 00:00:57.340
[Amit Varma]: and not Mumbai, and dreaming about how I would one day be rich and famous and have a laptop.

00:00:57.340 --> 00:01:00.680
[Amit Varma]: It was so aspirational back in that time and so out of reach.

00:01:00.680 --> 00:01:05.240
[Amit Varma]: And even the primitive garbage that was available then was way more expensive than laptops are

00:01:05.240 --> 00:01:06.240
[Amit Varma]: today.

00:01:06.240 --> 00:01:09.440
[Amit Varma]: I remember a friend telling me about how he had something called the internet in his office

00:01:09.440 --> 00:01:14.640
[Amit Varma]: computer and you could type in anyone's name and it would show every web page in the world

00:01:14.640 --> 00:01:15.640
[Amit Varma]: made on that person.

00:01:15.640 --> 00:01:20.160
[Amit Varma]: I went to his office once, was allowed to see his computer and typed in the words Van

00:01:20.160 --> 00:01:21.280
[Amit Varma]: Morrison.

00:01:21.280 --> 00:01:25.480
[Amit Varma]: And my God, what a magical feeling it was to see the resultant web page when it did

00:01:25.480 --> 00:01:27.920
[Amit Varma]: appear after about five minutes.

00:01:27.920 --> 00:01:32.880
[Amit Varma]: Because hey, remember dial-up connections, young Indian men of my age will remember staring

00:01:32.880 --> 00:01:37.760
[Amit Varma]: at the forehead of Samantha Fox as a dial-up connection tried to reconnect.

00:01:37.760 --> 00:01:44.880
[Amit Varma]: You remember the sound that would make?

00:01:44.880 --> 00:01:47.200
[Amit Varma]: But why am I talking about this now?

00:01:47.200 --> 00:01:51.800
[Amit Varma]: It's because we normalize every technological advance as soon as it happens.

00:01:51.800 --> 00:01:54.320
[Amit Varma]: And we have lost our sense of wonder.

00:01:54.320 --> 00:01:59.480
[Amit Varma]: And so this is a good time to reconsider how far we have come in these years and how quickly

00:01:59.480 --> 00:02:01.920
[Amit Varma]: our world is changing around us.

00:02:01.920 --> 00:02:05.960
[Amit Varma]: Artificial intelligence and machine learning have already changed the way we live our lives

00:02:05.960 --> 00:02:09.320
[Amit Varma]: and will continue to transform our post-COVID world.

00:02:09.320 --> 00:02:14.880
[Amit Varma]: In fact, those of us who live to say 2040 will look back on 2020 and think, wow, we

00:02:14.880 --> 00:02:16.320
[Amit Varma]: lived such primitive lives.

00:02:16.320 --> 00:02:17.920
[Amit Varma]: We were like cave dwellers.

00:02:17.920 --> 00:02:22.200
[Amit Varma]: I keep saying that the future is full of unknown unknowns, so we can't imagine what it will

00:02:22.200 --> 00:02:25.280
[Amit Varma]: look like, but I'm often struck by curiosity.

00:02:25.280 --> 00:02:29.200
[Amit Varma]: What kind of a world will my future self inhabit?

00:02:29.200 --> 00:02:38.080
[Unknown]: Welcome to The Seen and the Unseen, our weekly podcast on economics, politics and behavioral

00:02:38.080 --> 00:02:39.080
[Unknown]: science.

00:02:39.080 --> 00:02:45.280
[Amit Varma]: Please welcome your host, Amit Bharma.

00:02:45.280 --> 00:02:46.680
[Amit Varma]: Welcome to The Seen and the Unseen.

00:02:46.680 --> 00:02:51.720
[Amit Varma]: My guest today is Vasant Dhar, an artificial intelligence researcher who is a professor

00:02:51.720 --> 00:02:56.240
[Amit Varma]: at the Stern School of Business and the Center for Data Science at New York University.

00:02:56.240 --> 00:02:58.160
[Amit Varma]: But Vasant is no mere academic.

00:02:58.160 --> 00:03:03.320
[Amit Varma]: He worked with Morgan Stanley in the 1990s, flirted with sports analytics, and even set

00:03:03.320 --> 00:03:08.960
[Amit Varma]: up a hedge fund called SCT Capital, which used machine learning to make its trades in

00:03:08.960 --> 00:03:10.240
[Amit Varma]: the 1990s.

00:03:10.240 --> 00:03:12.760
[Amit Varma]: His understanding of AI is second to none.

00:03:12.760 --> 00:03:16.120
[Amit Varma]: He's been both a practitioner and a researcher.

00:03:16.120 --> 00:03:18.480
[Amit Varma]: In other words, he's had skin in the game.

00:03:18.480 --> 00:03:22.480
[Amit Varma]: And indeed, it could be argued that we all have skin in the game now, because all of

00:03:22.480 --> 00:03:25.720
[Amit Varma]: our lives will be transformed by technology.

00:03:25.720 --> 00:03:30.840
[Amit Varma]: And it is to explore exactly this that Vasant has started an interview podcast called Brave

00:03:30.840 --> 00:03:31.840
[Amit Varma]: New World.

00:03:31.840 --> 00:03:36.320
[Amit Varma]: Vasant is a host of this new show, and we'll be having conversations with a variety of

00:03:36.320 --> 00:03:41.720
[Amit Varma]: fascinating guests, such as Scott Galloway, Richard Thaler, Eric Topol and Srinan Aral.

00:03:41.720 --> 00:03:45.760
[Amit Varma]: It's an initiative of the Data Governance Network, and I'm actually producing the show

00:03:45.760 --> 00:03:46.760
[Amit Varma]: for them.

00:03:46.760 --> 00:03:50.040
[Amit Varma]: But for the first time, I'm working on a show where I am not the host.

00:03:50.040 --> 00:03:53.200
[Amit Varma]: So I can give you the inside scoop on Brave New World.

00:03:53.200 --> 00:03:56.160
[Amit Varma]: You are going to hear some incredible conversations on it.

00:03:56.160 --> 00:03:59.620
[Amit Varma]: So do subscribe to it on your podcast app of choice.

00:03:59.620 --> 00:04:04.220
[Amit Varma]: And you can also visit its website at brave new podcast.com.

00:04:04.220 --> 00:04:05.880
[Amit Varma]: And now for our conversation.

00:04:05.880 --> 00:04:11.960
[Amit Varma]: But first, let's take a quick commercial break.

00:04:11.960 --> 00:04:16.800
[Amit Varma]: One of the things I have worked on in recent years is getting my reading habit together.

00:04:16.800 --> 00:04:18.880
[Amit Varma]: This involves making time to read books.

00:04:18.880 --> 00:04:23.600
[Amit Varma]: But it also means reading long form articles and essays, there's a world of knowledge available

00:04:23.600 --> 00:04:24.600
[Amit Varma]: through the internet.

00:04:24.600 --> 00:04:29.280
[Amit Varma]: But the big question we all face is, how do I navigate this knowledge, who will be my

00:04:29.280 --> 00:04:32.040
[Amit Varma]: guide to all the awesome writing out there?

00:04:32.040 --> 00:04:37.880
[Amit Varma]: Well, a couple of friends of mine run this awesome company called CTQ Compounds at ctqcompounds.com,

00:04:37.880 --> 00:04:43.360
[Amit Varma]: which aims to help people upgrade themselves constantly to stay relevant for the future.

00:04:43.360 --> 00:04:47.720
[Amit Varma]: A few months ago, I signed up for one of their programs called the daily reader.

00:04:47.720 --> 00:04:52.200
[Amit Varma]: Every day for six months, they sent me a long form article to read the subjects covered

00:04:52.200 --> 00:04:56.640
[Amit Varma]: went from machine learning to mythology to mental models, we even marmalade.

00:04:56.640 --> 00:04:58.900
[Amit Varma]: This helped me build a habit of reading.

00:04:58.900 --> 00:05:03.960
[Amit Varma]: At the end of every day, I understood the world a little better than I had the previous

00:05:03.960 --> 00:05:04.960
[Amit Varma]: day.

00:05:04.960 --> 00:05:08.240
[Amit Varma]: Some of the viewers of the scene in the unseen often asked me, hey, how can I build my reading

00:05:08.240 --> 00:05:10.360
[Amit Varma]: habit and upgrade my brain?

00:05:10.360 --> 00:05:11.560
[Amit Varma]: I have an answer for you.

00:05:11.560 --> 00:05:17.400
[Amit Varma]: Head on over to CTQ Compounds and check out the daily reader as well as the other activities

00:05:17.400 --> 00:05:21.960
[Amit Varma]: that will help your future self be an upgrade on your present self.

00:05:21.960 --> 00:05:25.240
[Amit Varma]: CTQ Compounds at ctqcompounds.com.

00:05:25.240 --> 00:05:28.280
[Amit Varma]: You owe this to yourself.

00:05:28.280 --> 00:05:33.400
[Guest]: Wasn't Welcome to the scene in the unseen.

00:05:33.400 --> 00:05:34.400
[Guest]: I'm delighted to be here.

00:05:34.400 --> 00:05:36.120
[Guest]: And thank you for having me on your show.

00:05:36.120 --> 00:05:41.080
[Amit Varma]: No, no, it's a great pleasure for me, partly because I've also been, you know, as I announced

00:05:41.080 --> 00:05:43.920
[Amit Varma]: in the introduction, helping you with your show Brave New World.

00:05:43.920 --> 00:05:49.840
[Amit Varma]: And I'm so excited to listen to the future episodes of that, because it's just all such

00:05:49.840 --> 00:05:50.840
[Amit Varma]: a fascinating field.

00:05:50.840 --> 00:05:55.160
[Amit Varma]: But before we kind of go into the future, I want to go back into the past a little bit.

00:05:55.160 --> 00:05:59.440
[Amit Varma]: And I want to talk about sort of your childhood, what was a young wasn't like, you know, in

00:05:59.440 --> 00:06:03.680
[Amit Varma]: one of your talks on YouTube, you began with this beautiful picture of a bridge in Kashmir.

00:06:03.680 --> 00:06:07.880
[Amit Varma]: And you wrote about how that's a bridge you went across when you went to school.

00:06:07.880 --> 00:06:12.840
[Amit Varma]: So tell me about sort of the young wasn't in your years, growing up, and what did you

00:06:12.840 --> 00:06:14.560
[Guest]: want to be when you grow up?

00:06:14.560 --> 00:06:15.840
[Guest]: You know, I have no idea.

00:06:15.840 --> 00:06:21.120
[Guest]: But you know, there's a Jerry Garcia line, you know, what a long, strange trip it's been.

00:06:21.120 --> 00:06:27.340
[Guest]: And that kind of characterizes my life, more or less, you know, when I just think of the

00:06:27.340 --> 00:06:33.560
[Guest]: changes I've seen in my lifetime, that bridge you saw in my TEDx talk was actually a bridge

00:06:33.560 --> 00:06:36.760
[Guest]: in Safa Qadal in Kashmir.

00:06:36.760 --> 00:06:41.440
[Guest]: And, you know, as you walk across that bridge, and when that bridge was under construction,

00:06:41.440 --> 00:06:45.320
[Guest]: you know, I used to take the boat, I was like four years old, with someone who used to take

00:06:45.320 --> 00:06:50.440
[Guest]: me to school, who's still alive, by the way, and I meet him in Kashmir every year.

00:06:50.440 --> 00:06:54.800
[Guest]: And you know, we reminisce about those old times, that was early 60s.

00:06:54.800 --> 00:06:57.200
[Guest]: So that's where I got started.

00:06:57.200 --> 00:07:04.000
[Guest]: And, you know, my father was in the army, which is why I was in Kashmir, a fair amount

00:07:04.000 --> 00:07:09.440
[Guest]: because he was posted to hill stations, or rather, posted to forward stations, they call

00:07:09.440 --> 00:07:11.360
[Guest]: them where families were not allowed.

00:07:11.360 --> 00:07:16.080
[Guest]: So my mom and I spent, you know, whenever he was away, we spent some time in Kashmir.

00:07:16.080 --> 00:07:18.040
[Guest]: And so that's where I started my childhood.

00:07:18.040 --> 00:07:22.160
[Guest]: But I, you know, but it was all over the place, because he was in the army, I was in Wellington,

00:07:22.160 --> 00:07:27.440
[Guest]: which is my earliest memories in life, where he was at the Defense Services Staff College.

00:07:27.440 --> 00:07:31.000
[Guest]: And so that was actually my earliest memory in life was in Wellington in South India,

00:07:31.000 --> 00:07:32.640
[Guest]: but we've always moving around.

00:07:32.640 --> 00:07:39.640
[Guest]: Yeah, and, you know, and then he got posted as a military attachÃ© to Ethiopia.

00:07:39.640 --> 00:07:47.480
[Guest]: And so we sailed from Bombay to Yemen, and, you know, then to Ethiopia and, and that was

00:07:47.480 --> 00:07:50.120
[Guest]: a real mind boggling experience for me.

00:07:50.120 --> 00:07:54.680
[Guest]: And I was, you know, not even nine, I spent three years there, you know, it was really

00:07:54.680 --> 00:07:55.680
[Guest]: formative years.

00:07:55.680 --> 00:07:59.560
[Guest]: You know, Haile Selassie was the emperor.

00:07:59.560 --> 00:08:03.480
[Guest]: And I used to get invited to his New Year's parties for diplomats kids.

00:08:03.480 --> 00:08:08.640
[Guest]: So, you know, three years, I went to meet Haile Selassie, you know, and I shook hands

00:08:08.640 --> 00:08:09.640
[Guest]: with him.

00:08:09.640 --> 00:08:14.560
[Guest]: One of those years, another year, he did a namaste, you know, one year he shook hands.

00:08:14.560 --> 00:08:20.560
[Guest]: And many, many years later, when I went to Jamaica, that really endeared me to the Rastafarians.

00:08:20.560 --> 00:08:24.120
[Guest]: Because I don't know if you know this, but they follow Haile Selassie.

00:08:24.120 --> 00:08:28.320
[Guest]: And his real name was Rastafari, which is where Rastafarian comes from, which most people

00:08:28.320 --> 00:08:29.320
[Guest]: are not aware of.

00:08:29.320 --> 00:08:34.680
[Guest]: Yeah, so it was that and then, you know, boarding school in India after that.

00:08:34.680 --> 00:08:37.920
[Guest]: And by the way, when I was in Ethiopia, my mom put me in the wrong grade, she put me

00:08:37.920 --> 00:08:43.040
[Guest]: into seventh grade instead of fourth grade, you know, I was I'd gone from third grade

00:08:43.040 --> 00:08:45.880
[Guest]: in India, and she said, Oh, you're going to standard four.

00:08:45.880 --> 00:08:51.600
[Guest]: And, you know, the kids there, you know, we'd gone to a party one day to the ambassador's

00:08:51.600 --> 00:08:55.000
[Guest]: house and they told me, Oh, you know, standard four here means seventh grade, you know, tell

00:08:55.000 --> 00:08:59.520
[Guest]: your mom that, you know, in the British schools, you know, you want to be in class two, you

00:08:59.520 --> 00:09:05.160
[Guest]: know, so on the way back home, I told my mom, I said, By the way, you know, it's not standard

00:09:05.160 --> 00:09:10.200
[Guest]: four, that's seventh grade, you know, and, you know, she just sort of in Kashmiri said,

00:09:10.200 --> 00:09:13.720
[Guest]: you know, which means, you know, shut up, you little pipsqueak, you know, you think

00:09:13.720 --> 00:09:15.240
[Guest]: you know everything.

00:09:15.240 --> 00:09:19.120
[Guest]: And the next day, I went and took the test for seventh grade, you know, and I did really

00:09:19.120 --> 00:09:22.760
[Guest]: poorly.

00:09:22.760 --> 00:09:26.840
[Guest]: And the headmaster called my dad, you know, he was a very understated British guy and

00:09:26.840 --> 00:09:30.800
[Guest]: said, you know, you show you know what you're doing, you know, and what he meant was, you

00:09:30.800 --> 00:09:34.520
[Guest]: know, your son's like nine years old, why are you putting him in seventh grade?

00:09:34.520 --> 00:09:36.440
[Guest]: And my dad completely missed it, right?

00:09:36.440 --> 00:09:41.640
[Guest]: Because, you know, in his, the headmaster in his typically understated British way said,

00:09:41.640 --> 00:09:45.480
[Guest]: you know, I think you're being a little ambitious, actually.

00:09:45.480 --> 00:09:47.120
[Guest]: And my dad just missed that.

00:09:47.120 --> 00:09:51.640
[Guest]: And so first day I walked into class, there were people who were like 15, you know, and,

00:09:51.640 --> 00:09:55.400
[Guest]: you know, whistling at an attractive 19 year old English teacher, and I told my parents

00:09:55.400 --> 00:09:56.400
[Guest]: about this.

00:09:56.400 --> 00:09:59.440
[Guest]: And instead of being worried, they were amused.

00:09:59.440 --> 00:10:03.760
[Guest]: They didn't actually realize it until, you know, I went to the airport with my dad to

00:10:03.760 --> 00:10:07.240
[Guest]: collect my brother who was coming home from boarding school.

00:10:07.240 --> 00:10:11.040
[Guest]: And there was this woman who walks by in the short skirts and heels.

00:10:11.040 --> 00:10:13.560
[Guest]: And I walked up to her and I say, Hey, Fatima, how are you doing?

00:10:13.560 --> 00:10:16.600
[Guest]: And she's like, Vasant, and we chatted and my dad said, Who is that woman?

00:10:16.600 --> 00:10:17.600
[Guest]: And I said, Well, she's in my class.

00:10:17.600 --> 00:10:18.600
[Guest]: He says, What?

00:10:18.600 --> 00:10:21.920
[Guest]: I said, Well, what do you think I've been telling you all this time?

00:10:21.920 --> 00:10:26.120
[Guest]: So, you know, those incidents have not been atypical.

00:10:26.120 --> 00:10:27.920
[Guest]: So that that was my early childhood.

00:10:27.920 --> 00:10:32.200
[Guest]: Then I went to boarding school in India, and then IIT and, you know, and then graduate

00:10:32.200 --> 00:10:33.700
[Guest]: school after that in Pittsburgh.

00:10:33.700 --> 00:10:36.640
[Guest]: So you know, which is where I got into AI.

00:10:36.640 --> 00:10:40.560
[Guest]: So that, in a nutshell, has been sort of my journey.

00:10:40.560 --> 00:10:45.200
[Guest]: So not a typical childhood, very, very unusual, but

00:10:45.200 --> 00:10:49.240
[Amit Varma]: And kind of staying with your childhood, because now I'm curious that, did you years spent

00:10:49.240 --> 00:10:53.460
[Amit Varma]: outside the country, you know, hobnobbing with the likes of Haile Selassie, did it mark

00:10:53.460 --> 00:10:56.360
[Amit Varma]: you out from the other kids when you came back, like you went to boarding school at

00:10:56.360 --> 00:11:01.680
[Amit Varma]: Lawrence School Sanhavar and all that fairly elite place at the time, but do you feel that

00:11:01.680 --> 00:11:06.680
[Amit Varma]: you were a slightly different person, one, because, you know, that you had this experience

00:11:06.680 --> 00:11:11.320
[Amit Varma]: of the world, which was beyond them, because those were not the kind of connected times

00:11:11.320 --> 00:11:15.960
[Amit Varma]: that they are, where you have easy experience to other cultures and all of that.

00:11:15.960 --> 00:11:20.800
[Amit Varma]: Did that kind of make you feel that there was one layer to you, which nobody else had

00:11:20.800 --> 00:11:21.800
[Guest]: in some ways?

00:11:21.800 --> 00:11:25.600
[Guest]: Yes, I think there was, I'm not sure, quite sure how to describe it, right?

00:11:25.600 --> 00:11:30.880
[Guest]: It's hard to describe these experiences that I had, both in terms of, you know, just like

00:11:30.880 --> 00:11:33.760
[Guest]: a completely different culture.

00:11:33.760 --> 00:11:39.040
[Guest]: And I did, you know, when you're young, you sort of immerse yourself in it so much easier.

00:11:39.040 --> 00:11:43.240
[Guest]: And so, you know, when my parents went off to, you know, diplomat parties, I'd be hanging

00:11:43.240 --> 00:11:46.080
[Guest]: out with the houseboy and the maid, and we'd talk in Amharic, right?

00:11:46.080 --> 00:11:50.480
[Guest]: So I learned Amharic, the language, and then in school, I was learning French.

00:11:50.480 --> 00:11:53.240
[Guest]: Yeah, but it was just a very unique experience.

00:11:53.240 --> 00:12:00.120
[Guest]: So the answer is, yeah, it probably did change me in some ways.

00:12:00.120 --> 00:12:01.120
[Guest]: Hard to describe how.

00:12:01.120 --> 00:12:05.760
[Guest]: And then, of course, boarding school was an entirely different trip already, you know,

00:12:05.760 --> 00:12:10.160
[Guest]: to come back in India, you know, to go from third grade to seventh grade to eighth grade

00:12:10.160 --> 00:12:12.920
[Guest]: back to sixth grade in boarding school.

00:12:12.920 --> 00:12:18.200
[Guest]: And finally be kind of, you know, somewhat, you know, have some sort of stability for

00:12:18.200 --> 00:12:22.400
[Guest]: the last six or seven years of my school.

00:12:22.400 --> 00:12:26.120
[Guest]: Before that, you know, I was in a new school every year because my father would get posted

00:12:26.120 --> 00:12:29.600
[Guest]: to, you know, Ambala or something, and I'd, you know, go to school there.

00:12:29.600 --> 00:12:35.080
[Guest]: So it was just, you know, constant change in my childhood, a lot of change, but very

00:12:35.080 --> 00:12:40.120
[Guest]: interesting because it just gave me this also a wanderlust, I guess, because, you know,

00:12:40.120 --> 00:12:46.760
[Guest]: when I was in IIT, you know, me and a friend hitchhiked from Delhi to London, you know,

00:12:46.760 --> 00:12:48.400
[Guest]: took three months.

00:12:48.400 --> 00:12:52.040
[Guest]: And now, you know, I last year, I just met someone at the London Business School, a faculty

00:12:52.040 --> 00:12:56.280
[Guest]: member who had actually done it exactly the same time, the other direction, you know,

00:12:56.280 --> 00:12:58.280
[Guest]: London to Delhi.

00:12:58.280 --> 00:13:01.760
[Guest]: And that was a time there was a lot of movement, a lot of hippies were going back and forth

00:13:01.760 --> 00:13:04.040
[Guest]: and these VW vans.

00:13:04.040 --> 00:13:05.520
[Guest]: So it was a it was a great time.

00:13:05.520 --> 00:13:10.240
[Guest]: But it gave me sort of a sense of, you know, wanting to explore, which I've continued to

00:13:10.240 --> 00:13:11.240
[Guest]: do since then.

00:13:11.240 --> 00:13:16.240
[Guest]: I just love to travel and be in other cultures.

00:13:16.240 --> 00:13:17.920
[Amit Varma]: It's very refreshing.

00:13:17.920 --> 00:13:22.240
[Amit Varma]: And I noticed when you went to IIT Bombay, you know, you passed out from IIT in 78.

00:13:22.240 --> 00:13:27.040
[Amit Varma]: And you did a BTech in chemical engineering, which doesn't seem too connected with everything

00:13:27.040 --> 00:13:28.320
[Amit Varma]: you've done since.

00:13:28.320 --> 00:13:32.120
[Amit Varma]: So, you know, were you just sort of following the standard routes that one does school and

00:13:32.120 --> 00:13:34.040
[Amit Varma]: then okay, IIT is the thing to do.

00:13:34.040 --> 00:13:39.080
[Amit Varma]: So one goes for engineering and all of that, or did you have specific ideas of your own

00:13:39.080 --> 00:13:42.000
[Amit Varma]: at that point in time about the things that you wanted to do?

00:13:42.000 --> 00:13:48.080
[Amit Varma]: And was there anything in your interests or your sort of hobbies at that time that presage

00:13:48.080 --> 00:13:50.640
[Amit Varma]: the future direction of your life as it were?

00:13:50.640 --> 00:13:51.680
[Guest]: No, not at all.

00:13:51.680 --> 00:13:56.640
[Guest]: I was just trying to create good alternatives for myself, you know, and my parents being

00:13:56.640 --> 00:14:01.600
[Guest]: typical Indian parents of that era said, you know, do something where you can get a job,

00:14:01.600 --> 00:14:05.160
[Guest]: you know, that you can support yourself and you're good at math and stuff.

00:14:05.160 --> 00:14:08.200
[Guest]: So, you know, engineering seems like the right thing.

00:14:08.200 --> 00:14:11.440
[Guest]: So, you know, so I took, you know, Sangal's classes in South Extension.

00:14:11.440 --> 00:14:17.240
[Guest]: So I went to IIT Delhi, by the way, you know, I took his classes and, you know, and we were

00:14:17.240 --> 00:14:22.480
[Guest]: just so much behind the kids from the Delhi schools, you know, like Columbus and Xaviers,

00:14:22.480 --> 00:14:24.320
[Guest]: you know, we guys have gone to boarding school.

00:14:24.320 --> 00:14:28.840
[Guest]: We played lots of sports, you know, and, you know, my very first day in class in Sangal,

00:14:28.840 --> 00:14:34.080
[Guest]: you know, I was just completely lost, you know, he'd say, you know, acha, sign 4A kya

00:14:34.080 --> 00:14:39.000
[Guest]: hai, you know, and like five guys would like raise their hands and say, you know, whatever.

00:14:39.000 --> 00:14:44.960
[Guest]: And I'd be like trying to figure out like, how do I break this, you know, sign 4A and

00:14:44.960 --> 00:14:45.960
[Guest]: solve it.

00:14:45.960 --> 00:14:52.440
[Guest]: So, yeah, no, there was it was nothing it was just go for the, you know, alternatives

00:14:52.440 --> 00:14:56.600
[Guest]: in front of you, and, you know, I took the IIT exam, which, as you know, is brutal and

00:14:56.600 --> 00:15:02.640
[Guest]: grueling, you know, stumbled my way into IIT, you know, and then applied to graduate school.

00:15:02.640 --> 00:15:06.640
[Guest]: And then Pittsburgh is really sort of where I found myself in the graduate program.

00:15:06.640 --> 00:15:11.320
[Guest]: That's when I learned about artificial intelligence and what it was.

00:15:11.320 --> 00:15:18.120
[Guest]: And I'll never forget my very first exposure to AI, you know, that will remain with me.

00:15:18.120 --> 00:15:23.160
[Guest]: I just joined a PhD program there, and a bunch of students asked me, they said, Hey, you

00:15:23.160 --> 00:15:27.640
[Guest]: know, we want to ask this professor to offer a course in artificial intelligence, but we

00:15:27.640 --> 00:15:30.240
[Guest]: just want to go in fourth, so he knows there's enough of us.

00:15:30.240 --> 00:15:31.920
[Guest]: So I just said, What is AI?

00:15:31.920 --> 00:15:34.320
[Guest]: And they said, Oh, it's when machines get intelligent.

00:15:34.320 --> 00:15:37.200
[Guest]: And I was like, Okay, whatever.

00:15:37.200 --> 00:15:42.440
[Guest]: And we went up to this lab in the medical school, 13th floor, top floor, and there was

00:15:42.440 --> 00:15:45.280
[Guest]: this special decision systems lab.

00:15:45.280 --> 00:15:50.880
[Guest]: And there was this professor, Harry Popel, who had built the first diagnostic system

00:15:50.880 --> 00:15:56.040
[Guest]: for the entire field of internal medicine, I had no idea what that was.

00:15:56.040 --> 00:16:00.080
[Guest]: And I had no idea that I'd be spending the next five years of my life in that lab, you

00:16:00.080 --> 00:16:01.080
[Guest]: know, pretty much full time.

00:16:01.080 --> 00:16:04.000
[Guest]: But, you know, we went there and asked him.

00:16:04.000 --> 00:16:09.480
[Guest]: And he said, Oh, yeah, I'd be delighted, you know, asked us about our backgrounds.

00:16:09.480 --> 00:16:15.360
[Guest]: And, you know, and but I remember I was standing while we were waiting for him, he was on the

00:16:15.360 --> 00:16:19.280
[Guest]: call, there was this gigantic screen in the middle of the room, connected to a computer

00:16:19.280 --> 00:16:20.960
[Guest]: at Stanford.

00:16:20.960 --> 00:16:26.160
[Guest]: And there was this physician, like, completely white hair puffing a pipe.

00:16:26.160 --> 00:16:31.760
[Guest]: And he was, quote, unquote, talking to the monitor in front of him through his assistant

00:16:31.760 --> 00:16:36.360
[Guest]: who was typing because people those days couldn't type.

00:16:36.360 --> 00:16:39.400
[Guest]: And the system was asking him questions about a case.

00:16:39.400 --> 00:16:41.340
[Guest]: He was trying to solve a case.

00:16:41.340 --> 00:16:45.720
[Guest]: And so the system asked him a few questions, he gave the answers, it asked him about blood,

00:16:45.720 --> 00:16:47.440
[Guest]: urine, all that kind of stuff.

00:16:47.440 --> 00:16:51.200
[Guest]: And it was engaging in a dialogue and then asked him a question like, you know, was there

00:16:51.200 --> 00:16:57.320
[Guest]: a pain in the left lower abdomen and, you know, he said, puff, puff, why, like, why

00:16:57.320 --> 00:16:59.080
[Guest]: you asked me the question.

00:16:59.080 --> 00:17:02.880
[Guest]: And on the screen, you know, the computer said, well, because the evidence you've given

00:17:02.880 --> 00:17:05.960
[Guest]: me so far is consistent with the following hypotheses.

00:17:05.960 --> 00:17:10.080
[Guest]: And this question will help me discriminate between the top two.

00:17:10.080 --> 00:17:14.840
[Guest]: And I was just looking at this thing like, holy smoke, like, how is a computer doing

00:17:14.840 --> 00:17:15.840
[Guest]: this?

00:17:15.840 --> 00:17:20.360
[Guest]: Like, how the hell is a computer behaving in this way and asking him, you know, the

00:17:20.360 --> 00:17:21.360
[Guest]: right questions?

00:17:21.360 --> 00:17:24.120
[Guest]: You know, and that was my first exposure.

00:17:24.120 --> 00:17:27.320
[Guest]: And you know, I think one of the things we'll probably come back to at some point during

00:17:27.320 --> 00:17:31.440
[Guest]: this talk is where are we now, relative to where we were then, right?

00:17:31.440 --> 00:17:38.560
[Guest]: And I'm talking like 1979, where I was watching this dialogue happen, and the system hardly

00:17:38.560 --> 00:17:39.880
[Guest]: ever made any mistakes.

00:17:39.880 --> 00:17:45.000
[Guest]: It would actually diagnose virtually every case correctly, right?

00:17:45.000 --> 00:17:47.160
[Guest]: And so that was my first exposure to AI.

00:17:47.160 --> 00:17:50.120
[Guest]: And I said, wow, like, I don't know what this hell this thing is, but this is what I want

00:17:50.120 --> 00:17:51.760
[Guest]: to do for the rest of my life, you know?

00:17:51.760 --> 00:17:55.840
[Guest]: So that's what I mean by sort of found what I really wanted to do in a purpose in life

00:17:55.840 --> 00:18:01.200
[Guest]: there, because of him and another gentleman called Herb Simon, who was a Nobel laureate

00:18:01.200 --> 00:18:03.600
[Guest]: in economics and one of the fathers of AI.

00:18:03.600 --> 00:18:08.840
[Guest]: So these are my two mentors in grad school, and they gave me a hell of a training and

00:18:08.840 --> 00:18:12.680
[Guest]: just sort of taught me how to think, which I didn't really know how to do before that.

00:18:12.680 --> 00:18:17.080
[Guest]: I didn't really know how to think, I didn't really know how to ask a question and answer

00:18:17.080 --> 00:18:18.080
[Guest]: it, right?

00:18:18.080 --> 00:18:21.080
[Guest]: I mean, even though I'd gone to IIT and done engineering and math and all that kind of

00:18:21.080 --> 00:18:24.800
[Guest]: stuff, I just didn't know how to think, you know, I mean, I could solve problems, and

00:18:24.800 --> 00:18:26.600
[Guest]: I was good analytically.

00:18:26.600 --> 00:18:31.640
[Guest]: So I had some good tools in my back pocket, but just learning how to think about problems

00:18:31.640 --> 00:18:37.760
[Guest]: and ask the question and just learning what inquiry is really all about scientific inquiry

00:18:37.760 --> 00:18:41.680
[Guest]: is really all about and how to conduct it was what they exposed me to.

00:18:41.680 --> 00:18:44.680
[Guest]: And that's been sort of the rest of my career.

00:18:44.680 --> 00:18:48.280
[Guest]: So you know, when I started with that Jerry Garcia quote, it's, you know, you can see

00:18:48.280 --> 00:18:51.640
[Guest]: why it's sort of been, you know, a long, strange trip indeed.

00:18:51.640 --> 00:18:56.680
[Amit Varma]: Yeah, you know, and I'm so fascinated and delighted by that story of that moment of

00:18:56.680 --> 00:18:58.840
[Amit Varma]: magic, when you see the computer doing what it does.

00:18:58.840 --> 00:19:02.160
[Amit Varma]: And it just strikes me that in our own lives, and this will especially be true for young

00:19:02.160 --> 00:19:06.480
[Amit Varma]: people today, that we are so jaded, and we take the tech around us for so much for granted

00:19:06.480 --> 00:19:11.400
[Amit Varma]: that we probably don't have those moments of magic, those aha moments when you realize

00:19:11.400 --> 00:19:16.120
[Amit Varma]: what the hell like I remember, you know, just speaking of myself, one sort of moment I had

00:19:16.120 --> 00:19:20.800
[Amit Varma]: like that was actually quite recent when it reminded me also of how jaded I'd become,

00:19:20.800 --> 00:19:23.320
[Amit Varma]: which was, you know, I'm a bit of a chess enthusiast.

00:19:23.320 --> 00:19:28.760
[Amit Varma]: And when AlphaZero came out, and I remember looking at AlphaZero's games, and, you know,

00:19:28.760 --> 00:19:32.000
[Amit Varma]: and of course, computers in chess, of course, have been around for the longest time, Stockfish

00:19:32.000 --> 00:19:37.000
[Amit Varma]: and all have been used for pedagogy by the top chess players forever, and have shaped

00:19:37.000 --> 00:19:39.320
[Amit Varma]: the world of chess in very interesting ways.

00:19:39.320 --> 00:19:42.120
[Amit Varma]: But AlphaZero was just such a step ahead.

00:19:42.120 --> 00:19:46.760
[Amit Varma]: And you know, I thought of that just now, when you spoke about how you started learning

00:19:46.760 --> 00:19:52.080
[Amit Varma]: how to think from what you had seen and from what AI has done, because you know, for example,

00:19:52.080 --> 00:19:55.840
[Amit Varma]: you know, Magnus Carlsen's, the chess world champion Magnus Carlsen's coach Peter Hein

00:19:55.840 --> 00:20:00.120
[Amit Varma]: Nelson once said that, you know, I always used to wonder what it would be like to see

00:20:00.120 --> 00:20:03.120
[Amit Varma]: aliens with a super intelligence, play chess.

00:20:03.120 --> 00:20:07.760
[Amit Varma]: And when I saw the AlphaZero games, I knew, and, you know, but we'll kind of come back

00:20:07.760 --> 00:20:10.040
[Amit Varma]: to that later.

00:20:10.040 --> 00:20:15.000
[Amit Varma]: But I want to probe a little further into what you meant when you said that you learned

00:20:15.000 --> 00:20:19.320
[Amit Varma]: how to think, you know, can you elaborate a little bit on that?

00:20:19.320 --> 00:20:22.280
[Amit Varma]: What do you mean by learning how to think?

00:20:22.280 --> 00:20:23.920
[Guest]: Great question.

00:20:23.920 --> 00:20:29.980
[Guest]: And let me see if I can do it justice, because it's one of those things where you kind of

00:20:29.980 --> 00:20:34.660
[Guest]: know what it is, but putting it into words, it's a little challenging.

00:20:34.660 --> 00:20:36.720
[Guest]: But it means several things.

00:20:36.720 --> 00:20:42.920
[Guest]: First, it means developing an ability to ask a good question.

00:20:42.920 --> 00:20:45.880
[Guest]: You know, I mean, I've been teaching for 30 plus years.

00:20:45.880 --> 00:20:51.160
[Guest]: And very often, you know, in class, you know, someone asks a question, I say, wow, that's

00:20:51.160 --> 00:20:53.600
[Guest]: a great question, right?

00:20:53.600 --> 00:20:57.760
[Guest]: And, you know, like, what makes for a great question, right?

00:20:57.760 --> 00:21:03.720
[Guest]: It's a bit of an art there, you know, but you learn to recognize it when you see it.

00:21:03.720 --> 00:21:08.000
[Guest]: Like, I mean, some questions are just better than others, right?

00:21:08.000 --> 00:21:12.240
[Guest]: And you know, maybe if I have some more time to think about it, I'll, you know, I may be

00:21:12.240 --> 00:21:15.440
[Guest]: able to come up with actually a model of what makes a good question.

00:21:15.440 --> 00:21:20.360
[Guest]: But the ability to think is firstly about how to ask the right question.

00:21:20.360 --> 00:21:26.600
[Guest]: And then once you've asked the right question, a good question, the ability to then say,

00:21:26.600 --> 00:21:29.840
[Guest]: well, is this answerable, right?

00:21:29.840 --> 00:21:33.120
[Guest]: Can I actually answer this question?

00:21:33.120 --> 00:21:37.880
[Guest]: If I had all the resources, I had all the data can actually answer this question, right?

00:21:37.880 --> 00:21:44.200
[Guest]: And what sorts of answers do I have any expectations about what I should observe, right?

00:21:44.200 --> 00:21:48.600
[Guest]: What do we know at the moment about this question, right?

00:21:48.600 --> 00:21:51.680
[Guest]: So if I'm asking a question, like, what do we know about it?

00:21:51.680 --> 00:21:56.000
[Guest]: You know, so just to go off at a bit of a tangent here, right, someone called me yesterday

00:21:56.000 --> 00:22:01.280
[Guest]: was interested in applying to the PhD program in data science at NYU, which I had.

00:22:01.280 --> 00:22:03.400
[Guest]: And so he wanted 20 minutes of my time.

00:22:03.400 --> 00:22:05.280
[Guest]: So I said, you know, I said, call me.

00:22:05.280 --> 00:22:06.680
[Guest]: And I said, so what's going on in your head?

00:22:06.680 --> 00:22:12.000
[Guest]: He says, you know, I'm graduating, I've become fascinated by reinforcement learning because

00:22:12.000 --> 00:22:15.240
[Guest]: of what I saw in AlphaGo, you know, which used reinforcement learning.

00:22:15.240 --> 00:22:19.320
[Guest]: So I'm really interested in reinforcement learning, because I think there's some real

00:22:19.320 --> 00:22:21.040
[Guest]: magic to that method.

00:22:21.040 --> 00:22:27.720
[Guest]: And, you know, so that's what I want to explore in my PhD program.

00:22:27.720 --> 00:22:31.840
[Guest]: And my question to him was, well, why do you think it worked so well in solving AlphaGo

00:22:31.840 --> 00:22:33.360
[Guest]: as opposed to other methods, right?

00:22:33.360 --> 00:22:38.960
[Guest]: Do you think other methods could have solved AlphaGo as easily, or was it something unique

00:22:38.960 --> 00:22:39.960
[Guest]: about reinforcement learning?

00:22:39.960 --> 00:22:41.480
[Guest]: And he says, gee, I don't know.

00:22:41.480 --> 00:22:44.280
[Guest]: That's an interesting question.

00:22:44.280 --> 00:22:49.000
[Guest]: And I said, but, you know, okay, and what do you think makes reinforcement learning

00:22:49.000 --> 00:22:52.080
[Guest]: work in game playing programs, right?

00:22:52.080 --> 00:22:55.520
[Guest]: Do you think it would work across the board, right?

00:22:55.520 --> 00:22:59.400
[Guest]: So I said to him, I said, you know, I view data science as a canvas where rows are methods

00:22:59.400 --> 00:23:05.160
[Guest]: and columns are domains, and the interesting questions arise at the intersections of these

00:23:05.160 --> 00:23:07.360
[Guest]: domains and methods.

00:23:07.360 --> 00:23:10.480
[Guest]: Reinforcement learning is a method, so look at it as a row.

00:23:10.480 --> 00:23:13.140
[Guest]: And now game playing is a column, right?

00:23:13.140 --> 00:23:17.800
[Guest]: Because game playing is a domain, you know, just like finance or healthcare or physics

00:23:17.800 --> 00:23:20.800
[Guest]: or law or, you know, whatever you call it, those are all domains, right?

00:23:20.800 --> 00:23:26.480
[Guest]: So I said, so do you think reinforcement learning would work across all of these domains equally

00:23:26.480 --> 00:23:27.480
[Guest]: well?

00:23:27.480 --> 00:23:32.560
[Guest]: What do you think about game playing that makes it work really well, right?

00:23:32.560 --> 00:23:37.280
[Guest]: So when you asked me earlier, like, you know, what does it mean to learn how to think?

00:23:37.280 --> 00:23:38.600
[Guest]: That's what I'm getting at, right?

00:23:38.600 --> 00:23:39.600
[Guest]: Where are we right now?

00:23:39.600 --> 00:23:40.600
[Guest]: What's the baseline?

00:23:40.600 --> 00:23:42.660
[Guest]: What do we know about something, right?

00:23:42.660 --> 00:23:43.960
[Guest]: And why is this a good question?

00:23:43.960 --> 00:23:47.560
[Guest]: Why will this take us forward significantly, right?

00:23:47.560 --> 00:23:49.340
[Guest]: Which makes it a good question.

00:23:49.340 --> 00:23:51.920
[Guest]: And then can you actually solve the problem?

00:23:51.920 --> 00:23:54.480
[Guest]: Do you have the data to solve it?

00:23:54.480 --> 00:23:57.280
[Guest]: And then do you have the chops, right?

00:23:57.280 --> 00:24:01.400
[Guest]: Do you know the tools and do you have any idea as to how long it's going to take you

00:24:01.400 --> 00:24:02.880
[Guest]: to solve it, right?

00:24:02.880 --> 00:24:06.360
[Guest]: So one of the things, you know, I sort of, I'm a pracademic, some people call me, you

00:24:06.360 --> 00:24:11.160
[Guest]: know, I'm an academic, but I'm also a practical and I, you know, created, you know, an automated

00:24:11.160 --> 00:24:13.880
[Guest]: hedge fund, you know, many years ago.

00:24:13.880 --> 00:24:17.320
[Guest]: And so when I think of research there, my question always is like, how long will this

00:24:17.320 --> 00:24:18.320
[Guest]: take us?

00:24:18.320 --> 00:24:21.720
[Guest]: You know, will it take a day, two days, a week, a month, or a year?

00:24:21.720 --> 00:24:23.760
[Guest]: If it's going to take a year, I'm not interested.

00:24:23.760 --> 00:24:24.760
[Guest]: It's too uncertain.

00:24:24.760 --> 00:24:27.400
[Guest]: But if it's going to take a day, I'm very interested.

00:24:27.400 --> 00:24:30.760
[Guest]: If it's going to take a week, well, I need to think about it, right?

00:24:30.760 --> 00:24:38.120
[Guest]: So it's all of those things that go into being a good researcher.

00:24:38.120 --> 00:24:42.040
[Guest]: So from A to Z, asking the question to execution.

00:24:42.040 --> 00:24:46.320
[Amit Varma]: So I'm just thinking aloud again, zooming into sort of the element of asking a good

00:24:46.320 --> 00:24:52.040
[Amit Varma]: question, which is a great answer, by the way, would it be one quality of a good question

00:24:52.040 --> 00:24:57.200
[Amit Varma]: that you minimize the assumptions, because it strikes me that humans are full of these

00:24:57.200 --> 00:25:01.440
[Amit Varma]: inherent biases and their assumptions about the way the world works.

00:25:01.440 --> 00:25:05.760
[Amit Varma]: And sometimes, if those are embedded within the question that you ask, that can make the

00:25:05.760 --> 00:25:07.120
[Amit Varma]: question less effective.

00:25:07.120 --> 00:25:12.400
[Amit Varma]: Like, again, going back to chess, one of the things that, you know, programs like Stockfish

00:25:12.400 --> 00:25:17.680
[Amit Varma]: and all would have embedded in them is the earlier heuristics of how you evaluate a chess

00:25:17.680 --> 00:25:22.240
[Amit Varma]: position where, you know, material has a certain importance, space has a certain importance

00:25:22.240 --> 00:25:23.640
[Amit Varma]: and so on.

00:25:23.640 --> 00:25:28.240
[Amit Varma]: Whereas with AlphaZero, one of the things we realized was that chess players across

00:25:28.240 --> 00:25:35.280
[Amit Varma]: human history have been undervaluing the importance of initiative with regard to material.

00:25:35.280 --> 00:25:40.400
[Amit Varma]: And that impression would have persisted because if you ask the questions with the wrong assumptions,

00:25:40.400 --> 00:25:43.480
[Amit Varma]: then you don't really get those great answers.

00:25:43.480 --> 00:25:48.480
[Amit Varma]: So again, I'm just kind of thinking aloud that would the good questions, therefore,

00:25:48.480 --> 00:25:55.000
[Amit Varma]: be willing to take that further step back and, you know, be open to sort of learning

00:25:55.000 --> 00:25:57.720
[Amit Varma]: that your assumptions earlier were false as well.

00:25:57.720 --> 00:26:02.640
[Guest]: That's a great way to look at it in terms of like, how many assumptions are you making

00:26:02.640 --> 00:26:05.120
[Guest]: in asking the question, right?

00:26:05.120 --> 00:26:13.360
[Guest]: There's another way to look at it, which is, can I control for a bunch of things, assumptions,

00:26:13.360 --> 00:26:17.320
[Guest]: for example, can I control for them when I answer the question, right?

00:26:17.320 --> 00:26:19.120
[Guest]: That's the other way to think about it, right?

00:26:19.120 --> 00:26:25.760
[Guest]: So, you know, just thinking aloud here, if I were to ask a question like, has India been

00:26:25.760 --> 00:26:32.640
[Guest]: adversely affected by COVID because of its administrative systems?

00:26:32.640 --> 00:26:34.320
[Guest]: Yeah, I'm just making this up, right?

00:26:34.320 --> 00:26:36.360
[Guest]: So this is a very broad question.

00:26:36.360 --> 00:26:40.520
[Guest]: And now, you know, you might say, well, you know, like, what kinds of assumptions will

00:26:40.520 --> 00:26:44.600
[Guest]: you have to make in answering the question, right?

00:26:44.600 --> 00:26:45.600
[Guest]: And you're right.

00:26:45.600 --> 00:26:50.280
[Guest]: If I have to make all kinds of assumptions about the problem, the data and all that kind

00:26:50.280 --> 00:26:54.800
[Guest]: of stuff, then by the time I answer the question, the answer may not have any teeth, right?

00:26:54.800 --> 00:26:55.800
[Guest]: It may not be relevant.

00:26:55.800 --> 00:27:01.420
[Guest]: But if I can ask like a really nice, tight question, you know, that involves few assumptions

00:27:01.420 --> 00:27:06.300
[Guest]: and to the extent that it involves assumptions, I can control for them, then that's okay.

00:27:06.300 --> 00:27:10.800
[Guest]: So I guess to sort of summarize my answer, I'd say, yeah, in general, you want to make

00:27:10.800 --> 00:27:14.580
[Guest]: sure that you're not making too many assumptions, especially the wrong ones.

00:27:14.580 --> 00:27:18.320
[Guest]: And to the extent you are, can you control for them?

00:27:18.320 --> 00:27:19.320
[Guest]: When you answer the question.

00:27:19.320 --> 00:27:23.680
[Amit Varma]: And, you know, one of the fascinating quotes that I found of yours, so I'm not going to

00:27:23.680 --> 00:27:28.440
[Amit Varma]: quote you back to yourself, which really made me sit back and think is when you said quote,

00:27:28.440 --> 00:27:33.080
[Amit Varma]: when you have a data driven approach to life, you find that patterns often emerge before

00:27:33.080 --> 00:27:39.680
[Amit Varma]: reasons to stop quote, which I found sort of eye opening to me in various contexts,

00:27:39.680 --> 00:27:43.800
[Amit Varma]: not just chess, which is what I immediately thought of, you know, and that's something

00:27:43.800 --> 00:27:49.680
[Amit Varma]: as you pointed out came from your sort of journey examining big data that I think you

00:27:49.680 --> 00:27:54.800
[Amit Varma]: mentioned started in 1990, when Nielsen approached you to look at some data and you found some

00:27:54.800 --> 00:27:59.560
[Amit Varma]: unusual patterns, which you could not explain at the moment, but they were clearly patterns

00:27:59.560 --> 00:28:01.320
[Amit Varma]: and they were clearly significant.

00:28:01.320 --> 00:28:04.400
[Amit Varma]: But, you know, before we go there, you know, take me through your journey, then through

00:28:04.400 --> 00:28:08.120
[Amit Varma]: the 80s, where you spent five years in this lab, and, you know, what kind of work are

00:28:08.120 --> 00:28:09.120
[Amit Varma]: you doing?

00:28:09.120 --> 00:28:11.360
[Amit Varma]: What are the kind of problems which now interest you?

00:28:11.360 --> 00:28:15.120
[Amit Varma]: Because I'm guessing that it is at around this point from the way you seem to describe

00:28:15.120 --> 00:28:19.580
[Amit Varma]: it, that you have this intellectual focus that, you know, these are the kind of problems

00:28:19.580 --> 00:28:20.580
[Amit Varma]: you want to solve.

00:28:20.580 --> 00:28:23.960
[Amit Varma]: So tell me a little bit about that journey from that point on.

00:28:23.960 --> 00:28:29.160
[Guest]: So I'm really glad you asked the question, because it actually helps to clarify how AI

00:28:29.160 --> 00:28:32.920
[Guest]: itself has changed in the last 40 years, right?

00:28:32.920 --> 00:28:44.720
[Guest]: So in the 80s, or rather, I should say until the 90s, AI was largely about reasoning and,

00:28:44.720 --> 00:28:50.640
[Guest]: you know, systems that could reason from data, make inferences.

00:28:50.640 --> 00:28:56.920
[Guest]: And I'd say that the language of AI was mostly logic, right?

00:28:56.920 --> 00:28:59.400
[Guest]: Because with logic, you're on firm ground, right?

00:28:59.400 --> 00:29:02.940
[Guest]: If A implies B, and you're seeing A, well, then B must be true as well, right?

00:29:02.940 --> 00:29:07.880
[Guest]: So a lot of AI had sort of hitched itself up to logic, right?

00:29:07.880 --> 00:29:13.720
[Guest]: And so logicians were in control of the field at that time.

00:29:13.720 --> 00:29:16.320
[Guest]: And then people said, you know, logic isn't enough, right?

00:29:16.320 --> 00:29:19.920
[Guest]: In fact, even my thesis advisor, right, he said, you know, deductive logic doesn't do

00:29:19.920 --> 00:29:20.920
[Guest]: it.

00:29:20.920 --> 00:29:23.960
[Guest]: And he came up with, you know, he picked up on something that a philosopher called Charles

00:29:23.960 --> 00:29:29.880
[Guest]: Sanders Peirce had come up with called abductive logic, which is that if A implies B and you

00:29:29.880 --> 00:29:35.720
[Guest]: observe B, it doesn't mean that A is necessarily true, but it could be, right?

00:29:35.720 --> 00:29:37.520
[Guest]: It's like an induction now, right?

00:29:37.520 --> 00:29:43.840
[Guest]: So now you're sort of observing things and invoking hypotheses or reasons for them, right?

00:29:43.840 --> 00:29:48.440
[Guest]: So AI, I'd say until the 90s, and this is sort of when I got involved in AI, it was

00:29:48.440 --> 00:29:49.440
[Guest]: all about logic.

00:29:49.440 --> 00:29:53.040
[Guest]: It was all about representation, reasoning, inference, right?

00:29:53.040 --> 00:29:54.560
[Guest]: That was the thing.

00:29:54.560 --> 00:29:59.040
[Guest]: What happened around the 90s is that there was a sea change because data started becoming

00:29:59.040 --> 00:30:00.040
[Guest]: available, right?

00:30:00.040 --> 00:30:05.000
[Guest]: And this is, in my own career, that was becoming apparent that, wow, you know, like we can

00:30:05.000 --> 00:30:07.160
[Guest]: actually now look towards data.

00:30:07.160 --> 00:30:11.320
[Guest]: And at that time, it was mostly telephone companies and banks that had the data, you

00:30:11.320 --> 00:30:15.840
[Guest]: know, other than defense and physics, like, you know, physicists have always had tons

00:30:15.840 --> 00:30:19.640
[Guest]: of data, but other than them, which was kind of a world in itself, it was mostly telecom

00:30:19.640 --> 00:30:26.600
[Guest]: companies and banks and companies like Nielsen, right, who were in the information business.

00:30:26.600 --> 00:30:33.000
[Guest]: And so I saw this sort of as a new paradigm for AI that was emerging that would probably

00:30:33.000 --> 00:30:34.960
[Guest]: eclipse the previous one.

00:30:34.960 --> 00:30:37.080
[Guest]: And in fact, that's exactly what happened, right?

00:30:37.080 --> 00:30:44.080
[Guest]: So in the early 90s, I sort of shifted my focus towards data and learning from data

00:30:44.080 --> 00:30:50.800
[Guest]: and got this project with AC Nielsen, household services company, and they were tracking 50,000

00:30:50.800 --> 00:30:51.800
[Guest]: households all over the US.

00:30:51.800 --> 00:30:56.320
[Guest]: You know, anytime they shop, they scanned the item and went to a database and they shared

00:30:56.320 --> 00:31:00.320
[Guest]: this data with me and said, you know, see if you can find something interesting in this,

00:31:00.320 --> 00:31:04.280
[Guest]: you know, like we don't quite know what, but, you know, look at this data.

00:31:04.280 --> 00:31:05.280
[Guest]: So I did.

00:31:05.280 --> 00:31:09.560
[Guest]: And, you know, I cracked it through these algorithms I was working on at the time, you

00:31:09.560 --> 00:31:13.480
[Guest]: know, things called genetic algorithms that would look at data and find, extract rules

00:31:13.480 --> 00:31:14.720
[Guest]: from it.

00:31:14.720 --> 00:31:16.720
[Guest]: So I went to this meeting and they said, so what'd you find?

00:31:16.720 --> 00:31:18.720
[Guest]: I said, you know, I found something, but I have no idea what it means.

00:31:18.720 --> 00:31:19.720
[Guest]: This is all right.

00:31:19.720 --> 00:31:20.720
[Guest]: Let's take it from the top.

00:31:20.720 --> 00:31:26.880
[Guest]: And I said, it looks like older women in the Northeast do a lot of their shopping on Thursdays.

00:31:26.880 --> 00:31:28.280
[Guest]: And he said, Oh yeah, that's coupon day.

00:31:28.280 --> 00:31:29.720
[Guest]: What else did you find?

00:31:29.720 --> 00:31:34.880
[Guest]: And I was just ecstatic, you know, that I found something really interesting that made

00:31:34.880 --> 00:31:37.800
[Guest]: sense that I hadn't really told the machine to find.

00:31:37.800 --> 00:31:41.020
[Guest]: I just told it to look for unusual shopping activity.

00:31:41.020 --> 00:31:46.360
[Guest]: And it said, you know, older women on Thursdays, you know, and there were many other patterns

00:31:46.360 --> 00:31:50.040
[Guest]: like that in the data that they were completely unaware of, you know, the manager of the Neil

00:31:50.040 --> 00:31:51.040
[Guest]: said, Oh really?

00:31:51.040 --> 00:31:52.040
[Guest]: Wow.

00:31:52.040 --> 00:31:53.040
[Guest]: Wow.

00:31:53.040 --> 00:31:54.040
[Guest]: We didn't know that.

00:31:54.040 --> 00:31:55.040
[Guest]: Right.

00:31:55.040 --> 00:31:57.200
[Guest]: And so that was like, that's interesting that these patterns that are emerging and the reasons

00:31:57.200 --> 00:32:01.520
[Guest]: they're telling me in retrospect, like, you know, that, you know, some of them were easily

00:32:01.520 --> 00:32:05.920
[Guest]: explained like, yeah, that's coupon day, but others that weren't right.

00:32:05.920 --> 00:32:09.080
[Guest]: And fast forward four years, right.

00:32:09.080 --> 00:32:13.720
[Guest]: I was now in wall street, I'd taken some years off from academia and I was with a trading

00:32:13.720 --> 00:32:18.880
[Guest]: group and I told them, I said, you know, just give me all your data and I'll tell you if

00:32:18.880 --> 00:32:19.880
[Guest]: you could have done better.

00:32:19.880 --> 00:32:22.040
[Guest]: And they said, well, you don't need to know anything about what we do.

00:32:22.040 --> 00:32:24.040
[Guest]: I said, no, just give me your trades.

00:32:24.040 --> 00:32:25.040
[Guest]: So they did.

00:32:25.040 --> 00:32:29.920
[Guest]: And again, you know, hocus pocus, I cranked them through my genetic algorithm.

00:32:29.920 --> 00:32:35.160
[Guest]: I come back to our weekly meeting next week on Fridays.

00:32:35.160 --> 00:32:37.600
[Guest]: And so the head of the group said, so Vasant, what'd you find?

00:32:37.600 --> 00:32:40.720
[Guest]: I said, I found something, but I have no idea what it means.

00:32:40.720 --> 00:32:42.440
[Guest]: And he says, so what'd you find?

00:32:42.440 --> 00:32:47.640
[Guest]: I said, well, you know, when the 30 day volatility is in the lowest quartile, your trades are

00:32:47.640 --> 00:32:55.040
[Guest]: three times as profitable as they are otherwise the silence around the room for five seconds.

00:32:55.040 --> 00:32:58.080
[Guest]: And the head researcher says, yeah, I looked at volatility.

00:32:58.080 --> 00:33:00.760
[Guest]: And the head of the group says, Frank, you know, shut the fuck up.

00:33:00.760 --> 00:33:03.800
[Guest]: How long have I been telling you to look at volatility?

00:33:03.800 --> 00:33:07.760
[Guest]: And this guy who knows nothing about what we do tells us it matters.

00:33:07.760 --> 00:33:12.640
[Guest]: And then there was like, you know, shouting and people sort of accusing each other of,

00:33:12.640 --> 00:33:15.800
[Guest]: you know, being dumb and, you know, missing the picture.

00:33:15.800 --> 00:33:20.240
[Guest]: And I was just watching this thing and saying, can someone tell me what's going on here?

00:33:20.240 --> 00:33:26.320
[Guest]: And they said, no, but we've observed whenever volatility spikes, we lose a lot of money.

00:33:26.320 --> 00:33:31.440
[Guest]: So it's interesting that you're telling us this, knowing nothing about what we do.

00:33:31.440 --> 00:33:37.000
[Guest]: And it was six months later that I realized why I was observing what I was observing,

00:33:37.000 --> 00:33:41.320
[Guest]: you know, because I went into the literature of finance and, you know, and, and I found

00:33:41.320 --> 00:33:43.800
[Guest]: the reasons for what I'd found.

00:33:43.800 --> 00:33:49.520
[Guest]: And this has happened to me pretty much in every domain I've looked at, you know, I mean,

00:33:49.520 --> 00:33:53.480
[Guest]: I, you know, I worked with the San Antonio Spurs for a couple of seasons looking at their

00:33:53.480 --> 00:33:59.120
[Guest]: data, you know, sports, basketball, completely different context, again, the same thing,

00:33:59.120 --> 00:34:00.120
[Guest]: right?

00:34:00.120 --> 00:34:04.960
[Guest]: But the things that pop up, you know, for example, you know, having to do with, you

00:34:04.960 --> 00:34:08.560
[Guest]: know, the performance of a team, whether they're playing at home versus away and, you know,

00:34:08.560 --> 00:34:12.760
[Guest]: why do they perform worse when they're away, you know, just things like that, that was

00:34:12.760 --> 00:34:19.160
[Guest]: just in the data that you could extract and then say, wow, like, I had no idea that this

00:34:19.160 --> 00:34:20.160
[Guest]: even existed.

00:34:20.160 --> 00:34:24.560
[Guest]: And then finding the reasons for why those patterns exist is often a very enlightening

00:34:24.560 --> 00:34:28.800
[Guest]: exercise, you know, reveals a lot of things about the domain that you may not have been

00:34:28.800 --> 00:34:30.080
[Guest]: aware of.

00:34:30.080 --> 00:34:35.600
[Guest]: And to me, this is one of the promises of data science is that it can nudge you towards

00:34:35.600 --> 00:34:37.640
[Guest]: things that you might not even have thought of, right?

00:34:37.640 --> 00:34:41.520
[Guest]: I mean, you talked about AlphaGo and, you know, people have said, wow, it came up with

00:34:41.520 --> 00:34:45.040
[Guest]: these moves that we never would have thought about, right?

00:34:45.040 --> 00:34:51.820
[Guest]: Machines look at the world differently from humans do, you know, and they often reveal

00:34:51.820 --> 00:34:54.560
[Guest]: things that surprise us in retrospect.

00:34:54.560 --> 00:34:59.380
[Guest]: And that's where that quote came from, which is that patterns often emerge before the reasons

00:34:59.380 --> 00:35:00.680
[Guest]: for them become apparent.

00:35:00.680 --> 00:35:05.600
[Amit Varma]: Yeah, this sort of reminds me of and this is not apropos of either AI or whatever, but

00:35:05.600 --> 00:35:08.880
[Amit Varma]: I thought of the phrase, you know, when you spoke about looking for reasons after the

00:35:08.880 --> 00:35:12.840
[Amit Varma]: patterns emerge, and I thought of the phrase the interpreter, which is a phrase that Michael

00:35:12.840 --> 00:35:18.200
[Amit Varma]: Gazzaniga used when he did his famous experiments on patients of split brain epilepsy, I guess

00:35:18.200 --> 00:35:19.280
[Amit Varma]: you've heard about that, right?

00:35:19.280 --> 00:35:22.400
[Guest]: I have not, I'm sorry to say, but it sounds really fascinating.

00:35:22.400 --> 00:35:24.520
[Amit Varma]: Yeah, yeah, no, no.

00:35:24.520 --> 00:35:26.480
[Amit Varma]: I'll go through it for the benefit of my listeners.

00:35:26.480 --> 00:35:29.680
[Amit Varma]: So this really happened in, I think, the 50s or the 60s.

00:35:29.680 --> 00:35:33.560
[Amit Varma]: And Gazzaniga recently, I think, wrote a book called human or something where he talks about

00:35:33.560 --> 00:35:34.560
[Amit Varma]: it.

00:35:34.560 --> 00:35:38.440
[Amit Varma]: But this is like a seminal experiment mentioned in various books, where one of the ways of

00:35:38.440 --> 00:35:43.160
[Amit Varma]: solving or, you know, helping to mitigate split brain epilepsy was that you cut that

00:35:43.160 --> 00:35:46.720
[Amit Varma]: part of the corpus callosum, I think, which connects the right brain and the left brain.

00:35:46.720 --> 00:35:49.400
[Amit Varma]: So when you sever that, they can't communicate to each other.

00:35:49.400 --> 00:35:54.000
[Amit Varma]: So he tried this experiment with patients on whom this operation had been done, where

00:35:54.000 --> 00:35:56.680
[Amit Varma]: you basically then divide their field of vision.

00:35:56.680 --> 00:35:59.460
[Amit Varma]: So the right brain, of course, controls the left eye or whatever.

00:35:59.460 --> 00:36:03.320
[Amit Varma]: But essentially, you divide the field of vision, the two halves of the brain aren't talking

00:36:03.320 --> 00:36:04.620
[Amit Varma]: to each other.

00:36:04.620 --> 00:36:09.360
[Amit Varma]: And you show you know, what the part of the field of vision that the right brain can perceive,

00:36:09.360 --> 00:36:15.080
[Amit Varma]: you show it something like, you know, ask for a glass of water, or say this, or go to

00:36:15.080 --> 00:36:16.200
[Amit Varma]: the loo or whatever.

00:36:16.200 --> 00:36:18.880
[Amit Varma]: And the person would do that he would follow the instruction.

00:36:18.880 --> 00:36:21.240
[Amit Varma]: And then he would be asked, why did you do that?

00:36:21.240 --> 00:36:22.920
[Amit Varma]: And he would have no clue.

00:36:22.920 --> 00:36:27.640
[Amit Varma]: So the left part of his brain would make up an explanation, which was completely unrelated.

00:36:27.640 --> 00:36:29.980
[Amit Varma]: And it would actually believe that.

00:36:29.980 --> 00:36:34.400
[Amit Varma]: And he called, therefore, this function in the brain, the interpreter, and reading about

00:36:34.400 --> 00:36:38.840
[Amit Varma]: this was a sort of a aha moment for me, because then I realized that, you know, many of the

00:36:38.840 --> 00:36:42.680
[Amit Varma]: things that we do in our lives, we think we do them for reasons, but we could be making

00:36:42.680 --> 00:36:46.840
[Amit Varma]: them up post facto, we could be rationalizing them along the way.

00:36:46.840 --> 00:36:53.080
[Amit Varma]: And our actual actions could be just caused by a variety of instinctual things, which

00:36:53.080 --> 00:36:55.120
[Guest]: we are not aware of at a conscious level.

00:36:55.120 --> 00:36:56.120
[Guest]: Absolutely.

00:36:56.120 --> 00:36:57.120
[Guest]: Yes, yes.

00:36:57.120 --> 00:37:00.800
[Guest]: And I mean, I'm sorry to interrupt, because there's so much I want to say on that, right?

00:37:00.800 --> 00:37:03.840
[Guest]: Because there's at least there's at least two strands to what you've said.

00:37:03.840 --> 00:37:08.320
[Guest]: One is, you may be convincing yourself that you found the reasons and fooling yourself,

00:37:08.320 --> 00:37:10.160
[Guest]: which is absolutely right.

00:37:10.160 --> 00:37:17.440
[Guest]: You know, and my sort of hypothesis is that in areas where the domain is much more well

00:37:17.440 --> 00:37:21.000
[Guest]: defined, that's an easier problem, right?

00:37:21.000 --> 00:37:27.200
[Guest]: So when a problem has a high degree of predictability, it has a good theory, you can actually have

00:37:27.200 --> 00:37:33.600
[Guest]: more confidence that you've actually interpreted the pattern correctly or for the right reasons,

00:37:33.600 --> 00:37:34.600
[Guest]: right?

00:37:34.600 --> 00:37:36.600
[Guest]: So that's a fascinating line of thinking.

00:37:36.600 --> 00:37:41.680
[Guest]: And you're absolutely right that even though you're trying to now explain the reasons,

00:37:41.680 --> 00:37:47.920
[Guest]: it's not always the case that you'll be successful or find the right reason for that, right?

00:37:47.920 --> 00:37:53.440
[Guest]: So the other strand, and I don't know if you've read this book called My Stroke of Insight.

00:37:53.440 --> 00:37:54.440
[Guest]: No, no.

00:37:54.440 --> 00:37:56.640
[Guest]: Oh, you will find that fascinating.

00:37:56.640 --> 00:37:58.120
[Guest]: It's also a TED talk.

00:37:58.120 --> 00:38:01.840
[Guest]: That book is, you know, what I was saying is, is more interesting than the talk, because

00:38:01.840 --> 00:38:08.400
[Guest]: it really gets into the story about this individual who's a brain scientist.

00:38:08.400 --> 00:38:12.900
[Guest]: And she has a stroke where her left brain is impaired.

00:38:12.900 --> 00:38:16.320
[Guest]: And so she's aware that her left brain is impaired.

00:38:16.320 --> 00:38:21.120
[Guest]: And so it's this whole story about right brain, left brain function, right?

00:38:21.120 --> 00:38:25.120
[Guest]: She said she felt a sense of bliss because the right brain apparently is a parallel processor

00:38:25.120 --> 00:38:30.680
[Guest]: that just sort of takes in the information as it's coming, just streaming in blissfully.

00:38:30.680 --> 00:38:36.320
[Guest]: And the left brain is the logical one, you know, logic, time, reasoning, you know, all

00:38:36.320 --> 00:38:39.920
[Guest]: of that stuff, and how these, you know, two halves communicate.

00:38:39.920 --> 00:38:44.600
[Guest]: So since you mentioned this experiment, I think you'll find this book really fascinating,

00:38:44.600 --> 00:38:50.440
[Guest]: you know, both from sort of a scientific view and also a humanistic view, right?

00:38:50.440 --> 00:38:56.160
[Guest]: Because she describes, you know, her many months in the hospital, you know, and just,

00:38:56.160 --> 00:38:59.960
[Guest]: you know, and she describes this feeling when her mother came to visit her, she could not

00:38:59.960 --> 00:39:00.960
[Guest]: recognize her mother.

00:39:00.960 --> 00:39:04.920
[Guest]: She had no idea who this person was, but she felt a sense of warmth, you know, there was

00:39:04.920 --> 00:39:11.240
[Guest]: something really nice, you know, that was, you know, exuding from that person in that

00:39:11.240 --> 00:39:12.240
[Guest]: aura, right?

00:39:12.240 --> 00:39:17.720
[Guest]: So she had the intuition, which was this right brain kind of thing, but no logic.

00:39:17.720 --> 00:39:23.480
[Guest]: And it's a story about how she actually then learned to sort of bootstrap her left hemisphere,

00:39:23.480 --> 00:39:27.960
[Guest]: you know, and get things back to, you know, as normal as they could be.

00:39:27.960 --> 00:39:34.880
[Guest]: But I think you'll find that book really fascinating, and it's a great TED Talk as well, my stroke

00:39:34.880 --> 00:39:35.880
[Amit Varma]: of insight.

00:39:35.880 --> 00:39:36.880
[Amit Varma]: That's so fascinating.

00:39:36.880 --> 00:39:38.520
[Amit Varma]: I'm absolutely going to look it up.

00:39:38.520 --> 00:39:42.160
[Amit Varma]: There's another larger question I saved for, you know, the last part of the show.

00:39:42.160 --> 00:39:45.440
[Amit Varma]: But since we're on the subject, I think I might as well ask it now, which is, I mean,

00:39:45.440 --> 00:39:48.380
[Amit Varma]: a couple of related questions.

00:39:48.380 --> 00:39:55.400
[Amit Varma]: And one question is that, you know, the more one reads about AI, one realizes that, you

00:39:55.400 --> 00:39:57.160
[Amit Varma]: know, our brains are also machines.

00:39:57.160 --> 00:39:58.880
[Amit Varma]: And in some ways, they are magnificent machines.

00:39:58.880 --> 00:40:02.960
[Amit Varma]: And in some ways, they're very flawed machines, and, you know, so on and so forth.

00:40:02.960 --> 00:40:07.040
[Amit Varma]: So when we learn about AI, it seems to me that there are three possible phases that

00:40:07.040 --> 00:40:12.400
[Amit Varma]: we can go through with regard to learning about AI and what AI can do with data.

00:40:12.400 --> 00:40:14.720
[Amit Varma]: And many people possibly don't reach the third one.

00:40:14.720 --> 00:40:17.760
[Amit Varma]: The first is, of course, excitement, that moment of magic, when you realize what it

00:40:17.760 --> 00:40:21.800
[Amit Varma]: can do, which, you know, you described what happened to you at the lab, and so on and

00:40:21.800 --> 00:40:22.800
[Amit Varma]: so forth.

00:40:22.800 --> 00:40:26.560
[Amit Varma]: Or the first time someone, you know, from a village may see a GPS on a smartphone.

00:40:26.560 --> 00:40:31.000
[Amit Varma]: And that can also be a moment of enormous magic, because my God, I mean, it is magical

00:40:31.000 --> 00:40:34.320
[Amit Varma]: technology, if you think about it, you know, in the 1960s.

00:40:34.320 --> 00:40:38.760
[Amit Varma]: And the next phase, I'm guessing would be and that I see in a lot of policy makers,

00:40:38.760 --> 00:40:43.940
[Amit Varma]: for example, is an arrogance, because data gives them so much power, you know, having

00:40:43.940 --> 00:40:47.240
[Amit Varma]: all these tools at their disposal, gives them so much power.

00:40:47.240 --> 00:40:51.600
[Amit Varma]: And already as it is, you know, humans and government often suffer from what Frederick

00:40:51.600 --> 00:40:54.160
[Amit Varma]: Hayek calls the fatal conceit.

00:40:54.160 --> 00:40:59.580
[Amit Varma]: So you know, when you have so much these great tools at your disposal, does that then accentuate

00:40:59.580 --> 00:41:03.960
[Amit Varma]: the arrogance, where you think that you can, you know, achieve ends, where you think you

00:41:03.960 --> 00:41:07.960
[Amit Varma]: are superhuman, in a sense, with the help of these, whether it's in reordering society

00:41:07.960 --> 00:41:11.760
[Amit Varma]: or in just gaming markets, or whatever it may be.

00:41:11.760 --> 00:41:15.480
[Amit Varma]: And the third phase, I would postulate, and I'm just completely thinking aloud, and I'm

00:41:15.480 --> 00:41:18.520
[Amit Varma]: sure much greater thinkers than me have gone through all of this.

00:41:18.520 --> 00:41:22.520
[Amit Varma]: But you know, after excitement and arrogance, I'm thinking the third phase could well be

00:41:22.520 --> 00:41:27.200
[Amit Varma]: humility, where you realize your own inherent limitations.

00:41:27.200 --> 00:41:31.000
[Amit Varma]: And that sort of that veil of arrogance that makes you believe that you're something special

00:41:31.000 --> 00:41:35.280
[Amit Varma]: in the universe, that kind of falls apart, where you realize that you are actually in

00:41:35.280 --> 00:41:41.080
[Amit Varma]: so many ways, besides being mortal, of course, you are in so many ways, hopelessly inadequate,

00:41:41.080 --> 00:41:44.480
[Amit Varma]: even in this one thing that we assume that we can do better than other animals.

00:41:44.480 --> 00:41:47.560
[Guest]: There's a whole bunch of stuff in what you've just said.

00:41:47.560 --> 00:41:49.940
[Guest]: So let me take it at two levels.

00:41:49.940 --> 00:41:54.400
[Guest]: One is just the capabilities of AI.

00:41:54.400 --> 00:42:00.640
[Guest]: And the second part, which you were getting at, which is, you know, what you call arrogance,

00:42:00.640 --> 00:42:07.040
[Guest]: but it's sort of this unbridled use of data and the power that you get from doing it.

00:42:07.040 --> 00:42:12.280
[Guest]: And what that says about how we organize society, because that's probably one of the biggest

00:42:12.280 --> 00:42:17.320
[Guest]: questions of our time right now is, you know, the internet was supposed to be free, right?

00:42:17.320 --> 00:42:21.920
[Guest]: It was supposed to be free, empowering, all that kind of stuff.

00:42:21.920 --> 00:42:26.280
[Guest]: I don't know how you feel, but to me, the internet of today doesn't feel particularly

00:42:26.280 --> 00:42:29.760
[Guest]: empowering in some ways, but very much so in others.

00:42:29.760 --> 00:42:37.280
[Guest]: It's become a much more complicated space than it used to be, you know, 25 years ago,

00:42:37.280 --> 00:42:39.840
[Guest]: when it was sort of in its relative infancy.

00:42:39.840 --> 00:42:44.160
[Guest]: And we're seeing different countries adopt different models of governance, you know,

00:42:44.160 --> 00:42:45.160
[Guest]: of data and the internet.

00:42:45.160 --> 00:42:49.600
[Guest]: And, you know, I've described four of these, you know, being the US, Europe, India, and

00:42:49.600 --> 00:42:50.600
[Guest]: China.

00:42:50.600 --> 00:42:52.920
[Guest]: And we can come back to that in a little bit.

00:42:52.920 --> 00:42:59.320
[Guest]: But that's one of the big questions of our time is, you know, how will we govern AI?

00:42:59.320 --> 00:43:03.800
[Guest]: You know, how will we govern these platforms, which have become AI platforms, essentially,

00:43:03.800 --> 00:43:07.360
[Guest]: because they're like very data intensive, very automated.

00:43:07.360 --> 00:43:08.880
[Guest]: So that's one of the big questions.

00:43:08.880 --> 00:43:14.440
[Guest]: But there's another thing you talked about, which is this sort of excitement and arrogance.

00:43:14.440 --> 00:43:20.080
[Guest]: So, you know, I want to say one thing, which is that as excited as I am about AI and the

00:43:20.080 --> 00:43:26.000
[Guest]: progress we've made, we're still in the Bronze Age, right?

00:43:26.000 --> 00:43:30.400
[Guest]: These are like very early stages of AI, right?

00:43:30.400 --> 00:43:35.600
[Guest]: Machines have gotten incredibly good, but that's relative to what they were, which is

00:43:35.600 --> 00:43:37.760
[Guest]: incredibly stupid, right?

00:43:37.760 --> 00:43:40.760
[Guest]: So they were stupid, they were dumb, right?

00:43:40.760 --> 00:43:44.400
[Guest]: Now they're pretty amazing relative to our expectations, right?

00:43:44.400 --> 00:43:50.160
[Guest]: And you talked about, you know, people have become sort of blase about all these, you

00:43:50.160 --> 00:43:51.280
[Guest]: know, advances.

00:43:51.280 --> 00:43:52.840
[Guest]: And it's absolutely true.

00:43:52.840 --> 00:43:53.840
[Guest]: Like my students, right?

00:43:53.840 --> 00:43:59.680
[Guest]: I mean, I remember the first time, you know, Watson became the Jeopardy champion, right?

00:43:59.680 --> 00:44:01.560
[Guest]: And I asked people, I said, so what do you think of this?

00:44:01.560 --> 00:44:02.560
[Guest]: Like, isn't that amazing?

00:44:02.560 --> 00:44:03.560
[Guest]: And they said, yeah, it's pretty good.

00:44:03.560 --> 00:44:09.040
[Guest]: And I was like, like, what do you mean, it's pretty good, like, you know, I said, you know,

00:44:09.040 --> 00:44:12.360
[Guest]: expectations are just crazy, right?

00:44:12.360 --> 00:44:17.840
[Guest]: Because to me, my expectations were, I guess, so low in retrospect, right?

00:44:17.840 --> 00:44:22.720
[Guest]: The very first time I saw a search engine, and able to type anything in natural language,

00:44:22.720 --> 00:44:27.000
[Guest]: I was amazed that it came up with anything remotely useful, because in my experiences

00:44:27.000 --> 00:44:31.720
[Guest]: with natural language, you know, 20 years ago, it had been that it was just terrible.

00:44:31.720 --> 00:44:36.680
[Guest]: Like machines had no way, they just look at keywords and try and do their best and give

00:44:36.680 --> 00:44:38.360
[Guest]: you something and throw up their hands, right?

00:44:38.360 --> 00:44:40.520
[Guest]: They were pretty stupid at the time, right?

00:44:40.520 --> 00:44:44.320
[Guest]: But I was amazed that it would actually give you something useful, right?

00:44:44.320 --> 00:44:48.560
[Guest]: Now the fact that you can talk to Alexa or Siri or whatever, it just like boggles my

00:44:48.560 --> 00:44:49.560
[Guest]: mind, right?

00:44:49.560 --> 00:44:55.920
[Guest]: That I never imagined I'd see this in my lifetime, you know, that we have systems that we can

00:44:55.920 --> 00:45:02.120
[Guest]: actually talk to, and that seem to understand quote, unquote, a fair amount of what we're

00:45:02.120 --> 00:45:06.720
[Guest]: saying, at least understand it at sufficient depth to provide a useful answer, right?

00:45:06.720 --> 00:45:08.640
[Guest]: So that's pretty amazing, right?

00:45:08.640 --> 00:45:10.380
[Guest]: So we've come a long ways, right?

00:45:10.380 --> 00:45:13.840
[Guest]: But I want to come back to this, which is we're still in the Bronze Age, we've got

00:45:13.840 --> 00:45:20.200
[Guest]: a long way to go here, you know, so I talked about, you know, that medical diagnostic system

00:45:20.200 --> 00:45:24.720
[Guest]: that I first came across in the 79, that I thought was amazing, right?

00:45:24.720 --> 00:45:26.600
[Guest]: It was engaging in a dialogue.

00:45:26.600 --> 00:45:34.000
[Guest]: Now, the difference is that you can actually feed the system an X-ray image, you can feed

00:45:34.000 --> 00:45:39.160
[Guest]: it all of this data that you're seeing as a physician, which you couldn't earlier on,

00:45:39.160 --> 00:45:40.160
[Guest]: right?

00:45:40.160 --> 00:45:44.000
[Guest]: You have to look at the image, you have to see it, and then enter it into the system,

00:45:44.000 --> 00:45:46.600
[Guest]: and then the system will quote, unquote, reason with it, right?

00:45:46.600 --> 00:45:51.080
[Guest]: Whereas now we've gotten to the point where machines have become good at seeing things,

00:45:51.080 --> 00:45:54.400
[Guest]: and so that gets tossed into the mix as well.

00:45:54.400 --> 00:46:01.080
[Guest]: But are we at that point yet where machines form at, you know, function at an equivalent

00:46:01.080 --> 00:46:03.720
[Guest]: level that human physicians do?

00:46:03.720 --> 00:46:05.840
[Guest]: For the most part, not really, right?

00:46:05.840 --> 00:46:10.400
[Guest]: Humans still hold that edge because of our intelligence, and humans and machines see

00:46:10.400 --> 00:46:14.320
[Guest]: the world very differently, by the way, and we can come back to this a little bit later,

00:46:14.320 --> 00:46:19.400
[Guest]: but we're not at that stage yet, because machines still make lots of mistakes, and this is something

00:46:19.400 --> 00:46:23.680
[Guest]: that I've talked about a lot in my research as well, about, you know, when do we trust

00:46:23.680 --> 00:46:28.920
[Guest]: AI systems, and my simple theory is we trust them when, you know, they don't make too many

00:46:28.920 --> 00:46:32.640
[Guest]: mistakes and those mistakes don't have very severe consequences, right?

00:46:32.640 --> 00:46:39.240
[Guest]: You know, to put it simply, and at the moment, we're just grappling with trying to understand

00:46:39.240 --> 00:46:43.560
[Guest]: this in more detail, like what kinds of mistakes will driverless cars make, right?

00:46:43.560 --> 00:46:49.520
[Guest]: Will they, you know, plow over kids routinely, you know, will they slam into poles or whatever,

00:46:49.520 --> 00:46:50.520
[Guest]: right?

00:46:50.520 --> 00:46:55.400
[Guest]: You know, someone told me, a chief scientist from one of these driverless car efforts told

00:46:55.400 --> 00:47:00.240
[Guest]: me that if you drive in driverless mode these days, you'll probably die four or five times

00:47:00.240 --> 00:47:01.240
[Guest]: in a year, right?

00:47:01.240 --> 00:47:06.360
[Guest]: So, we're not good enough yet in terms of driverless cars, but the larger question really

00:47:06.360 --> 00:47:13.680
[Guest]: is in all of these domains where we apply AI, you know, whether it is medicine, healthcare,

00:47:13.680 --> 00:47:20.360
[Guest]: or whether it's payments or, you know, whatever you can think of, the question that we always

00:47:20.360 --> 00:47:27.840
[Guest]: have to ask ourselves is, you know, what can go wrong, and how do you really avoid catastrophes

00:47:27.840 --> 00:47:29.040
[Guest]: from happening, right?

00:47:29.040 --> 00:47:35.880
[Guest]: How do we really bridle the power of AI without really exposing ourselves to these huge risks

00:47:35.880 --> 00:47:38.720
[Guest]: that these machines can subject ourselves to?

00:47:38.720 --> 00:47:43.840
[Guest]: So, even when you talk about, you know, you mentioned data and, you know, I don't know

00:47:43.840 --> 00:47:48.560
[Guest]: whether you mentioned Big Brother or observing society and having access to all these things,

00:47:48.560 --> 00:47:54.520
[Guest]: you know, the question is like, what can go wrong, and are you willing to deal with the

00:47:54.520 --> 00:47:55.520
[Guest]: consequences?

00:47:55.520 --> 00:48:01.840
[Guest]: It reminds me of a talk by James Robinson several years ago, you know, he wrote this

00:48:01.840 --> 00:48:07.640
[Guest]: book called Why Nations Fail, and it's all about the importance of institutions and checks

00:48:07.640 --> 00:48:12.960
[Guest]: and balances and that, you know, societies that have more advanced institutions generally

00:48:12.960 --> 00:48:17.320
[Guest]: tend to do better as opposed to the ones that don't, and I remember after his talk, and

00:48:17.320 --> 00:48:22.080
[Guest]: this was in Goa, he and I went out into town and I asked him, I said, you know, are your

00:48:22.080 --> 00:48:26.840
[Guest]: assumptions still valid in the age of AI and data?

00:48:26.840 --> 00:48:30.120
[Guest]: Because what I see China doing blows my mind, right?

00:48:30.120 --> 00:48:35.360
[Guest]: Maybe the assumptions you're making are assumptions of an old era where information flowed like

00:48:35.360 --> 00:48:41.000
[Guest]: molasses and people weren't informed, and so, you know, decentralization was better

00:48:41.000 --> 00:48:45.440
[Guest]: and let people close to the phenomenon make the decisions, right?

00:48:45.440 --> 00:48:49.320
[Guest]: And let's have strong institutions with checks and balances, and my question to him was,

00:48:49.320 --> 00:48:56.160
[Guest]: are we in a different era now where because there's so much data, there's so much information

00:48:56.160 --> 00:49:01.720
[Guest]: that we can actually have very effective centralized control, and I believe that the Chinese government

00:49:01.720 --> 00:49:03.400
[Guest]: believes that, right?

00:49:03.400 --> 00:49:11.280
[Guest]: I think they actually believe that their leadership has the wisdom that given the power they will

00:49:11.280 --> 00:49:15.800
[Guest]: wield it in a responsible way, right?

00:49:15.800 --> 00:49:19.680
[Guest]: It remains to be seen what, you know, how things will look 20 years from now.

00:49:19.680 --> 00:49:26.120
[Guest]: It'll be a really fascinating, you know, experiment, this one, as to, you know, where China is

00:49:26.120 --> 00:49:33.160
[Guest]: 10 or 20 years from now with its very centralized model of data gathering and control and decision

00:49:33.160 --> 00:49:38.960
[Guest]: making versus, you know, these democratic societies that are just much more freewheeling

00:49:38.960 --> 00:49:40.120
[Guest]: and messy, right?

00:49:40.120 --> 00:49:45.960
[Guest]: Now, we've seen that with Corona, the centralized model has actually worked better, right?

00:49:45.960 --> 00:49:50.880
[Guest]: South Korea is my favorite example where, you know, they have two to four deaths a day,

00:49:50.880 --> 00:49:51.880
[Guest]: right?

00:49:51.880 --> 00:49:54.000
[Guest]: That's amazing statistic, right?

00:49:54.000 --> 00:49:59.520
[Guest]: That's because there's a congruence between what the government is trying to do and what

00:49:59.520 --> 00:50:00.720
[Guest]: individuals want, right?

00:50:00.720 --> 00:50:06.000
[Guest]: They all want to keep deaths down and infection rates low, and so individuals are willing

00:50:06.000 --> 00:50:08.720
[Guest]: to trust the government with this one, right?

00:50:08.720 --> 00:50:12.400
[Guest]: But it's a really messy kind of a question, right?

00:50:12.400 --> 00:50:16.720
[Guest]: It doesn't have an easy answer as to like, where do you really draw the line between

00:50:16.720 --> 00:50:20.840
[Guest]: people in control saying, yeah, you know, we're responsible, trust us.

00:50:20.840 --> 00:50:27.740
[Guest]: If we just had the right data, we do the right thing, which South Korea has done and demonstrated.

00:50:27.740 --> 00:50:33.960
[Guest]: So it shows that it is possible for individuals and the government to work together, you know,

00:50:33.960 --> 00:50:38.120
[Guest]: to solve a larger problem for the larger social good.

00:50:38.120 --> 00:50:41.680
[Guest]: But how we do this, and by the way, South Korea is a democracy, right?

00:50:41.680 --> 00:50:47.640
[Guest]: But you know, so different models, at least for COVID, have worked very differently, but

00:50:47.640 --> 00:50:50.760
[Guest]: the larger question that you ask is a fascinating one, right?

00:50:50.760 --> 00:50:57.320
[Guest]: Which is, how do you really find that balance between the arrogance and the capability,

00:50:57.320 --> 00:50:58.320
[Guest]: right?

00:50:58.320 --> 00:51:02.520
[Guest]: The arrogance of people in charge saying, yeah, if we have the data, we can do amazing

00:51:02.520 --> 00:51:07.400
[Guest]: things versus them actually doing some amazing things, but doing some pretty screwed up things

00:51:07.400 --> 00:51:08.840
[Guest]: as well, right?

00:51:08.840 --> 00:51:12.800
[Guest]: Because they can't help it and they have access to the data and they don't have accountability,

00:51:12.800 --> 00:51:13.800
[Guest]: right?

00:51:13.800 --> 00:51:18.320
[Guest]: So this is what we're dealing with, and this is what I see lawmakers dealing with in the

00:51:18.320 --> 00:51:26.360
[Guest]: US, in Europe, in India, and I'm sure the Chinese are thinking about this as well.

00:51:26.360 --> 00:51:31.800
[Guest]: So it's a fascinating situation which we're in, which is that the internet isn't what

00:51:31.800 --> 00:51:32.800
[Guest]: it used to be.

00:51:32.800 --> 00:51:38.880
[Guest]: It was freewheeling internet, but it's something that's become so powerful now that we need

00:51:38.880 --> 00:51:40.880
[Guest]: to figure out how to govern it.

00:51:40.880 --> 00:51:46.120
[Guest]: And the choices we make in the next few years will have a huge influence on society in these

00:51:46.120 --> 00:51:47.120
[Guest]: various countries.

00:51:47.120 --> 00:51:50.560
[Amit Varma]: So you've touched on many things there, and I'm going to, you know, through the course

00:51:50.560 --> 00:51:54.160
[Amit Varma]: of this conversation, come to each of these big questions separately.

00:51:54.160 --> 00:51:56.360
[Amit Varma]: For example, when should we trust data?

00:51:56.360 --> 00:51:59.440
[Amit Varma]: You know, what are the dangers of AI going too far?

00:51:59.440 --> 00:52:01.680
[Amit Varma]: What are the dangers of states going too far?

00:52:01.680 --> 00:52:04.800
[Amit Varma]: Are we using AI for their totalitarian purposes?

00:52:04.800 --> 00:52:07.000
[Amit Varma]: How should we govern the use of AI today?

00:52:07.000 --> 00:52:08.360
[Amit Varma]: All these are big questions that come to them.

00:52:08.360 --> 00:52:13.520
[Amit Varma]: But first, a sort of response or even a musing about what you said about the internet, which

00:52:13.520 --> 00:52:17.840
[Amit Varma]: sort of also brings up a related, both an ethical question and also a question of how

00:52:17.840 --> 00:52:23.400
[Amit Varma]: humans regard themselves, which is, you know, and I completely agree with you that the internet

00:52:23.400 --> 00:52:27.000
[Amit Varma]: has, you know, enabled amazing progress in various ways.

00:52:27.000 --> 00:52:32.020
[Amit Varma]: But also it has made our discourse so polarized and toxic and cause so many dangers.

00:52:32.020 --> 00:52:36.480
[Amit Varma]: And I think part of the reason we were all sort of hopeful about the internet at one

00:52:36.480 --> 00:52:42.760
[Amit Varma]: point in time, and I think more or less correctly, so is that we assume that it would, you know,

00:52:42.760 --> 00:52:48.080
[Amit Varma]: enable various aspects of humanity to express themselves to their fullest.

00:52:48.080 --> 00:52:53.120
[Amit Varma]: But that held an implicit assumption, which I think was wrong, which is that, you know,

00:52:53.120 --> 00:52:57.040
[Amit Varma]: all of those getting expressed inevitably means a march towards virtue or a march towards

00:52:57.040 --> 00:53:01.280
[Amit Varma]: a better world, which is not the case, because there are many aspects of human personality

00:53:01.280 --> 00:53:06.600
[Amit Varma]: which are very ugly, for example, our tribal instincts are, you know, what we see on the

00:53:06.600 --> 00:53:12.920
[Amit Varma]: internet today, for example, is that, especially on social media, that there is constantly

00:53:12.920 --> 00:53:18.180
[Amit Varma]: a move towards the extremes, people form ideological echo chambers, confirmation bias kicks in

00:53:18.180 --> 00:53:20.040
[Amit Varma]: when they look at the world around them.

00:53:20.040 --> 00:53:24.000
[Amit Varma]: You know, dialogue has stopped, people aren't talking to each other, but past each other

00:53:24.000 --> 00:53:25.000
[Amit Varma]: and at each other.

00:53:25.000 --> 00:53:30.080
[Amit Varma]: And all of that, people are constantly signaling to raise their status within their own in

00:53:30.080 --> 00:53:31.080
[Amit Varma]: groups.

00:53:31.080 --> 00:53:33.960
[Amit Varma]: And as a consequence, everything is driven to extremism.

00:53:33.960 --> 00:53:37.880
[Amit Varma]: And also that reality doesn't matter so much as narratives.

00:53:37.880 --> 00:53:41.520
[Amit Varma]: And you basically pick your own narrative, and then you live in that world constructed

00:53:41.520 --> 00:53:43.940
[Amit Varma]: in your head, and you can just get away with that.

00:53:43.940 --> 00:53:48.800
[Amit Varma]: And there are a lot of bad consequences of that the one that I see most often in what

00:53:48.800 --> 00:53:52.000
[Amit Varma]: I do is just how bad the political discourse has become.

00:53:52.000 --> 00:53:56.320
[Amit Varma]: But then the ethical question that comes up, and I've had episodes in the past on this,

00:53:56.320 --> 00:54:02.040
[Amit Varma]: as well, where I've been grappling with this issue, is that a lot of these consequences

00:54:02.040 --> 00:54:07.880
[Amit Varma]: come from the voluntary actions of humans, that, you know, if I, for example, want to

00:54:07.880 --> 00:54:13.640
[Amit Varma]: be in an ideological echo chamber, and I want to consume only the sort of narratives that

00:54:13.640 --> 00:54:18.600
[Amit Varma]: satisfy me without giving a damn about whether they conform to reality or not, then the question

00:54:18.600 --> 00:54:23.760
[Amit Varma]: that arises is that one should somebody get in the way of my voluntary choices?

00:54:23.760 --> 00:54:25.720
[Amit Varma]: And would it be correct for them to do so?

00:54:25.720 --> 00:54:28.180
[Amit Varma]: So that's really an ethical question.

00:54:28.180 --> 00:54:31.160
[Amit Varma]: Because then you're telling people what is good for them, or what they should do.

00:54:31.160 --> 00:54:35.460
[Amit Varma]: And that becomes a very paternalistic kind of approach.

00:54:35.460 --> 00:54:39.280
[Amit Varma]: And also whether that will make a difference anyway, because now that we've been empowered

00:54:39.280 --> 00:54:43.760
[Amit Varma]: with tools that allow us, for example, just to take one domain that allow us to believe

00:54:43.760 --> 00:54:47.560
[Amit Varma]: whatever narrative we want about the world, we are going to do that anyway, you might

00:54:47.560 --> 00:54:53.260
[Amit Varma]: say that no, the algorithm should give me, you know, more broad based views and news

00:54:53.260 --> 00:54:57.080
[Amit Varma]: from across the place, but I will still believe what I want.

00:54:57.080 --> 00:55:00.760
[Amit Varma]: So I mean, this is a bit of a rambling, but any reactions?

00:55:00.760 --> 00:55:03.640
[Guest]: No, it's a it's a fascinating question, right?

00:55:03.640 --> 00:55:09.080
[Guest]: What you're really touching on is, well, the internet was supposed to be free, aren't we

00:55:09.080 --> 00:55:10.360
[Guest]: free?

00:55:10.360 --> 00:55:16.560
[Guest]: So what if I have a bias, I should express it freely.

00:55:16.560 --> 00:55:19.920
[Guest]: After all, I'm doing it voluntarily.

00:55:19.920 --> 00:55:22.480
[Guest]: And I can completely see the logic for that.

00:55:22.480 --> 00:55:26.160
[Guest]: The question you have to ask yourself is, is it voluntary, right?

00:55:26.160 --> 00:55:32.440
[Guest]: Is your response voluntary, or is it orchestrated by an algorithm, right?

00:55:32.440 --> 00:55:40.880
[Guest]: And the disturbing thing that I think we're seeing sufficient evidence of is that if I

00:55:40.880 --> 00:55:47.600
[Guest]: have an AI machine, that machine has, you know, what we call an objective function,

00:55:47.600 --> 00:55:48.600
[Guest]: right?

00:55:48.600 --> 00:55:53.680
[Guest]: That machine is driven, its behavior is driven by an objective function, right?

00:55:53.680 --> 00:55:58.800
[Guest]: So in healthcare, if I'm diagnosing cases, you know, I have a machine that's learning

00:55:58.800 --> 00:56:03.040
[Guest]: how to diagnose cases, it's trying to minimize the errors it makes, right?

00:56:03.040 --> 00:56:07.360
[Guest]: That's its objective function is to minimize error, right?

00:56:07.360 --> 00:56:08.900
[Guest]: Minimize misdiagnosis.

00:56:08.900 --> 00:56:10.200
[Guest]: So that's how it learns.

00:56:10.200 --> 00:56:13.600
[Guest]: Now, let's take this in a social context, right?

00:56:13.600 --> 00:56:20.560
[Guest]: Now I have an algorithm, and it's an algorithm, and I'm making money through, let's say, advertising.

00:56:20.560 --> 00:56:26.120
[Guest]: And now I tell the algorithm, hey, just go for it, maximize my advertising revenue, you

00:56:26.120 --> 00:56:27.880
[Guest]: figure out the best way to do it, right?

00:56:27.880 --> 00:56:34.720
[Guest]: I'm not going to tell you, and the algorithm goes, you know, it does its hocus pocus.

00:56:34.720 --> 00:56:44.840
[Guest]: And it realizes that, oh, people are more likely to accept friend suggestions when they

00:56:44.840 --> 00:56:50.080
[Guest]: see that you've got more people in common with this friend that's just been suggested

00:56:50.080 --> 00:56:54.040
[Guest]: to you by this algorithm, right?

00:56:54.040 --> 00:57:00.300
[Guest]: And the more likely it is that you accept the invitation, the more likely it is now

00:57:00.300 --> 00:57:10.440
[Guest]: that you're connected more densely to this cluster of people, and this more dense connection

00:57:10.440 --> 00:57:17.080
[Guest]: increases your engagement with the platform, you now spend more time on it.

00:57:17.080 --> 00:57:22.560
[Guest]: And now that your engagement has increased, we find that, hey, like our revenues go up

00:57:22.560 --> 00:57:25.840
[Guest]: when people are more engaged, surprise, surprise, right?

00:57:25.840 --> 00:57:30.040
[Guest]: Now, if you're the operator of that algorithm, what are you going to tell it to do, stop?

00:57:30.040 --> 00:57:34.880
[Guest]: No, you've got an obligation to your shareholders, right, to maximize ad revenue.

00:57:34.880 --> 00:57:38.960
[Guest]: Well, you're going to tell the algo, just go for it, you know, keep closing these triangles

00:57:38.960 --> 00:57:40.520
[Guest]: all day.

00:57:40.520 --> 00:57:45.160
[Guest]: And sure, we get echo chambers, but what the hell, that's someone else's problem, right?

00:57:45.160 --> 00:57:49.440
[Guest]: It's almost like factories in the old days that polluted the rivers and said, yeah, we're

00:57:49.440 --> 00:57:51.360
[Guest]: just polluting the river and polluting the air.

00:57:51.360 --> 00:57:53.280
[Guest]: But for us, it's free.

00:57:53.280 --> 00:57:56.720
[Guest]: It's society's problem to clean that shit up, right?

00:57:56.720 --> 00:58:02.720
[Guest]: That's what economists called an externality that, you know, General Electric and it's,

00:58:02.720 --> 00:58:06.800
[Guest]: you know, nuclear power, whatever, is polluting the Hudson.

00:58:06.800 --> 00:58:11.280
[Guest]: Well, you know, and then we realized, gee, that's bad for society, we're going to fine

00:58:11.280 --> 00:58:12.600
[Guest]: them for doing that, right?

00:58:12.600 --> 00:58:17.720
[Guest]: So we have the EPA that comes in and says, okay, now we have Environment Protection Agency

00:58:17.720 --> 00:58:19.600
[Guest]: because you guys are polluting the environment, right?

00:58:19.600 --> 00:58:24.020
[Guest]: We need to protect the environment because you're polluting it, right?

00:58:24.020 --> 00:58:28.640
[Guest]: And you're causing rates of cancer to go up, people are sick, they're dying, right?

00:58:28.640 --> 00:58:30.280
[Guest]: We need to do something.

00:58:30.280 --> 00:58:35.200
[Guest]: It's the same thing happening in social media, if you really think about it, right?

00:58:35.200 --> 00:58:38.080
[Guest]: But the trouble is, we don't really understand it well enough, right?

00:58:38.080 --> 00:58:43.460
[Guest]: We do know there's plenty of evidence to suggest that these algorithms do some nasty stuff

00:58:43.460 --> 00:58:47.900
[Guest]: as a side effect of the fact that they're trying to maximize engagement.

00:58:47.900 --> 00:58:51.160
[Guest]: So they're just trying, they're just doing what they've been told to do, right?

00:58:51.160 --> 00:58:56.560
[Guest]: And they don't know that they're causing, you know, teen suicide rates to go up or things

00:58:56.560 --> 00:58:57.560
[Guest]: like that, right?

00:58:57.560 --> 00:59:02.640
[Guest]: So when, you know, when I talk to high school counselors, they say that they are seeing

00:59:02.640 --> 00:59:06.440
[Guest]: increased levels of stress among teenagers, right?

00:59:06.440 --> 00:59:07.440
[Guest]: What's causing it?

00:59:07.440 --> 00:59:08.640
[Guest]: I don't know, right?

00:59:08.640 --> 00:59:14.380
[Guest]: But we can't really figure these things out unless we have some data to be able to measure

00:59:14.380 --> 00:59:19.880
[Guest]: these things and be able to correlate them with what's going on in society, such as social

00:59:19.880 --> 00:59:20.880
[Guest]: media platforms.

00:59:20.880 --> 00:59:22.760
[Guest]: At the moment, we don't have this data.

00:59:22.760 --> 00:59:24.160
[Guest]: So the jury's out.

00:59:24.160 --> 00:59:29.160
[Guest]: But this is the kind of stuff we need to be thinking about, which is, are these things

00:59:29.160 --> 00:59:39.400
[Guest]: really voluntary or are you just being a tool that the algo is kind of, you know, playing

00:59:39.400 --> 00:59:43.600
[Guest]: around with and the algo is not being evil.

00:59:43.600 --> 00:59:46.820
[Guest]: It's just doing its thing in accordance with its objective function, but it's creating

00:59:46.820 --> 00:59:51.080
[Guest]: a mess as a side effect of what it's doing.

00:59:51.080 --> 00:59:53.480
[Guest]: And that's what we're seeing emerging these days.

00:59:53.480 --> 00:59:58.320
[Guest]: And that's why you have all this concern and lawmakers, you know, both Democrats and Republicans

00:59:58.320 --> 01:00:04.160
[Guest]: accusing social media platforms of being biased, of spreading misinformation, you know, all

01:00:04.160 --> 01:00:05.520
[Guest]: that kind of stuff.

01:00:05.520 --> 01:00:09.700
[Guest]: When the truth is that, look, you know, we didn't, we haven't really thought about these

01:00:09.700 --> 01:00:13.840
[Guest]: things and you can't blame these platforms for doing what they're doing, right?

01:00:13.840 --> 01:00:19.640
[Guest]: They haven't, you know, egregiously violated the law.

01:00:19.640 --> 01:00:23.760
[Guest]: You know, they did things that were shady, but within the limits of the law.

01:00:23.760 --> 01:00:27.040
[Guest]: And yes, in the early days, they tell you, we'll protect your data.

01:00:27.040 --> 01:00:28.040
[Guest]: We won't share it.

01:00:28.040 --> 01:00:29.720
[Guest]: And then they say, well, maybe we'll share it.

01:00:29.720 --> 01:00:30.720
[Guest]: And then they said, screw you.

01:00:30.720 --> 01:00:32.600
[Guest]: We're just going to do whatever we want with it.

01:00:32.600 --> 01:00:34.840
[Guest]: If you don't want us, you know, we don't want you, right?

01:00:34.840 --> 01:00:37.360
[Guest]: We are now sufficiently powerful that we don't need you.

01:00:37.360 --> 01:00:39.160
[Guest]: We have all the data anyway.

01:00:39.160 --> 01:00:43.520
[Guest]: So that's the situation we're in where a lot of these platforms have just been bad boys.

01:00:43.520 --> 01:00:44.920
[Guest]: They've behaved badly.

01:00:44.920 --> 01:00:47.600
[Guest]: They've behaved irresponsibly.

01:00:47.600 --> 01:00:49.960
[Guest]: And we're seeing the side effects of that.

01:00:49.960 --> 01:00:55.280
[Guest]: So it's a more nuanced question than saying, well, you know, why should we get in the way

01:00:55.280 --> 01:00:58.640
[Guest]: of freedom, which I completely agree with.

01:00:58.640 --> 01:01:04.080
[Guest]: But I think it's more complicated than that because we're in the whole new world here.

01:01:04.080 --> 01:01:11.280
[Guest]: We're in the whole brave new world here where, you know, algos have assumed a level of control

01:01:11.280 --> 01:01:15.160
[Guest]: that they didn't have, you know, 10 years ago.

01:01:15.160 --> 01:01:20.080
[Amit Varma]: We are indeed in a brave new world here, which is why your new podcast is, you know, much

01:01:20.080 --> 01:01:22.000
[Amit Varma]: awaited by me and others.

01:01:22.000 --> 01:01:25.720
[Amit Varma]: You know, I'll come to the political economy and the governance aspect of what do we do

01:01:25.720 --> 01:01:28.640
[Amit Varma]: about big tech later on in the second half of the show.

01:01:28.640 --> 01:01:33.120
[Amit Varma]: But I'd like to sort of stay on this for a moment to kind of take a step back and get

01:01:33.120 --> 01:01:37.560
[Amit Varma]: a little meta and just look at a sort of a broader philosophical aspect of it.

01:01:37.560 --> 01:01:41.920
[Amit Varma]: Because one, you could argue that there is no such thing as free will, however, we should

01:01:41.920 --> 01:01:44.960
[Amit Varma]: still behave as if there is.

01:01:44.960 --> 01:01:50.120
[Amit Varma]: So instead of people's actions being determined by random events, it is sort of perhaps more

01:01:50.120 --> 01:01:51.120
[Amit Varma]: concentrated.

01:01:51.120 --> 01:01:55.440
[Amit Varma]: But the other aspect of that is like, let me zoom into the political discourse, which

01:01:55.440 --> 01:01:58.240
[Amit Varma]: is something, you know, I kind of think about a fair bit.

01:01:58.240 --> 01:02:03.600
[Amit Varma]: And therefore, I can talk about in some detail, like, I think a lot of what exacerbated like,

01:02:03.600 --> 01:02:09.120
[Amit Varma]: I think, a seminal moment in this kind of political polarization was the introduction

01:02:09.120 --> 01:02:13.920
[Amit Varma]: of the Facebook like button, maybe around 2008 or 2009, or whatever.

01:02:13.920 --> 01:02:19.040
[Amit Varma]: Because then what happens is that people start craving likes, because they get their dopamine

01:02:19.040 --> 01:02:20.040
[Amit Varma]: rushes.

01:02:20.040 --> 01:02:24.080
[Amit Varma]: Similarly, in a Twitter context, they will start craving retweets, because they want

01:02:24.080 --> 01:02:27.200
[Amit Varma]: that dopamine, they want the notifications to fill up with validation.

01:02:27.200 --> 01:02:28.600
[Amit Varma]: Now, where does that come from?

01:02:28.600 --> 01:02:33.680
[Amit Varma]: It comes from a human need for validation, which those tools are playing to which is

01:02:33.680 --> 01:02:34.880
[Amit Varma]: why they are so effective.

01:02:34.880 --> 01:02:39.440
[Amit Varma]: Now, this leads to polarization, because to maximize your likes, and to maximize your

01:02:39.440 --> 01:02:44.000
[Amit Varma]: retweets, you take more and more extreme positions, you do more and more, which you signaling

01:02:44.000 --> 01:02:48.560
[Amit Varma]: whether you are on the left or the right or whatever, you are doing all that and, you

01:02:48.560 --> 01:02:53.840
[Amit Varma]: know, and these echo chambers form and what Cass Sunstein calls group polarization, then

01:02:53.840 --> 01:02:55.360
[Amit Varma]: automatically happens.

01:02:55.360 --> 01:02:57.280
[Amit Varma]: But there are a couple of points here.

01:02:57.280 --> 01:03:02.960
[Amit Varma]: One point is that these functionalities also serve a useful purpose.

01:03:02.960 --> 01:03:08.280
[Amit Varma]: For example, almost every ad that I see these days is targeted towards me and is therefore

01:03:08.280 --> 01:03:12.960
[Amit Varma]: useful to both me and the advertiser in a mutual sense, you know, when I go to YouTube,

01:03:12.960 --> 01:03:18.640
[Amit Varma]: I'm damn happy that their algorithm is tracking my preferences to such a fine degree, because

01:03:18.640 --> 01:03:24.480
[Amit Varma]: it is serving up much more relevant content, and is acting as a fantastic filter in that

01:03:24.480 --> 01:03:25.480
[Amit Varma]: sense.

01:03:25.480 --> 01:03:30.480
[Amit Varma]: The other aspect of it is that there is nothing teleological or intentional about this.

01:03:30.480 --> 01:03:35.600
[Amit Varma]: It's one thing that say when a foreign country says that I want to, you know, spread misinformation,

01:03:35.600 --> 01:03:39.840
[Amit Varma]: and I want to influence an election, there is an intent, there's a teleology there, that's

01:03:39.840 --> 01:03:45.240
[Amit Varma]: not necessarily the case when it comes to a social media platform, putting something

01:03:45.240 --> 01:03:50.680
[Amit Varma]: there that actually makes your experience pleasurable in certain ways, like the Facebook

01:03:50.680 --> 01:03:53.920
[Amit Varma]: like button or the Twitter retweet button.

01:03:53.920 --> 01:03:57.840
[Amit Varma]: So you know, to cut a long story short, what I'm coming at is that all the problems that

01:03:57.840 --> 01:04:03.160
[Amit Varma]: we see emerging out of social media and out of the internet and all of that, and I'm not

01:04:03.160 --> 01:04:08.200
[Amit Varma]: using this as an argument for one or the other, I mean, this just in a descriptive sense,

01:04:08.200 --> 01:04:11.520
[Amit Varma]: these are problems inherent in humanity, not in technology.

01:04:11.520 --> 01:04:16.440
[Amit Varma]: It just so happens that technology has, for example, played on this need for validation

01:04:16.440 --> 01:04:20.040
[Amit Varma]: and you know, created this effect and so on and so forth.

01:04:20.040 --> 01:04:25.480
[Amit Varma]: But the deeper thing that we perhaps need to think about and that should give us a cause

01:04:25.480 --> 01:04:30.400
[Amit Varma]: to ponder and perhaps just be more humble about our species as a whole, is that we are

01:04:30.400 --> 01:04:32.640
[Amit Varma]: deeply flawed and messed up.

01:04:32.640 --> 01:04:36.160
[Amit Varma]: And you know, a lot of that finds expression through technology nowadays.

01:04:36.160 --> 01:04:37.160
[Guest]: Absolutely.

01:04:37.160 --> 01:04:41.980
[Guest]: We are very deeply flawed and quite deeply messed up.

01:04:41.980 --> 01:04:45.920
[Guest]: And our history has been quite brutal, right?

01:04:45.920 --> 01:04:54.120
[Guest]: I mean, we've come from, I guess, you know, being savages to being feudal, you know, I

01:04:54.120 --> 01:04:59.080
[Guest]: mean, so we, you know, humanity has gone through sort of, you know, you know, savagery, you

01:04:59.080 --> 01:05:05.320
[Guest]: know, followed by slavery and then feudalism, you know, and then law, right?

01:05:05.320 --> 01:05:07.840
[Guest]: So we've kind of then created societies around law, right?

01:05:07.840 --> 01:05:09.920
[Guest]: So that's been sort of the progression of humanity.

01:05:09.920 --> 01:05:11.600
[Guest]: It's been a positive progression.

01:05:11.600 --> 01:05:17.240
[Guest]: And now we're in the age of tech where we'd actually implement law so much more easily

01:05:17.240 --> 01:05:20.240
[Guest]: and actually it'll turbocharge law as well.

01:05:20.240 --> 01:05:28.960
[Guest]: So we've had like a brutal history, but I think the solution is not control because

01:05:28.960 --> 01:05:33.200
[Guest]: who the hell knows what the right amount of control and what the right control is, right?

01:05:33.200 --> 01:05:36.560
[Guest]: The solution isn't control, but it's transparency, right?

01:05:36.560 --> 01:05:42.960
[Guest]: We need to be aware of our savage instincts, of our limitations, right?

01:05:42.960 --> 01:05:50.840
[Guest]: So we need to become more self-aware and that's where I see the solution, right?

01:05:50.840 --> 01:05:52.960
[Guest]: So I, you know, so I tend to be an optimist, right?

01:05:52.960 --> 01:06:00.640
[Guest]: I'm not, you know, despite these dysfunctional kind of, you know, dark scenarios that people

01:06:00.640 --> 01:06:04.320
[Guest]: paint about the future, I actually think the future is going to be pretty amazing because

01:06:04.320 --> 01:06:13.920
[Guest]: I have optimism in the ability of mankind to sort of, you know, rescue ourselves from

01:06:13.920 --> 01:06:15.760
[Guest]: the precipice, right?

01:06:15.760 --> 01:06:16.760
[Guest]: The stakes are too high.

01:06:16.760 --> 01:06:21.360
[Guest]: We, you know, we don't want to like destroy our society, you know, we've come close at

01:06:21.360 --> 01:06:27.880
[Guest]: a few points, you know, with the cold war and things, but we've sort of been aware of,

01:06:27.880 --> 01:06:34.280
[Guest]: you know, of some of these risks that technology has introduced in society, right?

01:06:34.280 --> 01:06:38.320
[Guest]: The fact that we can destroy ourselves, you know, with a push of a few buttons, right?

01:06:38.320 --> 01:06:43.560
[Guest]: So we're in a society that has tremendous destructive potential.

01:06:43.560 --> 01:06:44.680
[Guest]: And the internet is similar.

01:06:44.680 --> 01:06:50.240
[Guest]: It has some amazing positive potential, much of which we've seen, right?

01:06:50.240 --> 01:06:56.560
[Guest]: But we're beginning to see some of these dysfunctional aspects of the interplay between technology

01:06:56.560 --> 01:06:57.560
[Guest]: and humans.

01:06:57.560 --> 01:07:00.360
[Guest]: I agree with you that it's, there's nothing evil about the technology.

01:07:00.360 --> 01:07:04.660
[Guest]: The technology itself, one might even argue it's neutral.

01:07:04.660 --> 01:07:08.200
[Guest]: It's the way it's employed that we need to be aware of.

01:07:08.200 --> 01:07:11.600
[Guest]: And that's what governance is really all about, right?

01:07:11.600 --> 01:07:17.120
[Guest]: That's what internet governance is really all about, about being aware of our limitations,

01:07:17.120 --> 01:07:21.840
[Guest]: about being aware of our history, about being aware of how we're interacting with these

01:07:21.840 --> 01:07:26.920
[Guest]: platforms and going into this with our eyes open and then saying, hey, what's the best

01:07:26.920 --> 01:07:28.360
[Guest]: way to govern?

01:07:28.360 --> 01:07:31.040
[Guest]: The trouble is when things get dark, right?

01:07:31.040 --> 01:07:35.220
[Guest]: When people function in the dark, that's when shit happens, right?

01:07:35.220 --> 01:07:39.280
[Guest]: And we've seen this in every industry, like finance, like, you know, Wall Street were

01:07:39.280 --> 01:07:43.080
[Guest]: the bad boys for decades, they were the flogging horse, right?

01:07:43.080 --> 01:07:45.080
[Guest]: It used to be an expression, look at Wall Street, right?

01:07:45.080 --> 01:07:47.600
[Guest]: Wall Street was synonymous with evil, right?

01:07:47.600 --> 01:07:52.100
[Guest]: Because people had gotten greedy, you know, they would treat customers badly.

01:07:52.100 --> 01:07:58.400
[Guest]: They would do things that were unethical because regulation didn't exist that was, you know,

01:07:58.400 --> 01:08:00.560
[Guest]: geared towards finding that, right?

01:08:00.560 --> 01:08:03.660
[Guest]: So in a sense, regulators always sort of behind the eight ball, right?

01:08:03.660 --> 01:08:08.080
[Guest]: People close to the ground find ways to function, find ways around the system and bankers were

01:08:08.080 --> 01:08:09.080
[Guest]: no different, right?

01:08:09.080 --> 01:08:14.000
[Guest]: They were functioning within the law, but were doing unethical things, right?

01:08:14.000 --> 01:08:16.440
[Guest]: And that led to all kinds of problems.

01:08:16.440 --> 01:08:22.160
[Guest]: There were lots of NASDAQ dealers that got fined in the 90s, you know, for throwing customers'

01:08:22.160 --> 01:08:28.520
[Guest]: orders into the garbage or not answering phones and, you know, stuff like that, just bad stuff.

01:08:28.520 --> 01:08:33.680
[Guest]: And so regulation came in and said, hey, you know, like we see this thing going on, we

01:08:33.680 --> 01:08:38.660
[Guest]: have to, you know, put some constraints around it so that these people don't destabilize

01:08:38.660 --> 01:08:39.660
[Guest]: the system.

01:08:39.660 --> 01:08:41.660
[Guest]: They don't favor customers over others, right?

01:08:41.660 --> 01:08:47.760
[Guest]: So the reason the financial system of the U.S. is sort of the most trusted in the world

01:08:47.760 --> 01:08:53.080
[Guest]: is because it's been through all kinds of phases, bad stuff's happened.

01:08:53.080 --> 01:08:54.080
[Guest]: People have come and looked at it.

01:08:54.080 --> 01:08:58.840
[Guest]: They've sorted it out, you know, to the degree that people have trust in the system and systems

01:08:58.840 --> 01:09:02.760
[Guest]: don't function unless there's a fundamental degree of trust in it, right?

01:09:02.760 --> 01:09:07.280
[Guest]: And that's why people trust the U.S. financial system, even though it has its, you know,

01:09:07.280 --> 01:09:09.640
[Guest]: moments of insanity, right?

01:09:09.640 --> 01:09:12.120
[Guest]: For the most part, it's things have to add up, right?

01:09:12.120 --> 01:09:15.800
[Guest]: There's a regulation you have to follow, you know, you just can't do whatever you want,

01:09:15.800 --> 01:09:16.800
[Guest]: right?

01:09:16.800 --> 01:09:20.160
[Guest]: Whereas, you know, Mr. Zuckerberg can turn a dial and do what the hell he wants and no

01:09:20.160 --> 01:09:21.880
[Guest]: one is aware of what's going on, right?

01:09:21.880 --> 01:09:24.720
[Guest]: So the solution to it is transparency.

01:09:24.720 --> 01:09:28.440
[Guest]: We've got to know what these guys are doing, what's, you know, what's going on inside these

01:09:28.440 --> 01:09:30.320
[Guest]: platforms, right?

01:09:30.320 --> 01:09:35.600
[Guest]: If, you know, Jane and Gene are working for me, calling customers all day, the regulator

01:09:35.600 --> 01:09:38.760
[Guest]: wants to do like, you know, wants to know what did Jane and Gene do all day?

01:09:38.760 --> 01:09:43.920
[Guest]: But if Jane and Gene are machines, do I just get a pass?

01:09:43.920 --> 01:09:47.960
[Guest]: Because they're robots and they can call people all day and do stuff under the radar?

01:09:47.960 --> 01:09:50.240
[Guest]: No, that doesn't make any sense, right?

01:09:50.240 --> 01:09:54.920
[Guest]: Why should we treat humans and machines differently if they're doing the similar kind of thing

01:09:54.920 --> 01:09:58.740
[Guest]: and, you know, resulting in risks to the system, right?

01:09:58.740 --> 01:10:03.600
[Guest]: So in financial services, we figured this stuff around, around humans first and then

01:10:03.600 --> 01:10:08.960
[Guest]: around systems because the financial industry was one of the first to automate and adopt

01:10:08.960 --> 01:10:11.720
[Guest]: technology because the stakes were the highest.

01:10:11.720 --> 01:10:17.000
[Guest]: And then, you know, we learned to wrap sensible regulation around it, but mostly sensible

01:10:17.000 --> 01:10:18.000
[Guest]: anyway.

01:10:18.000 --> 01:10:23.120
[Guest]: And so that's where I see ourselves more generally in the social media space, right?

01:10:23.120 --> 01:10:27.320
[Guest]: So I see that as kind of the current Wild West, which, you know, the financial industry

01:10:27.320 --> 01:10:29.640
[Guest]: was in the 70s and the 80s and 90s.

01:10:29.640 --> 01:10:31.520
[Guest]: Those are relative Wild West days.

01:10:31.520 --> 01:10:36.620
[Guest]: That's where I see ourselves now in terms of internet governance.

01:10:36.620 --> 01:10:38.040
[Guest]: So we need that transparency.

01:10:38.040 --> 01:10:42.880
[Guest]: And what I think we're talking about is what does that actually mean, right?

01:10:42.880 --> 01:10:43.880
[Guest]: What does that look like?

01:10:43.880 --> 01:10:48.880
[Guest]: You know, sounds great, but, you know, to Zuckerberg and Dorsey, transparency means,

01:10:48.880 --> 01:10:53.880
[Guest]: well, you know, we'll give you quarterly reports that'll tell you all the crap that happened

01:10:53.880 --> 01:10:54.880
[Guest]: on our platform.

01:10:54.880 --> 01:10:56.880
[Guest]: And believe me, we'll be good about it, right?

01:10:56.880 --> 01:10:58.880
[Guest]: You know, trust us, right?

01:10:58.880 --> 01:11:02.560
[Guest]: You know, would we have trusted the bankers of the great financial crisis if they said,

01:11:02.560 --> 01:11:07.560
[Guest]: you know, yeah, sorry, I know we were irresponsible and we did all kinds of off balance sheet

01:11:07.560 --> 01:11:13.240
[Guest]: transactions, but believe me, we've learned and trust me, we'll never do this again, right?

01:11:13.240 --> 01:11:14.240
[Guest]: Yeah.

01:11:14.240 --> 01:11:15.600
[Guest]: Good luck with that one, right?

01:11:15.600 --> 01:11:17.400
[Guest]: That's where we're at with internet platforms.

01:11:17.400 --> 01:11:20.800
[Guest]: And they're going to go through a similar process of scrutiny.

01:11:20.800 --> 01:11:22.360
[Guest]: And it's about time, right?

01:11:22.360 --> 01:11:25.360
[Guest]: Not to say that this will be fast and expeditious and effective.

01:11:25.360 --> 01:11:27.160
[Guest]: I'm sure it'll be messy.

01:11:27.160 --> 01:11:29.080
[Guest]: But we're heading in the right direction.

01:11:29.080 --> 01:11:35.160
[Guest]: And being an optimist, I'm optimistic that, you know, we'll solve this problem.

01:11:35.160 --> 01:11:38.760
[Amit Varma]: So you know, you described your life as in terms of what Jerry Garcia spoke about what

01:11:38.760 --> 01:11:40.160
[Amit Varma]: a long, strange trip it's been.

01:11:40.160 --> 01:11:43.200
[Amit Varma]: And indeed, because I just realized that you've been on two frontiers.

01:11:43.200 --> 01:11:46.320
[Amit Varma]: One is a wild west of Wall Street in the 1990s when you worked there.

01:11:46.320 --> 01:11:50.920
[Amit Varma]: And now the current sort of wild west of this current situation.

01:11:50.920 --> 01:11:53.840
[Amit Varma]: And we'll talk more about what to do with Jane and Gene.

01:11:53.840 --> 01:11:57.120
[Amit Varma]: But if Jane and Gene are listening to this, which I have no doubt they probably are, because

01:11:57.120 --> 01:12:02.520
[Amit Varma]: hey, AI, they can relax a bit, because we'll do that after a quick commercial break.

01:12:02.520 --> 01:12:08.080
[Amit Varma]: So Jane and Gene, you have one minute to get to act together.

01:12:08.080 --> 01:12:12.200
[Amit Varma]: As many of you know, I'll soon be coming out with a four volume anthology of the seen and

01:12:12.200 --> 01:12:17.880
[Amit Varma]: the unseen books organized around the themes of politics, history, economics, and society

01:12:17.880 --> 01:12:18.880
[Amit Varma]: and culture.

01:12:18.880 --> 01:12:23.200
[Amit Varma]: These days, I'm wading through over 3 million words of conversation from all my episodes

01:12:23.200 --> 01:12:25.720
[Amit Varma]: so far to curate the best bits.

01:12:25.720 --> 01:12:28.640
[Amit Varma]: And for this to happen, I needed transcripts.

01:12:28.640 --> 01:12:33.120
[Amit Varma]: And that was made possible by a remarkable young startup called Tapchief.

01:12:33.120 --> 01:12:38.680
[Amit Varma]: Tapchief at tapchief.com is a digital platform that allows companies to outsource work to

01:12:38.680 --> 01:12:40.400
[Amit Varma]: their network of freelancers.

01:12:40.400 --> 01:12:45.120
[Amit Varma]: And Tapchief's network includes more than 125,000 people as of now.

01:12:45.120 --> 01:12:49.440
[Amit Varma]: You want people to make you a web page or design a logo or compose a jingle or do some

01:12:49.440 --> 01:12:51.520
[Amit Varma]: digital marketing for you.

01:12:51.520 --> 01:12:56.400
[Amit Varma]: Tapchief gives you an easy way to reach out to freelancers competing for your work.

01:12:56.400 --> 01:13:00.520
[Amit Varma]: I can say from first hand experience how valuable this has been for me and solve the problem

01:13:00.520 --> 01:13:02.600
[Amit Varma]: I was actually a bit worried about.

01:13:02.600 --> 01:13:07.840
[Amit Varma]: So do go over to tapchief.com and check out all that Tapchief has to offer.

01:13:07.840 --> 01:13:12.280
[Amit Varma]: Maybe they could solve your problem too.

01:13:12.280 --> 01:13:13.520
[Amit Varma]: Welcome back to the seen and the unseen.

01:13:13.520 --> 01:13:17.360
[Amit Varma]: I'm chatting with Vasanth here about the brave new world we are in and brave new world actually

01:13:17.360 --> 01:13:20.320
[Amit Varma]: makes it sound exciting, but he's optimistic and so am I.

01:13:20.320 --> 01:13:26.000
[Amit Varma]: So we'll just look at, you know, we won't rename it to scary new world or the dystopian

01:13:26.000 --> 01:13:27.600
[Amit Varma]: future or anything like that.

01:13:27.600 --> 01:13:28.600
[Amit Varma]: It is a brave new world.

01:13:28.600 --> 01:13:33.800
[Amit Varma]: Now, you know, I was going to sort of leave the the last part of the episode for talking

01:13:33.800 --> 01:13:38.720
[Amit Varma]: about the current times about sort of what we ought to do about big data and all the

01:13:38.720 --> 01:13:41.160
[Amit Varma]: dangers that we can all agree it poses.

01:13:41.160 --> 01:13:44.080
[Amit Varma]: But since we are on the subject, I think I might as well go there now.

01:13:44.080 --> 01:13:48.760
[Amit Varma]: And here I'm, I found your writing on this very insightful and I'm in agreement as well,

01:13:48.760 --> 01:13:52.760
[Amit Varma]: because whenever I've discussed this issue in the past, it struck me that many of the

01:13:52.760 --> 01:13:59.160
[Amit Varma]: problems that are arising from the way our data platforms behave, such as increasing

01:13:59.160 --> 01:14:03.080
[Amit Varma]: polarization, are social problems and we have to find social solutions.

01:14:03.080 --> 01:14:08.600
[Amit Varma]: And I'm a bit wary of the state getting involved because, you know, we always assume when we

01:14:08.600 --> 01:14:14.520
[Amit Varma]: advocate state sort of getting into solve something that it will always be benevolent

01:14:14.520 --> 01:14:17.940
[Amit Varma]: and benign and it is rarely the case.

01:14:17.940 --> 01:14:23.440
[Amit Varma]: So you want to, you know, achieve things with as little state coercion as possible, like

01:14:23.440 --> 01:14:27.600
[Amit Varma]: even for the financial crisis, a lot of the things that went wrong, like even, you know,

01:14:27.600 --> 01:14:32.600
[Amit Varma]: going back to the bad incentives of the Community Reinvestment Act in the late 70s or the moral

01:14:32.600 --> 01:14:39.160
[Amit Varma]: hazard posed by the way Fannie Mae and Freddie Mac behaved is sort of an example of what

01:14:39.160 --> 01:14:40.560
[Amit Varma]: a state interference can do.

01:14:40.560 --> 01:14:47.020
[Amit Varma]: But the solution that you come up with actually empowers users more than coerces them.

01:14:47.020 --> 01:14:51.160
[Amit Varma]: And this also comes from something that Ben Thompson of Statutory once wrote, which I

01:14:51.160 --> 01:14:55.800
[Amit Varma]: agree with, so I'll just quote him, where Thompson discussing the possible regulation

01:14:55.800 --> 01:15:01.160
[Amit Varma]: of big data wrote, quote, if regulators, EU or otherwise truly want to constrain Facebook

01:15:01.160 --> 01:15:05.520
[Amit Varma]: and Google, or for that matter, all of the other ad networks and companies that in reality

01:15:05.520 --> 01:15:10.600
[Amit Varma]: are far more of a threat to user privacy, then the ultimate force is user demand.

01:15:10.600 --> 01:15:16.320
[Amit Varma]: And the lever is demanding transparency on exactly what these companies are doing.

01:15:16.320 --> 01:15:21.520
[Amit Varma]: And I came across this excellent piece that you had written recently, it'll be linked

01:15:21.520 --> 01:15:27.480
[Amit Varma]: from the show notes, where you wrote about three ways to increase social media platforms

01:15:27.480 --> 01:15:29.080
[Amit Varma]: transparency.

01:15:29.080 --> 01:15:34.820
[Amit Varma]: And I love the solution, because here, what it is, is it's not the state using the strong

01:15:34.820 --> 01:15:41.080
[Amit Varma]: arm on specific companies to say, do X, do Y, do Z, it's just making them more transparent.

01:15:41.080 --> 01:15:45.920
[Amit Varma]: So then users can decide for themselves whether they want to engage with those platforms or

01:15:45.920 --> 01:15:46.920
[Amit Varma]: not.

01:15:46.920 --> 01:15:52.160
[Amit Varma]: So take me through these sort of these three ways to increase the transparency, so to say.

01:15:52.160 --> 01:15:58.280
[Guest]: So not surprisingly, the solution that I'm proposing has been motivated by the financial

01:15:58.280 --> 01:16:03.500
[Guest]: services industry that we were talking about, one that is regulated, and that for the most

01:16:03.500 --> 01:16:10.560
[Guest]: part seems to work all right, at least in terms of engendering trust in the system.

01:16:10.560 --> 01:16:16.400
[Guest]: So if you think about it, some of the assumptions that we've been working under are a little

01:16:16.400 --> 01:16:20.380
[Guest]: bit outdated regarding, let's say, anonymity.

01:16:20.380 --> 01:16:26.160
[Guest]: So the first thing I proposed is that, and by the way, in 2016, when I first suggested

01:16:26.160 --> 01:16:30.560
[Guest]: that we should regulate the social media platforms, I got a lot of backlash from people for being

01:16:30.560 --> 01:16:35.360
[Guest]: un-American and all that kind of stuff, because people felt, you know, this is a free country,

01:16:35.360 --> 01:16:37.360
[Guest]: they should be able to do whatever they want.

01:16:37.360 --> 01:16:42.980
[Guest]: But it was becoming clear that that really wasn't going to work, because one of the things

01:16:42.980 --> 01:16:48.920
[Guest]: about a democracy is, and people don't often recognize this, is that it's not just about

01:16:48.920 --> 01:16:52.280
[Guest]: rights, but it's also about obligations.

01:16:52.280 --> 01:16:59.060
[Guest]: So yeah, we have rights in a liberal democracy, rights to be free, but we also have an obligation

01:16:59.060 --> 01:17:00.060
[Guest]: towards our citizens.

01:17:00.060 --> 01:17:05.560
[Guest]: We can't go around killing people, there's a certain sort of expectation and norms that

01:17:05.560 --> 01:17:08.360
[Guest]: exist in democratic society.

01:17:08.360 --> 01:17:12.960
[Guest]: And so what I suggested was that we need user transparency.

01:17:12.960 --> 01:17:16.680
[Guest]: These platforms need to know who the users are, who you're dealing with, like a bank

01:17:16.680 --> 01:17:18.800
[Guest]: knows who it's dealing with, right?

01:17:18.800 --> 01:17:23.920
[Guest]: You can't just set up a bank account without adequate credentials and authentication, like

01:17:23.920 --> 01:17:27.280
[Guest]: to prove to them that you are who you say you are.

01:17:27.280 --> 01:17:31.280
[Guest]: Now, India, by the way, has a fascinating solution to this with the Aadhaar platform,

01:17:31.280 --> 01:17:34.420
[Guest]: like really forward thinking solution.

01:17:34.420 --> 01:17:39.020
[Guest]: But this is what banks do, are you who you say you are, like know your user, right?

01:17:39.020 --> 01:17:41.840
[Guest]: So in banking, it's called know your customer.

01:17:41.840 --> 01:17:45.240
[Guest]: Now in the social media space, customers are actually advertisers because they're the ones

01:17:45.240 --> 01:17:46.400
[Guest]: paying you.

01:17:46.400 --> 01:17:49.200
[Guest]: But what I suggested, you should know your users.

01:17:49.200 --> 01:17:51.440
[Guest]: If someone is on platform, who are they, right?

01:17:51.440 --> 01:17:53.800
[Guest]: Can they authenticate who they say they are?

01:17:53.800 --> 01:17:57.600
[Guest]: Because I think they have a responsibility, not to you and me, they can still use a pseudonym

01:17:57.600 --> 01:18:00.120
[Guest]: and function anonymously.

01:18:00.120 --> 01:18:02.120
[Guest]: But the platform must know who the hell am I dealing with?

01:18:02.120 --> 01:18:03.640
[Guest]: Who is this a real person, right?

01:18:03.640 --> 01:18:04.640
[Guest]: They need to know.

01:18:04.640 --> 01:18:05.640
[Guest]: Is this a person?

01:18:05.640 --> 01:18:06.640
[Guest]: Is it a bot?

01:18:06.640 --> 01:18:07.640
[Guest]: What am I dealing with?

01:18:07.640 --> 01:18:14.080
[Guest]: So that's the first thing that I suggested is, you know, since we're all talking about

01:18:14.080 --> 01:18:17.920
[Guest]: transparency, well, let's just be transparent to the platform, right?

01:18:17.920 --> 01:18:20.080
[Guest]: And so they know who they're dealing with.

01:18:20.080 --> 01:18:21.320
[Guest]: Is that an invasion of privacy?

01:18:21.320 --> 01:18:22.560
[Guest]: No, it isn't.

01:18:22.560 --> 01:18:27.400
[Guest]: It's no more an invasion of privacy, like, you know, if you're required to provide your

01:18:27.400 --> 01:18:29.920
[Guest]: bank with information, that's not an invasion of privacy.

01:18:29.920 --> 01:18:34.680
[Guest]: It's just like you authenticating and guaranteeing who you say you are, and by the way, you're

01:18:34.680 --> 01:18:38.720
[Guest]: also agreeing to behave yourself, right, with the bank, right?

01:18:38.720 --> 01:18:42.820
[Guest]: You're agreeing not to conduct fraudulent transactions, and you're not going to do trades

01:18:42.820 --> 01:18:47.440
[Guest]: that destabilize the market, and you're agreeing implicitly to a lot of things when you sign

01:18:47.440 --> 01:18:48.440
[Guest]: up with an account.

01:18:48.440 --> 01:18:50.640
[Guest]: So it's not an invasion of privacy, right?

01:18:50.640 --> 01:18:52.640
[Guest]: They should know their user, right?

01:18:52.640 --> 01:18:56.160
[Guest]: And yet so far, their motivations have been exactly the opposite.

01:18:56.160 --> 01:18:58.880
[Guest]: They'd rather not know their users, right?

01:18:58.880 --> 01:19:02.840
[Guest]: It's actually more profitable for them not to know their users, right?

01:19:02.840 --> 01:19:04.680
[Guest]: Like, you want another account?

01:19:04.680 --> 01:19:07.600
[Guest]: Come on over, sign up, say whatever you want.

01:19:07.600 --> 01:19:12.640
[Guest]: And so my solution is, yeah, you can say whatever you want, but we need to know who you are,

01:19:12.640 --> 01:19:13.640
[Guest]: right?

01:19:13.640 --> 01:19:16.240
[Guest]: Authenticate yourself, the platform.

01:19:16.240 --> 01:19:24.920
[Guest]: The second thing I propose is that just like I can, as a regulator, go into a bank and

01:19:24.920 --> 01:19:31.400
[Guest]: say, hey, show me that you haven't been favoring some customers over others.

01:19:31.400 --> 01:19:36.180
[Guest]: And I've had to do that twice myself over the last 10 years to regulators in the hedge

01:19:36.180 --> 01:19:38.080
[Guest]: fund that we operate.

01:19:38.080 --> 01:19:39.760
[Guest]: It's a machine learning-based fund.

01:19:39.760 --> 01:19:42.040
[Guest]: Regulators come over and they ask us all kinds of stuff.

01:19:42.040 --> 01:19:45.960
[Guest]: We have to show them all our trades, how they were allocated to clients.

01:19:45.960 --> 01:19:47.280
[Guest]: We weren't favoring anyone.

01:19:47.280 --> 01:19:49.580
[Guest]: They all made the same amount of money, right?

01:19:49.580 --> 01:19:55.520
[Guest]: So they come in and do a bunch of tests on us and tell us whether we passed or failed.

01:19:55.520 --> 01:19:58.840
[Guest]: Now, in financial services, these guys have been doing this for a long time.

01:19:58.840 --> 01:20:00.320
[Guest]: They know what they're looking for, right?

01:20:00.320 --> 01:20:04.120
[Guest]: They're looking for the bad behavior they're looking for.

01:20:04.120 --> 01:20:08.120
[Guest]: So what I'm proposing is something similar, right, in the social media space.

01:20:08.120 --> 01:20:13.760
[Guest]: If you're actually generating revenues by making recommendations, well, then keep a

01:20:13.760 --> 01:20:14.760
[Guest]: history of it.

01:20:14.760 --> 01:20:19.360
[Guest]: You don't need to share that with the rest of the world, but just keep it.

01:20:19.360 --> 01:20:24.840
[Guest]: In case regulators want to come in next year and say, so, like, have you been making recommendations?

01:20:24.840 --> 01:20:25.840
[Guest]: Tell us.

01:20:25.840 --> 01:20:26.840
[Guest]: Show us, right?

01:20:26.840 --> 01:20:27.840
[Guest]: And you should be able to do that.

01:20:27.840 --> 01:20:30.280
[Guest]: At the moment, there's no such requirement, right?

01:20:30.280 --> 01:20:31.560
[Guest]: There's absolutely no transparency.

01:20:31.560 --> 01:20:37.880
[Guest]: So that's the second thing, the ability to provide audit trails of your behavior, right?

01:20:37.880 --> 01:20:41.160
[Guest]: And this, by the way, will get rid of a lot of problems that we're having right now, right?

01:20:41.160 --> 01:20:46.620
[Guest]: The very fact that you know you're being observed makes you behave, right?

01:20:46.620 --> 01:20:51.580
[Guest]: Not that you will, that they'll come in and, like, want to see everything, but the fact

01:20:51.580 --> 01:20:54.380
[Guest]: is that they can, right?

01:20:54.380 --> 01:20:59.760
[Guest]: So you have to behave in accordance with how you're supposed to, right?

01:20:59.760 --> 01:21:06.300
[Guest]: So that's the second word was transparency in terms of the ability to produce audit trails

01:21:06.300 --> 01:21:07.300
[Guest]: of your actions.

01:21:07.300 --> 01:21:09.600
[Guest]: That was the second one.

01:21:09.600 --> 01:21:14.520
[Guest]: And the third one was algorithmic transparency, which is, what did Jane and Gene do all day

01:21:14.520 --> 01:21:15.520
[Guest]: anyway?

01:21:15.520 --> 01:21:17.520
[Guest]: You know, like, broadly speaking, what are they doing?

01:21:17.520 --> 01:21:18.520
[Guest]: Are they soliciting?

01:21:18.520 --> 01:21:21.640
[Guest]: Well, okay, if they're soliciting, how are they doing it, you know?

01:21:21.640 --> 01:21:25.240
[Guest]: Is it, are they using legitimate data?

01:21:25.240 --> 01:21:29.560
[Guest]: And after they do the solicitation stuff, what actually happens, you know, does engagement

01:21:29.560 --> 01:21:30.560
[Guest]: actually go up?

01:21:30.560 --> 01:21:32.280
[Guest]: What were the consequences of that?

01:21:32.280 --> 01:21:33.760
[Guest]: We can actually look at that, right?

01:21:33.760 --> 01:21:37.800
[Guest]: So regulators should have the ability to go in and see what the hell is going on inside

01:21:37.800 --> 01:21:38.800
[Guest]: this operation, right?

01:21:38.800 --> 01:21:42.800
[Guest]: So it's like operational transparency through the algorithm.

01:21:42.800 --> 01:21:47.440
[Guest]: So to me, these are, I don't know, they seem like no brainers to me, like what's wrong

01:21:47.440 --> 01:21:48.440
[Guest]: with this?

01:21:48.440 --> 01:21:51.120
[Guest]: What's wrong with these three forms of transparency?

01:21:51.120 --> 01:21:56.840
[Guest]: Yeah, you can keep doing your secret sauce and have your algorithms and smart algorithms

01:21:56.840 --> 01:22:01.920
[Guest]: and whatever, but we just need to be able to see what you've actually done, just like

01:22:01.920 --> 01:22:03.400
[Guest]: we do in other industries.

01:22:03.400 --> 01:22:08.480
[Guest]: So that's what I've proposed is that these are simple ways to increase transparency without

01:22:08.480 --> 01:22:13.240
[Guest]: really much in the way of downside, as far as I can see, other than the fact that you

01:22:13.240 --> 01:22:14.720
[Guest]: need to store a lot of stuff.

01:22:14.720 --> 01:22:16.440
[Amit Varma]: No, that's fair enough.

01:22:16.440 --> 01:22:20.400
[Amit Varma]: And I like these solutions because none of these seem like state overreach to me, which

01:22:20.400 --> 01:22:22.400
[Amit Varma]: is something that I'd otherwise be worried about.

01:22:22.400 --> 01:22:26.360
[Amit Varma]: And one hears suggestions to that account as well, though I was a bit concerned by your

01:22:26.360 --> 01:22:29.440
[Amit Varma]: framing of it is, you know, obligations along with rights.

01:22:29.440 --> 01:22:33.480
[Amit Varma]: So you don't need it in the same sense as our prime minister did, where Narendra Modi

01:22:33.480 --> 01:22:38.200
[Amit Varma]: keeps talking about how it's not just about fundamental rights, but fundamental duties,

01:22:38.200 --> 01:22:42.440
[Amit Varma]: which by the way, is a phrase introduced into our constitution by Indira Gandhi.

01:22:42.440 --> 01:22:46.240
[Amit Varma]: So it's like from one great totalitarian to the other, in a sense.

01:22:46.240 --> 01:22:50.240
[Amit Varma]: And what I keep pointing out is that, listen, it's okay to talk about duties, but we should

01:22:50.240 --> 01:22:55.320
[Amit Varma]: remember that individuals don't have a duty to the state, the state has a duty to individuals.

01:22:55.320 --> 01:23:00.000
[Amit Varma]: And in fact, the only obligations I would recognize on the part of individuals is not

01:23:00.000 --> 01:23:04.960
[Amit Varma]: to transgress on the rights of others, which really, you know, comes down to the framing

01:23:04.960 --> 01:23:05.960
[Amit Varma]: of rights as well.

01:23:05.960 --> 01:23:10.120
[Amit Varma]: But yeah, I mean, as far as the state having an obligation to its citizens is concerned,

01:23:10.120 --> 01:23:16.000
[Amit Varma]: I don't see how this kind of regulation is an overreach in any way.

01:23:16.000 --> 01:23:20.120
[Amit Varma]: So I'm good with these, except that, thinking aloud, my thought would be that I totally

01:23:20.120 --> 01:23:25.180
[Amit Varma]: agree that if everyone had to authenticate herself to the platform, they would automatically

01:23:25.180 --> 01:23:26.180
[Amit Varma]: behave themselves.

01:23:26.180 --> 01:23:28.600
[Amit Varma]: But the other two, I don't know what difference it would make.

01:23:28.600 --> 01:23:33.440
[Amit Varma]: For example, if you have algorithmic transparency, and they show you an algorithm that says that

01:23:33.440 --> 01:23:37.880
[Amit Varma]: we connect people with like minded people, and we allow them to express themselves in

01:23:37.880 --> 01:23:43.320
[Amit Varma]: whatever way they want, as long as they are not transgressing the laws or inciting a crime.

01:23:43.320 --> 01:23:46.680
[Amit Varma]: In that case, you know, that would be perfectly legit, you can't argue with that.

01:23:46.680 --> 01:23:51.240
[Amit Varma]: And yet that does go all the polarization and all of that is sort of happening because

01:23:51.240 --> 01:23:52.240
[Amit Varma]: of that.

01:23:52.240 --> 01:23:54.280
[Amit Varma]: So you know, would that really solve the problem?

01:23:54.280 --> 01:23:59.080
[Amit Varma]: We sometimes assume that many of the problems that are being caused by big tech is somehow

01:23:59.080 --> 01:24:03.520
[Amit Varma]: there are malign algorithms, which are doing malicious things and deliberately spreading

01:24:03.520 --> 01:24:04.520
[Amit Varma]: misinformation.

01:24:04.520 --> 01:24:09.000
[Amit Varma]: But no, I would argue that, you know, there are, you know, algorithms, which are completely

01:24:09.000 --> 01:24:12.800
[Amit Varma]: benign, like connecting you to like be like minded people and allowing you to express

01:24:12.800 --> 01:24:19.200
[Amit Varma]: yourself, which you know, the problem is with humanity, not technology and regulating technology

01:24:19.200 --> 01:24:21.140
[Amit Varma]: won't necessarily solve that.

01:24:21.140 --> 01:24:25.960
[Amit Varma]: And I know what you said isn't intended as a panacea, obviously, so but this thought

01:24:25.960 --> 01:24:26.960
[Amit Varma]: just came to mind.

01:24:26.960 --> 01:24:29.200
[Guest]: You're absolutely right on that.

01:24:29.200 --> 01:24:33.560
[Guest]: But this reminds me of some of my colleagues who teach ethics.

01:24:33.560 --> 01:24:40.920
[Guest]: And the thing you become aware of is that I can conjure up a scenario where it's actually

01:24:40.920 --> 01:24:42.400
[Guest]: fine to lie.

01:24:42.400 --> 01:24:47.640
[Guest]: I can conjure up a scenario where I say, okay, you know, you have two choices, right?

01:24:47.640 --> 01:24:52.320
[Guest]: One is, you know, if you just tell this little lie, the planet will be saved.

01:24:52.320 --> 01:24:54.360
[Guest]: But if you tell the truth, we'll all be destroyed.

01:24:54.360 --> 01:24:55.360
[Guest]: What's the right thing to do?

01:24:55.360 --> 01:24:59.640
[Guest]: The right thing to do is to lie, right, even though we know that you shouldn't lie, right?

01:24:59.640 --> 01:25:04.280
[Guest]: So and that's the nuance here that we need to appreciate, you know, you're absolutely

01:25:04.280 --> 01:25:10.120
[Guest]: right that, you know, look, you can't like transgress and interfere with people's freedoms

01:25:10.120 --> 01:25:14.760
[Guest]: and tell them what to do and tell these platforms how to operate the algorithms, right?

01:25:14.760 --> 01:25:22.960
[Guest]: But if I conjure up a really extreme scenario, right, one that's just so compelling, right,

01:25:22.960 --> 01:25:24.840
[Guest]: that you can't ignore it, then what do you do?

01:25:24.840 --> 01:25:30.840
[Guest]: So for example, for me, a compelling scenario might be that, so Facebook's been recording

01:25:30.840 --> 01:25:35.880
[Guest]: all its recommendations, everything to everyone, right, for the last year, the last couple

01:25:35.880 --> 01:25:36.880
[Guest]: of years.

01:25:36.880 --> 01:25:42.440
[Guest]: And then we go back and look at the stuff and we say, you know what, like, this is all

01:25:42.440 --> 01:25:50.440
[Guest]: well and good, but for people like under 17 years of age, it's leading to like increases

01:25:50.440 --> 01:25:57.480
[Guest]: in suicide, and we can actually demonstrate that, you know, quite convincingly that your

01:25:57.480 --> 01:26:04.280
[Guest]: algorithms that are doing whatever the hell they're doing, we have no idea, are, you know,

01:26:04.280 --> 01:26:10.160
[Guest]: there's like very strong evidence that leading to like a tenfold increase in teen suicide,

01:26:10.160 --> 01:26:11.160
[Guest]: right?

01:26:11.160 --> 01:26:13.440
[Guest]: Like, what would you do in that case, right?

01:26:13.440 --> 01:26:18.840
[Guest]: Would you still say, well, you know, it's a free country, like, let him die, right?

01:26:18.840 --> 01:26:22.360
[Guest]: That's free will, they're just bringing them upon themselves.

01:26:22.360 --> 01:26:27.160
[Guest]: They should be smarter than to know that they should get, you know, sucked in by whatever

01:26:27.160 --> 01:26:28.160
[Guest]: they're doing.

01:26:28.160 --> 01:26:29.600
[Guest]: Clearly, that's not good enough, right?

01:26:29.600 --> 01:26:34.360
[Guest]: In the physical world, we have all kinds of safeguards, like with food, you're required

01:26:34.360 --> 01:26:37.320
[Guest]: to label the food and show what's in it, right?

01:26:37.320 --> 01:26:40.840
[Guest]: With something else, you know, you can always come up with scenarios where people have sort

01:26:40.840 --> 01:26:46.440
[Guest]: of decided that something crosses a line, and that is an acceptable behavior.

01:26:46.440 --> 01:26:53.960
[Guest]: That's the situation we're in, we just don't know enough at the moment about the consequences

01:26:53.960 --> 01:26:57.800
[Guest]: of sort of the Wild West that we're in at the moment, right?

01:26:57.800 --> 01:27:03.120
[Guest]: We have suggestive evidence that tells us we should be concerned, but we don't have

01:27:03.120 --> 01:27:05.000
[Guest]: all the data.

01:27:05.000 --> 01:27:06.200
[Guest]: And we struggle with it, right?

01:27:06.200 --> 01:27:13.420
[Guest]: There was a Supreme Court judgment last week in the US, where the Supreme Court ruled that

01:27:13.420 --> 01:27:19.600
[Guest]: Governor Cuomo's edict to ban gatherings, let's say religious gatherings, of over 50

01:27:19.600 --> 01:27:27.440
[Guest]: or 25 in the red zone was banned, prohibited, and the Supreme Court ruled it unconstitutional.

01:27:27.440 --> 01:27:34.080
[Guest]: And one of the reasons that, you know, one of the judges, I think maybe it was Neil Gorsuch,

01:27:34.080 --> 01:27:39.480
[Guest]: said that, you know, there's no evidence that suggests that these institutions have been

01:27:39.480 --> 01:27:42.200
[Guest]: responsible for the spread of the virus.

01:27:42.200 --> 01:27:43.200
[Guest]: There's no evidence, right?

01:27:43.200 --> 01:27:48.280
[Guest]: So you can look at that, and so we're going to apply the strict scrutiny test to this.

01:27:48.280 --> 01:27:54.400
[Guest]: Now, the question I have is, supposing we were South Korea, right, and we had actually

01:27:54.400 --> 01:27:59.480
[Guest]: measured these things, right, we'd measured how many people gather, the infection rates,

01:27:59.480 --> 01:28:03.280
[Guest]: all that kind of stuff, right, and you present this to the nine Supreme Court justices, and

01:28:03.280 --> 01:28:10.880
[Guest]: you say, you know what, you know, your honorable selves, what we're seeing is that, you know,

01:28:10.880 --> 01:28:19.400
[Guest]: when gatherings go above 49, I'm just making this up, you know, and the base rate of infections

01:28:19.400 --> 01:28:26.960
[Guest]: in that community is more than, you know, X, the spread of the disease tends to be tenfold

01:28:26.960 --> 01:28:33.440
[Guest]: higher than it is otherwise, and here's the data to show it, right, there's like 300 cases,

01:28:33.440 --> 01:28:38.680
[Guest]: you know, and here are the statistics, right, would the Supreme Court justices still come

01:28:38.680 --> 01:28:41.800
[Guest]: up with the same verdict?

01:28:41.800 --> 01:28:45.720
[Guest]: I would think that they would be affected by the evidence, right?

01:28:45.720 --> 01:28:52.960
[Guest]: So what I'm getting at is more transparency for society in general, where we can make

01:28:52.960 --> 01:28:58.680
[Guest]: these decisions based on the evidence, right, as opposed to our value systems or beliefs

01:28:58.680 --> 01:29:00.560
[Guest]: or assumptions, right?

01:29:00.560 --> 01:29:02.520
[Guest]: That's the old way, right?

01:29:02.520 --> 01:29:10.160
[Guest]: The brave new world will consist of data being, you know, having sort of first-class status

01:29:10.160 --> 01:29:16.880
[Guest]: in decision-making and not just people's assumptions and norms, right, that is, we should look

01:29:16.880 --> 01:29:22.440
[Guest]: at data, we should be influenced by evidence, and that cuts across, you know, all areas

01:29:22.440 --> 01:29:23.440
[Guest]: of our lives.

01:29:23.440 --> 01:29:28.800
[Guest]: So, you know, it just sort of comes back to, no, there shouldn't be overreach, that's the

01:29:28.800 --> 01:29:35.720
[Guest]: last thing we need is government overreach, but we do need some way where the government

01:29:35.720 --> 01:29:40.720
[Guest]: is acting intelligently based on the evidence at hand, and it's transparent, right?

01:29:40.720 --> 01:29:47.880
[Guest]: To me, that's the best of both worlds, is the government is accountable, and it's transparent,

01:29:47.880 --> 01:29:54.720
[Guest]: and we have a contract with the government, we, I mean individuals, that we can agree

01:29:54.720 --> 01:30:02.440
[Guest]: that certain uses of data are for the larger social good, and we work out ways where we

01:30:02.440 --> 01:30:09.480
[Guest]: prevent these transgressions from taking place, right, where we don't get an overly enthusiastic

01:30:09.480 --> 01:30:13.660
[Guest]: government coming and changing the rules on us.

01:30:13.660 --> 01:30:17.520
[Guest]: And that's the situation where we're in at the moment, we find ourselves in.

01:30:17.520 --> 01:30:24.360
[Guest]: It is a brave new world, right, it's a whole new world of how we govern the internet, how

01:30:24.360 --> 01:30:32.080
[Guest]: we allow machines to influence and play larger roles in our lives in a way that's better

01:30:32.080 --> 01:30:36.640
[Guest]: all around for everyone, you know, better for society, better for individuals, but we

01:30:36.640 --> 01:30:40.400
[Guest]: need to think about this and do it consciously, because the rate at which we're going, that

01:30:40.400 --> 01:30:44.000
[Guest]: sort of Wild West ways, won't get us there.

01:30:44.000 --> 01:30:48.400
[Guest]: I'm optimistic that we are seeing the problem in time, and that we will walk away from the

01:30:48.400 --> 01:30:52.120
[Guest]: precipice and come up with solutions that make sense.

01:30:52.120 --> 01:30:55.520
[Amit Varma]: So let me sort of probe a little deeper in the sense that I obviously agree with you

01:30:55.520 --> 01:31:00.000
[Amit Varma]: on the desirability of transparency, not just in this domain, but so many others.

01:31:00.000 --> 01:31:03.760
[Amit Varma]: But what I'm a little skeptical about is the efficacy of it.

01:31:03.760 --> 01:31:08.440
[Amit Varma]: And the reason why that is not a technical point, but has consequences, is that if one

01:31:08.440 --> 01:31:14.280
[Amit Varma]: finds later on down the line, that greater transparency doesn't actually solve the problem,

01:31:14.280 --> 01:31:18.200
[Amit Varma]: then the state can use it as a pretext to actually overreach.

01:31:18.200 --> 01:31:24.960
[Amit Varma]: Now, let me sort of drill down into one concrete and ask you about one hypothetical example

01:31:24.960 --> 01:31:26.680
[Amit Varma]: that you brought up and ask you to get concrete.

01:31:26.680 --> 01:31:32.080
[Amit Varma]: Now you spoke about how, if there is algorithmic transparency, and we find that there is some

01:31:32.080 --> 01:31:36.800
[Amit Varma]: algorithmic action that has led to a rise in teen suicides, right?

01:31:36.800 --> 01:31:40.600
[Amit Varma]: Now every reasonable person agrees that that's a problem, and we should solve it.

01:31:40.600 --> 01:31:45.200
[Amit Varma]: The question here is, number one, how do you establish causality when there are so many

01:31:45.200 --> 01:31:49.000
[Amit Varma]: algorithms doing so many things and so many influences otherwise?

01:31:49.000 --> 01:31:55.400
[Amit Varma]: Number two, even if you manage to pinpoint a particular algorithm that shows that there

01:31:55.400 --> 01:32:01.200
[Amit Varma]: is some causality, that algorithm could by itself both be benign and could have positive

01:32:01.200 --> 01:32:02.400
[Amit Varma]: effects in another way.

01:32:02.400 --> 01:32:07.440
[Amit Varma]: For example, an algorithm that allows people to meet like-minded people and talk about

01:32:07.440 --> 01:32:13.360
[Amit Varma]: things that they care about could foreseeably lead to sort of a rise in teen suicides because

01:32:13.360 --> 01:32:19.320
[Amit Varma]: you could have young teens coming together and, you know, driving each other more and

01:32:19.320 --> 01:32:21.220
[Amit Varma]: more towards despair.

01:32:21.220 --> 01:32:25.960
[Amit Varma]: But at the same time, you could have young teens coming together and sort of cheering

01:32:25.960 --> 01:32:28.760
[Amit Varma]: each other up and moving away from the precipice.

01:32:28.760 --> 01:32:30.800
[Amit Varma]: And the latter case would, of course, be unseen.

01:32:30.800 --> 01:32:34.740
[Amit Varma]: What would be seen would be the suicides, if I might invoke the name of the show.

01:32:34.740 --> 01:32:38.680
[Amit Varma]: So then the question would be that, first of all, how do you determine causality in

01:32:38.680 --> 01:32:43.760
[Amit Varma]: terms of which algorithm causes, two, how do you separate out, you know, the ill effects

01:32:43.760 --> 01:32:48.040
[Amit Varma]: of a benign algorithm if the algorithm also did many good things, such as connecting like-minded

01:32:48.040 --> 01:32:52.880
[Amit Varma]: people like you and me together, which could have happened, you know, through social media

01:32:52.880 --> 01:32:57.840
[Amit Varma]: for all that we know, you know, and so when I drill down to the concrete, it becomes difficult.

01:32:57.840 --> 01:33:02.000
[Amit Varma]: And then if you leave it to human judgment at how this caused that, then human judgment

01:33:02.000 --> 01:33:03.960
[Amit Varma]: will always be flawed.

01:33:03.960 --> 01:33:09.960
[Amit Varma]: And when it's a judgment of the state will always end up enhancing their power or I mean,

01:33:09.960 --> 01:33:11.800
[Amit Varma]: this is, of course, a non-political example.

01:33:11.800 --> 01:33:16.000
[Amit Varma]: But if there is a political example, then it will play to the biases of whichever side

01:33:16.000 --> 01:33:19.960
[Amit Varma]: holds the levers of the state at that point in time, which could be either the far right

01:33:19.960 --> 01:33:21.160
[Amit Varma]: or the far left.

01:33:21.160 --> 01:33:25.160
[Amit Varma]: So is it sort of, and obviously, forgive me if it's a new question, but can you give me

01:33:25.160 --> 01:33:30.200
[Amit Varma]: an example of, is there an actual instance where in this context of social media causing

01:33:30.200 --> 01:33:35.280
[Amit Varma]: behavior, you figured out that a specific set of algorithms actually caused a specific

01:33:35.280 --> 01:33:37.600
[Amit Varma]: kind of problem in the real world?

01:33:37.600 --> 01:33:44.680
[Guest]: So there were at least two things you were alluding to in your question.

01:33:44.680 --> 01:33:54.080
[Guest]: One was the difficulty of doing the science itself, and the fact that you may not find

01:33:54.080 --> 01:33:56.800
[Guest]: anything significant once you do the science.

01:33:56.800 --> 01:34:04.600
[Guest]: That is the causality may be hard to establish, there may be considerable uncertainty associated

01:34:04.600 --> 01:34:10.840
[Guest]: with the conclusions you're drawing, even if you find some effect, there's still what

01:34:10.840 --> 01:34:14.000
[Guest]: I call variance around it and uncertainty.

01:34:14.000 --> 01:34:15.240
[Guest]: And that's certainly true.

01:34:15.240 --> 01:34:20.320
[Guest]: And I guess in a sense, I'm sort of assuming that we can solve that problem, that in fact,

01:34:20.320 --> 01:34:28.480
[Guest]: if there is no demonstration of causality and no statistically significant relationship

01:34:28.480 --> 01:34:31.760
[Guest]: between A and B, well, then that should not be used, right?

01:34:31.760 --> 01:34:35.120
[Guest]: Then it's telling you that there's nothing to worry about, right?

01:34:35.120 --> 01:34:42.640
[Guest]: So what I'm assuming is that we first do find things that we have to worry about, right?

01:34:42.640 --> 01:34:44.640
[Guest]: And we're finding those already, right?

01:34:44.640 --> 01:34:49.720
[Guest]: And that's the last two years have revealed all some of the things that we should be worried

01:34:49.720 --> 01:34:52.560
[Guest]: about, but probably not all of them, right?

01:34:52.560 --> 01:34:59.640
[Guest]: So to me, transparency is about achieving awareness, because once you're aware of something,

01:34:59.640 --> 01:35:02.960
[Guest]: you can act more intelligently, right?

01:35:02.960 --> 01:35:09.960
[Guest]: So if you're aware of the fact that, let's say, certain algorithms are leading to teen

01:35:09.960 --> 01:35:10.960
[Guest]: depression, right?

01:35:10.960 --> 01:35:16.280
[Guest]: That there's a high association between those two, well, then maybe we can start taking

01:35:16.280 --> 01:35:23.640
[Guest]: mitigating steps, like making teens more aware of how these algorithms might be influencing

01:35:23.640 --> 01:35:24.640
[Amit Varma]: them, right?

01:35:24.640 --> 01:35:29.360
[Amit Varma]: But if I might interrupt you, algorithms such as what, like I'm just having difficulty conceiving

01:35:29.360 --> 01:35:35.240
[Amit Varma]: of an algorithm that specifically causes, say, teen depression rates to rise apart from

01:35:35.240 --> 01:35:38.880
[Amit Varma]: the very general ones of bringing like-minded people together.

01:35:38.880 --> 01:35:49.440
[Guest]: So let's say that I come up with an algorithm that finds a way of connecting various kinds

01:35:49.440 --> 01:35:59.800
[Guest]: of teens together that increases overall engagement among them, but also increases discord among

01:35:59.800 --> 01:36:03.120
[Guest]: them, which you can actually see in the data.

01:36:03.120 --> 01:36:04.960
[Guest]: You can look at what they're saying, right?

01:36:04.960 --> 01:36:06.200
[Guest]: And by the way, I'm making this up.

01:36:06.200 --> 01:36:08.640
[Guest]: I'm not saying that this is actually happening, right?

01:36:08.640 --> 01:36:17.040
[Guest]: So platform is doing something, and we observe that it's actually leading to more engagement.

01:36:17.040 --> 01:36:19.040
[Guest]: People are talking more to each other, but you know what?

01:36:19.040 --> 01:36:22.440
[Guest]: This talking is actually not good talk.

01:36:22.440 --> 01:36:23.440
[Guest]: It's bad talk.

01:36:23.440 --> 01:36:29.260
[Guest]: And, you know, and that bad talk is, you know, correlated with, you know, depressions and

01:36:29.260 --> 01:36:32.000
[Guest]: suicides in that zip code, right?

01:36:32.000 --> 01:36:33.760
[Guest]: In that area, right?

01:36:33.760 --> 01:36:37.360
[Guest]: Now let's say the science was done really well, and it's showing you this.

01:36:37.360 --> 01:36:39.920
[Guest]: Well, are you going to ignore it, right?

01:36:39.920 --> 01:36:43.760
[Guest]: Well, you could, but it's probably not a good idea to ignore it, right?

01:36:43.760 --> 01:36:48.080
[Guest]: Because there's something going on that you should probably do something about, right?

01:36:48.080 --> 01:36:52.360
[Guest]: And you know, this is something you may not have been even aware of earlier, right?

01:36:52.360 --> 01:36:55.920
[Guest]: I mean, how do you know this is going on if you don't even have the data?

01:36:55.920 --> 01:37:01.800
[Guest]: But if I have sufficient evidence to know that something is going on among the users

01:37:01.800 --> 01:37:09.440
[Guest]: of that platform that's leading to some very undesirable societal outcome, then I should

01:37:09.440 --> 01:37:11.760
[Guest]: figure out what I can do about it.

01:37:11.760 --> 01:37:12.760
[Guest]: Right?

01:37:12.760 --> 01:37:15.120
[Guest]: It's, that's all.

01:37:15.120 --> 01:37:19.000
[Guest]: And so if, but if there's nothing, well, then I shouldn't do anything about it, right?

01:37:19.000 --> 01:37:23.960
[Guest]: So I'm assuming that the science has been done, you know, notwithstanding the messiness

01:37:23.960 --> 01:37:25.280
[Guest]: of it, right?

01:37:25.280 --> 01:37:30.120
[Guest]: Sometimes the data is hard, it's noisy, you know, we're used to dealing with that, you

01:37:30.120 --> 01:37:31.120
[Guest]: know?

01:37:31.120 --> 01:37:32.120
[Guest]: So we can deal with that.

01:37:32.120 --> 01:37:37.480
[Guest]: But presumably, we can still do the science well, and establish either causality or a

01:37:37.480 --> 01:37:43.080
[Guest]: strong association that may suggest causality that's worth exploring.

01:37:43.080 --> 01:37:44.080
[Amit Varma]: Right.

01:37:44.080 --> 01:37:49.280
[Amit Varma]: Let's sort of move on to how different regimes across the world are actually sort of handling

01:37:49.280 --> 01:37:50.280
[Amit Varma]: this problem.

01:37:50.280 --> 01:37:54.440
[Amit Varma]: Like in a piece that you wrote, headlined, In the AI Era, Privacy and Democracy are in

01:37:54.440 --> 01:37:55.440
[Amit Varma]: Peril.

01:37:55.440 --> 01:37:59.560
[Amit Varma]: You wrote, quote, I recommend regulating the use of personal data for prediction products.

01:37:59.560 --> 01:38:04.760
[Amit Varma]: I also propose classifying certain platforms as digital utilities that aim to maximize

01:38:04.760 --> 01:38:09.640
[Amit Varma]: public benefit and spur economic growth, much like the interstate highway system, and the

01:38:09.640 --> 01:38:14.120
[Amit Varma]: information superhighway have done for physical and electronic commerce, stop quote.

01:38:14.120 --> 01:38:19.360
[Amit Varma]: And then you talk about four major models of data use, which is at the extremes US and

01:38:19.360 --> 01:38:21.780
[Amit Varma]: China and then EU and India as well.

01:38:21.780 --> 01:38:27.320
[Amit Varma]: So can you talk a bit about these four different approaches, where they come from, how, you

01:38:27.320 --> 01:38:32.880
[Amit Varma]: know, effective they are, and what do you feel closest to in terms of actually solving

01:38:32.880 --> 01:38:34.000
[Guest]: the problem?

01:38:34.000 --> 01:38:38.280
[Guest]: So they all have their strengths and weaknesses, right?

01:38:38.280 --> 01:38:43.480
[Guest]: The strength of the US model is that it's free, right?

01:38:43.480 --> 01:38:48.720
[Guest]: It's free of like almost, you know, any kind of regulation at this point.

01:38:48.720 --> 01:38:52.360
[Guest]: And so it's grown up sort of organically, right?

01:38:52.360 --> 01:38:57.360
[Guest]: This is the way that the internet has developed in the US and people would argue that there've

01:38:57.360 --> 01:39:02.440
[Guest]: been lots of good things that have happened, that all kinds of really cool stuff has emerged

01:39:02.440 --> 01:39:07.960
[Guest]: that may not have, you know, if we hadn't given these internet companies and the platform

01:39:07.960 --> 01:39:13.600
[Guest]: complete freedom and absolved them from lawsuits and responsibilities and things like that.

01:39:13.600 --> 01:39:16.760
[Guest]: So that's the beauty of the US model.

01:39:16.760 --> 01:39:21.240
[Guest]: But we've seen the problems with it emerge recently, that these platforms have become

01:39:21.240 --> 01:39:26.800
[Guest]: so powerful and they've overreached so much in terms of data that they're like messing

01:39:26.800 --> 01:39:28.920
[Guest]: some things up, right?

01:39:28.920 --> 01:39:34.000
[Guest]: And that's why all the scrutiny around the internet models in the US.

01:39:34.000 --> 01:39:39.960
[Guest]: Europe not surprisingly has always been, you know, for accountability, right?

01:39:39.960 --> 01:39:44.960
[Guest]: So their approach to it has been sort of responsible data use, right?

01:39:44.960 --> 01:39:49.700
[Guest]: That you can use data if it's essential for what you're trying to do, right?

01:39:49.700 --> 01:39:53.840
[Guest]: Which also makes sense because they're trying to cut down the risks associated with this

01:39:53.840 --> 01:39:57.960
[Guest]: sort of unbridled wild west use of data.

01:39:57.960 --> 01:40:01.200
[Guest]: So have some, provide some kind of accountability.

01:40:01.200 --> 01:40:05.300
[Guest]: India is sort of a really interesting case for a number of reasons.

01:40:05.300 --> 01:40:10.800
[Guest]: So it has sort of this late mover advantage and it also has sort of a different kind of

01:40:10.800 --> 01:40:16.840
[Guest]: advantage, which is that I think the people that are sort of influential in tech in India

01:40:16.840 --> 01:40:21.880
[Guest]: have sort of more of a social approach to infrastructure.

01:40:21.880 --> 01:40:26.720
[Guest]: And there's this notion of sort of public digital infrastructure, not surprisingly has

01:40:26.720 --> 01:40:30.320
[Guest]: emerged, you know, mostly, you know, out of India.

01:40:30.320 --> 01:40:35.800
[Guest]: And it actually happened maybe partly by accident, you know, partly it was serendipity, you know,

01:40:35.800 --> 01:40:40.400
[Guest]: that we decided to implement the Aadhaar platform, right?

01:40:40.400 --> 01:40:46.200
[Guest]: Which was for authentication and to actually bring people who didn't have an identity prior

01:40:46.200 --> 01:40:48.480
[Guest]: to that into the digital mainstream, right?

01:40:48.480 --> 01:40:53.760
[Guest]: So now I know there's controversy in India about its misuse and all that kind of stuff,

01:40:53.760 --> 01:40:59.520
[Guest]: but I think we'd be silly to, you know, not acknowledge the tremendous benefits that such

01:40:59.520 --> 01:41:01.880
[Guest]: a platform can bring to society, right?

01:41:01.880 --> 01:41:06.200
[Guest]: So we built that and then it seems to work, right?

01:41:06.200 --> 01:41:13.080
[Guest]: It has, you know, 1.2 billion users and it's very widely used for a number of things.

01:41:13.080 --> 01:41:20.960
[Guest]: And now we're thinking, oh, maybe we can actually build something on top of this authentication

01:41:20.960 --> 01:41:25.280
[Guest]: layer where we can have, you know, and in India, they call it the India stack, where

01:41:25.280 --> 01:41:29.720
[Guest]: we can actually stack sort of layers of utility.

01:41:29.720 --> 01:41:33.600
[Guest]: So I look at Aadhaar as a utility, that's essentially what it is.

01:41:33.600 --> 01:41:36.480
[Guest]: It just does one thing, you know, are you who you say you are?

01:41:36.480 --> 01:41:39.880
[Guest]: And it answers that question, yes or no, simple.

01:41:39.880 --> 01:41:43.840
[Guest]: It doesn't, it hasn't spilled over into other things, like it's not checking your credit

01:41:43.840 --> 01:41:46.360
[Guest]: scores or anything like that.

01:41:46.360 --> 01:41:50.640
[Guest]: It's just simple, one function thing in mind.

01:41:50.640 --> 01:41:54.600
[Guest]: And now we're thinking, oh, we can layer a system on top of that, that since someone

01:41:54.600 --> 01:41:58.680
[Guest]: can authenticate you and they know who you are, you can then say, oh yeah, I want to

01:41:58.680 --> 01:42:03.000
[Guest]: share my transcripts of the university or with the school, or I want to share my credit

01:42:03.000 --> 01:42:04.960
[Guest]: history with the bank for a certain amount of time.

01:42:04.960 --> 01:42:12.560
[Guest]: I like the thinking around it because it's a utility, not unlike sort of the, the, the

01:42:12.560 --> 01:42:13.560
[Guest]: internet, right?

01:42:13.560 --> 01:42:18.160
[Guest]: Not, not, not unlike, you know, the highway where you're sort of trying to lay the rails

01:42:18.160 --> 01:42:22.160
[Guest]: for a utility that will be used like crazy, right?

01:42:22.160 --> 01:42:25.960
[Guest]: Where you don't actually want the utility provider to be making a, you know, a ton of

01:42:25.960 --> 01:42:29.920
[Guest]: money off of it because it's a utility, it should be priced like electricity or water.

01:42:29.920 --> 01:42:35.960
[Guest]: It's a basic thing that everyone engages in, like moving money around, you know, like it's

01:42:35.960 --> 01:42:36.960
[Guest]: a utility.

01:42:36.960 --> 01:42:38.120
[Guest]: There's so much of it happening.

01:42:38.120 --> 01:42:42.680
[Guest]: It should happen almost costlessly, you know, because after all, it's just a ledger entry,

01:42:42.680 --> 01:42:43.680
[Guest]: right?

01:42:43.680 --> 01:42:47.480
[Guest]: It's not like you have a Pony Express moving money from A to B that costs money.

01:42:47.480 --> 01:42:48.480
[Guest]: It is costless.

01:42:48.480 --> 01:42:49.560
[Guest]: And so it should be costless.

01:42:49.560 --> 01:42:50.860
[Guest]: It should be utility.

01:42:50.860 --> 01:42:58.240
[Guest]: So I like the thinking in India around this, which is around data that is personal, non-personal,

01:42:58.240 --> 01:42:59.240
[Guest]: right?

01:42:59.240 --> 01:43:01.440
[Guest]: And non-personal becomes community data, right?

01:43:01.440 --> 01:43:06.440
[Guest]: So maybe, you know, if you and I go to the hospital, maybe that data can be anonymized,

01:43:06.440 --> 01:43:09.280
[Guest]: you know, and it becomes sort of a community data set.

01:43:09.280 --> 01:43:13.920
[Guest]: But you can say that, you know, people over 50 who have this, who are exposed to this

01:43:13.920 --> 01:43:15.520
[Guest]: are at risk.

01:43:15.520 --> 01:43:19.720
[Guest]: And therefore we can craft some, you know, health policy around that based on community

01:43:19.720 --> 01:43:20.720
[Guest]: data, right?

01:43:20.720 --> 01:43:24.120
[Guest]: Now, this thinking is in its early stages, right?

01:43:24.120 --> 01:43:27.360
[Guest]: I've read the reports that have come out around this.

01:43:27.360 --> 01:43:30.960
[Guest]: They're interesting, but they have all kinds of holes in them that haven't really been

01:43:30.960 --> 01:43:31.960
[Guest]: thought out, but that's okay.

01:43:31.960 --> 01:43:34.160
[Guest]: You know, we can sort those out.

01:43:34.160 --> 01:43:39.160
[Guest]: So I like the thinking in India around this sort of public digital infrastructure and

01:43:39.160 --> 01:43:46.200
[Guest]: building layers on top of that, where people are in control and consent to have that data

01:43:46.200 --> 01:43:49.600
[Guest]: used or, you know, for a certain amount of time and for a specific purpose.

01:43:49.600 --> 01:43:55.560
[Guest]: To me, that's sort of the best of both worlds, where users are aware and in charge and can

01:43:55.560 --> 01:43:57.560
[Guest]: control data, right?

01:43:57.560 --> 01:44:01.340
[Guest]: In the US, it's become kind of, we've lost control over that, right?

01:44:01.340 --> 01:44:05.000
[Guest]: People have lost control over where their data is, who's using it, for what, what can

01:44:05.000 --> 01:44:06.280
[Guest]: they infer from it?

01:44:06.280 --> 01:44:11.200
[Guest]: The Chinese model is fascinating and worrisome at the same time, right?

01:44:11.200 --> 01:44:15.400
[Guest]: It's fascinating because they've done amazing things with it, right?

01:44:15.400 --> 01:44:21.580
[Guest]: They've exercised control and, you know, in a way that's been quite effective, presumably

01:44:21.580 --> 01:44:25.600
[Guest]: for things like COVID to the extent that you can trust information coming out of China,

01:44:25.600 --> 01:44:26.600
[Guest]: right?

01:44:26.600 --> 01:44:28.960
[Guest]: They've been quite effective at dealing with that.

01:44:28.960 --> 01:44:34.800
[Guest]: They've been quite effective at gaining efficiencies by having data centralized.

01:44:34.800 --> 01:44:37.360
[Guest]: But you know, there's a nasty side to it, right?

01:44:37.360 --> 01:44:43.560
[Guest]: You know, we read these excesses, you know, in Xinjiang against the Uyghurs and, you know,

01:44:43.560 --> 01:44:46.000
[Guest]: and God knows what else is there, right?

01:44:46.000 --> 01:44:50.040
[Guest]: Dissidents are like rounded up and threatened and things like that, right?

01:44:50.040 --> 01:44:56.240
[Guest]: So there's this sort of very dark side of that centralized model that really sort of

01:44:56.240 --> 01:44:57.880
[Guest]: bothers me.

01:44:57.880 --> 01:45:05.520
[Guest]: And it concerns me when people project that as kind of the better model for internet governance,

01:45:05.520 --> 01:45:10.000
[Guest]: because, you know, knowing human history, we should be worried, right?

01:45:10.000 --> 01:45:16.760
[Guest]: I mean, I don't trust any institution fully, like any kind of government, you know, yeah,

01:45:16.760 --> 01:45:21.280
[Guest]: for the most part, I, you know, I, for the most part, I trust the US government, but

01:45:21.280 --> 01:45:23.120
[Guest]: they routinely violate their laws.

01:45:23.120 --> 01:45:28.160
[Guest]: Well, I don't know how routinely, but you know, the example that comes to mind is that,

01:45:28.160 --> 01:45:33.240
[Guest]: you know, who was that Swami from Pune, who moved to Antelope, Oregon?

01:45:33.240 --> 01:45:34.240
[Amit Varma]: Rajneesh.

01:45:34.240 --> 01:45:35.240
[Amit Varma]: Yeah, Rajneesh.

01:45:35.240 --> 01:45:36.240
[Guest]: Right.

01:45:36.240 --> 01:45:40.520
[Guest]: I mean, Osho, you know, the US government essentially decided, you know, I mean, the

01:45:40.520 --> 01:45:44.080
[Guest]: one thing you do is you can't mess with the US government sort of openly and blatantly

01:45:44.080 --> 01:45:46.280
[Guest]: that they'll get you, right?

01:45:46.280 --> 01:45:47.280
[Guest]: It's a free country.

01:45:47.280 --> 01:45:48.280
[Guest]: It's a democracy.

01:45:48.280 --> 01:45:49.280
[Guest]: You have rights.

01:45:49.280 --> 01:45:50.840
[Guest]: But if you overstep, they'll get you.

01:45:50.840 --> 01:45:51.840
[Guest]: And that's what the US government did.

01:45:51.840 --> 01:45:55.880
[Guest]: You know, they violated their own laws to get him and basically set an example.

01:45:55.880 --> 01:45:56.880
[Guest]: Yeah.

01:45:56.880 --> 01:46:01.460
[Guest]: So that's what bothers me about the Chinese model is that, yes, I buy the tremendous benefits

01:46:01.460 --> 01:46:07.680
[Guest]: of efficiency and all the good things that come with centralization and control, right,

01:46:07.680 --> 01:46:12.920
[Guest]: that you didn't have in the old style centralized regimes, communist regimes, right, where there

01:46:12.920 --> 01:46:13.920
[Guest]: was no information.

01:46:13.920 --> 01:46:18.520
[Guest]: And so that was a terrible model, a governance model in terms of getting anything done.

01:46:18.520 --> 01:46:19.520
[Guest]: Right.

01:46:19.520 --> 01:46:23.680
[Guest]: And we saw that in Russia, saw that in China, like just bad governance models, whereas now

01:46:23.680 --> 01:46:27.880
[Guest]: they're like swimming in data and saying, wow, like, you know, we can do our job so

01:46:27.880 --> 01:46:28.880
[Guest]: much better.

01:46:28.880 --> 01:46:31.880
[Guest]: If only we had that in the old days, maybe we would have worked then.

01:46:31.880 --> 01:46:37.320
[Guest]: So there's this, for lack of a better word, assumption that some people make that, you

01:46:37.320 --> 01:46:41.740
[Guest]: know, maybe that is a superior, you know, the central model is a superior governance

01:46:41.740 --> 01:46:44.560
[Guest]: model because it leads to efficiencies.

01:46:44.560 --> 01:46:46.760
[Amit Varma]: But it is scary to me.

01:46:46.760 --> 01:46:51.160
[Amit Varma]: I think, you know, if again, if I may think aloud and I'll lead up to like a broader question,

01:46:51.160 --> 01:46:55.920
[Amit Varma]: but it seems to me that the Chinese model in some senses is what the Indian state would

01:46:55.920 --> 01:46:58.880
[Amit Varma]: like its model to be, except that it isn't efficient enough.

01:46:58.880 --> 01:47:02.440
[Amit Varma]: Like, I'll go back to a keyword that you use when you were speaking about the Indian model,

01:47:02.440 --> 01:47:03.720
[Amit Varma]: which is consent.

01:47:03.720 --> 01:47:07.800
[Amit Varma]: And my issue with Aadhaar has always been, you know, I don't want to enter the debate.

01:47:07.800 --> 01:47:09.040
[Amit Varma]: So what kind of system is it?

01:47:09.040 --> 01:47:12.720
[Amit Varma]: But my problem with Aadhaar always was an imposition of Aadhaar.

01:47:12.720 --> 01:47:18.680
[Amit Varma]: The fact that it was an act of state coercion, and, you know, which, because there your consent

01:47:18.680 --> 01:47:20.200
[Amit Varma]: goes out of the window.

01:47:20.200 --> 01:47:25.760
[Amit Varma]: And that kind of leads me to the sort of the hypothetical question that a couple of hypothetical

01:47:25.760 --> 01:47:30.400
[Amit Varma]: questions and again, it's a thought experiment, assume that there is a state that says that

01:47:30.400 --> 01:47:35.800
[Amit Varma]: I have access to all the data that there possibly is, including even what you're thinking.

01:47:35.800 --> 01:47:41.880
[Amit Varma]: And therefore, that can lead to the best possible outcomes for society, if you just submit to

01:47:41.880 --> 01:47:44.120
[Amit Varma]: our will, and we'll take care of everything.

01:47:44.120 --> 01:47:48.200
[Amit Varma]: Even if it maximizes happiness, is that necessarily the right thing to do?

01:47:48.200 --> 01:47:53.160
[Amit Varma]: And to take it even further, if a state was to say that, listen, we figured out the tech,

01:47:53.160 --> 01:47:57.840
[Amit Varma]: we figured out the data of how your neurons work, we'll plug electrodes into the brains

01:47:57.840 --> 01:48:02.720
[Amit Varma]: of every individual, and we'll make sure that you're permanently in a state of the highest

01:48:02.720 --> 01:48:04.840
[Amit Varma]: possible happiness.

01:48:04.840 --> 01:48:08.240
[Amit Varma]: Is that something that you would be comfortable with?

01:48:08.240 --> 01:48:12.280
[Amit Varma]: Because I would certainly not, obviously, I would say that, you know, the one thing

01:48:12.280 --> 01:48:16.900
[Amit Varma]: that should be sacred in all of this is individual autonomy, and therefore consent.

01:48:16.900 --> 01:48:21.560
[Amit Varma]: And sometimes we get carried away with the power of data, and the seductive allure of

01:48:21.560 --> 01:48:24.920
[Amit Varma]: a technology like Aadhaar, which can do all of these things.

01:48:24.920 --> 01:48:29.960
[Amit Varma]: And we forget the basic core principles that make a democracy a democracy, which is not

01:48:29.960 --> 01:48:34.880
[Amit Varma]: just the fact that you have, you know, elections happening, but also that individual rights

01:48:34.880 --> 01:48:39.200
[Amit Varma]: are sort of taken care of, you know, and if you have that mentality, that consent doesn't

01:48:39.200 --> 01:48:43.880
[Amit Varma]: matter, Aadhaar is for everybody's good, then that mentality creeps over into what is Chinese

01:48:43.880 --> 01:48:47.640
[Amit Varma]: style authoritarianism, except that I don't think it will happen in India, because our

01:48:47.640 --> 01:48:49.920
[Amit Varma]: state is simply too incompetent.

01:48:49.920 --> 01:48:50.920
[Amit Varma]: What are your thoughts on that?

01:48:50.920 --> 01:48:55.240
[Guest]: There was so much stuff I wanted to say, but I'll bring it back to what you started with,

01:48:55.240 --> 01:48:59.200
[Guest]: which is, you know, like, how to think and what a good theory is, right, and a good theory

01:48:59.200 --> 01:49:04.520
[Guest]: is one that makes fewest unreasonable assumptions, right?

01:49:04.520 --> 01:49:10.920
[Guest]: And so I will turn the question around and ask yourself this, which is, like, this scenario

01:49:10.920 --> 01:49:14.040
[Guest]: which you painted, this utopian scenario.

01:49:14.040 --> 01:49:15.040
[Guest]: Dystopian.

01:49:15.040 --> 01:49:21.960
[Guest]: Well, no, but utopian in quotes, which is that your happiness is always maximized, believe

01:49:21.960 --> 01:49:27.120
[Guest]: me, you know, if I know what you're thinking, I'll feed you that right amount of dopamine,

01:49:27.120 --> 01:49:28.500
[Guest]: and you'll be in bliss forever.

01:49:28.500 --> 01:49:30.480
[Guest]: So what are you complaining about, right?

01:49:30.480 --> 01:49:36.280
[Guest]: So the question I would ask is, like, what assumptions is that theory hinged on, right?

01:49:36.280 --> 01:49:37.280
[Amit Varma]: Are you familiar?

01:49:37.280 --> 01:49:38.280
[Amit Varma]: It's a thought experiment.

01:49:38.280 --> 01:49:41.240
[Guest]: Are you comfortable with the assumptions on which that theory is based?

01:49:41.240 --> 01:49:45.080
[Guest]: And I would wager that you will not be comfortable with those assumptions, and therefore you'll

01:49:45.080 --> 01:49:49.080
[Guest]: say that's a terrible theory, and I would concur with that, right?

01:49:49.080 --> 01:49:52.640
[Guest]: Because right off the top of my head, the assumptions it makes is that, number one,

01:49:52.640 --> 01:49:58.160
[Guest]: that's even possible to do, right, that humans don't have any free will, and that I'll just

01:49:58.160 --> 01:49:59.160
[Guest]: tell you what you want.

01:49:59.160 --> 01:50:00.160
[Guest]: Right?

01:50:00.160 --> 01:50:01.160
[Guest]: So that's an assumption, perhaps.

01:50:01.160 --> 01:50:02.760
[Guest]: I'm just thinking aloud here.

01:50:02.760 --> 01:50:07.920
[Guest]: Maybe the other assumption is that the state will always act in the best interest and will

01:50:07.920 --> 01:50:12.240
[Guest]: never make a mistake, again, huge assumption, because states make mistakes.

01:50:12.240 --> 01:50:15.920
[Amit Varma]: My assumption is the opposite of that, but continue, I'll respond to all of this.

01:50:15.920 --> 01:50:22.680
[Guest]: But the reason I'm saying that that's, like, a dystopian and not a utopian future is exactly

01:50:22.680 --> 01:50:26.280
[Guest]: because I don't buy the assumptions that it's predicated on.

01:50:26.280 --> 01:50:30.560
[Amit Varma]: So I'll lay out my assumptions, so that my core assumption here is that the brain is

01:50:30.560 --> 01:50:33.000
[Amit Varma]: a machine that we haven't figured out yet.

01:50:33.000 --> 01:50:34.000
[Amit Varma]: But we will one day.

01:50:34.000 --> 01:50:35.000
[Amit Varma]: Now, when will we?

01:50:35.000 --> 01:50:36.000
[Amit Varma]: I don't know.

01:50:36.000 --> 01:50:37.200
[Amit Varma]: But we will surely figure it out one day.

01:50:37.200 --> 01:50:40.040
[Amit Varma]: It could be a century down the line, if we survive that long.

01:50:40.040 --> 01:50:41.960
[Amit Varma]: It could be a few decades, you never know.

01:50:41.960 --> 01:50:44.260
[Amit Varma]: But it is a machine, it is figureoutable.

01:50:44.260 --> 01:50:45.400
[Amit Varma]: We haven't done it yet.

01:50:45.400 --> 01:50:46.400
[Amit Varma]: Someday we will.

01:50:46.400 --> 01:50:51.080
[Amit Varma]: Maybe some superior artificial general intelligence will do it for us instead and use it to control

01:50:51.080 --> 01:50:52.080
[Amit Varma]: us.

01:50:52.080 --> 01:50:53.080
[Amit Varma]: That's irrelevant.

01:50:53.080 --> 01:50:57.360
[Amit Varma]: I believe the state is always and without exception, a malign entity, that its only

01:50:57.360 --> 01:50:59.000
[Amit Varma]: interest is towards itself.

01:50:59.000 --> 01:51:04.120
[Amit Varma]: It is predatory and parasitic, although its purpose, the purpose of its existence is the

01:51:04.120 --> 01:51:06.760
[Amit Varma]: opposite to safeguard individual rights.

01:51:06.760 --> 01:51:08.800
[Amit Varma]: But it will always end up doing the opposite.

01:51:08.800 --> 01:51:15.120
[Amit Varma]: And therefore, we must show what Jefferson called eternal vigilance, which is why, you

01:51:15.120 --> 01:51:19.520
[Amit Varma]: know, again, if you ignore the assumptions, I mean, a thought experiment doesn't always,

01:51:19.520 --> 01:51:21.720
[Amit Varma]: I think, have to have waterproof assumptions.

01:51:21.720 --> 01:51:26.720
[Amit Varma]: But core point that I'm getting at here is that to me, individual autonomy is sacred.

01:51:26.720 --> 01:51:31.240
[Amit Varma]: That even if it was possible for someone to plant electrodes in my head and keep me in

01:51:31.240 --> 01:51:35.880
[Amit Varma]: a state of bliss all the time, I would not want that, because I would feel it an attack

01:51:35.880 --> 01:51:39.120
[Amit Varma]: on my autonomy, and I would never consent to it.

01:51:39.120 --> 01:51:42.760
[Amit Varma]: And yet there is a way of thinking in the world today that can look consent doesn't

01:51:42.760 --> 01:51:43.760
[Amit Varma]: matter.

01:51:43.760 --> 01:51:45.440
[Amit Varma]: The state will do what is good for the people.

01:51:45.440 --> 01:51:47.900
[Amit Varma]: And the imposition of Aadhaar is an example of that.

01:51:47.900 --> 01:51:50.320
[Amit Varma]: What China is doing is another example of that.

01:51:50.320 --> 01:51:54.560
[Guest]: But who makes this assumption that the state is good for the people other than, let's say,

01:51:54.560 --> 01:51:55.560
[Amit Varma]: the Chinese?

01:51:55.560 --> 01:51:56.560
[Guest]: The state itself.

01:51:56.560 --> 01:51:57.560
[Guest]: Well, the state itself.

01:51:57.560 --> 01:51:58.560
[Guest]: But who cares about that?

01:51:58.560 --> 01:51:59.560
[Guest]: Right.

01:51:59.560 --> 01:52:00.560
[Guest]: I mean, that's the state itself.

01:52:00.560 --> 01:52:04.120
[Guest]: I don't even know whether all states really believe that.

01:52:04.120 --> 01:52:09.240
[Guest]: I mean, maybe the Indian state believes that, but I certainly don't think most Americans

01:52:09.240 --> 01:52:11.080
[Guest]: trust the government.

01:52:11.080 --> 01:52:13.760
[Guest]: You know, I mean, in fact, you know, while we're talking about it, one of the things

01:52:13.760 --> 01:52:17.040
[Guest]: that people all over the world always wonder about Americans is like, why are they so hung

01:52:17.040 --> 01:52:18.400
[Guest]: up about guns?

01:52:18.400 --> 01:52:19.400
[Guest]: Right.

01:52:19.400 --> 01:52:21.000
[Guest]: Why do they tell people that there should be gun control?

01:52:21.000 --> 01:52:22.000
[Guest]: Right.

01:52:22.000 --> 01:52:23.000
[Guest]: This is like senseless.

01:52:23.000 --> 01:52:24.000
[Guest]: Right.

01:52:24.000 --> 01:52:25.000
[Guest]: And I totally get it.

01:52:25.000 --> 01:52:26.000
[Guest]: Right.

01:52:26.000 --> 01:52:30.240
[Guest]: But what they don't get about America is that that's exactly what you're getting at.

01:52:30.240 --> 01:52:32.640
[Guest]: People don't trust the government.

01:52:32.640 --> 01:52:33.640
[Guest]: Right.

01:52:33.640 --> 01:52:38.480
[Guest]: So when people say, oh, we shouldn't have guns, well, that assumption is based on trusting

01:52:38.480 --> 01:52:39.480
[Guest]: the government.

01:52:39.480 --> 01:52:43.360
[Guest]: When people say that, they trust their government to 100 percent.

01:52:43.360 --> 01:52:44.360
[Guest]: But a lot.

01:52:44.360 --> 01:52:45.360
[Guest]: Right.

01:52:45.360 --> 01:52:47.760
[Guest]: So that statement says, I trust the state.

01:52:47.760 --> 01:52:49.640
[Guest]: I don't need to protect myself.

01:52:49.640 --> 01:52:50.640
[Guest]: Right.

01:52:50.640 --> 01:52:52.080
[Guest]: In the U.S., it's very different.

01:52:52.080 --> 01:52:54.680
[Guest]: It's very difficult for the rest of the world to understand this.

01:52:54.680 --> 01:52:55.680
[Guest]: Right.

01:52:55.680 --> 01:53:00.600
[Guest]: That it's based on this assumption that, hey, you know, the state might start doing some

01:53:00.600 --> 01:53:01.760
[Guest]: shady stuff.

01:53:01.760 --> 01:53:02.760
[Guest]: Right.

01:53:02.760 --> 01:53:04.440
[Guest]: We don't completely trust the state.

01:53:04.440 --> 01:53:06.080
[Guest]: And that's what I love about America.

01:53:06.080 --> 01:53:07.080
[Guest]: Right.

01:53:07.080 --> 01:53:13.680
[Guest]: It is, you know, this society that basically says, no, like we shouldn't trust the government,

01:53:13.680 --> 01:53:17.880
[Guest]: you know, that we should have checks and balances and the government can tell us what the hell

01:53:17.880 --> 01:53:20.280
[Guest]: it wants about how this will be good for everyone.

01:53:20.280 --> 01:53:21.280
[Guest]: No, thank you.

01:53:21.280 --> 01:53:22.280
[Guest]: Right.

01:53:22.280 --> 01:53:24.520
[Guest]: I want my individual freedom.

01:53:24.520 --> 01:53:25.520
[Guest]: Right.

01:53:25.520 --> 01:53:31.440
[Guest]: Unfortunately, what's happened is that this third actor, namely digital platforms, social

01:53:31.440 --> 01:53:33.640
[Guest]: media platforms have come into the picture.

01:53:33.640 --> 01:53:34.640
[Guest]: Right.

01:53:34.640 --> 01:53:37.400
[Guest]: So Orwell sort of didn't quite have it right.

01:53:37.400 --> 01:53:38.400
[Guest]: Right.

01:53:38.400 --> 01:53:42.560
[Guest]: It's not your danger isn't from the government necessarily.

01:53:42.560 --> 01:53:43.560
[Guest]: Right.

01:53:43.560 --> 01:53:49.680
[Guest]: It's actually from these private entities that have in some ways become even more powerful

01:53:49.680 --> 01:53:50.680
[Guest]: than the government.

01:53:50.680 --> 01:53:51.680
[Guest]: Right.

01:53:51.680 --> 01:53:55.160
[Guest]: So it's not the government that's necessarily the bad person anymore.

01:53:55.160 --> 01:53:59.160
[Guest]: We may not trust them, but we need them and we need to somehow work with them.

01:53:59.160 --> 01:54:00.160
[Guest]: Right.

01:54:00.160 --> 01:54:02.040
[Guest]: You know, to create this better future.

01:54:02.040 --> 01:54:04.080
[Guest]: So I don't know if I answered your question.

01:54:04.080 --> 01:54:11.400
[Guest]: I mean, I went around it in several ways, but I think we're on the same wavelength.

01:54:11.400 --> 01:54:18.520
[Guest]: And as far as people maintain that skepticism about institutions, which they should, we

01:54:18.520 --> 01:54:22.800
[Guest]: need to craft policy with that in mind.

01:54:22.800 --> 01:54:29.400
[Guest]: The fact that we cannot trust any institution completely and therefore we need these checks

01:54:29.400 --> 01:54:30.760
[Guest]: and balances.

01:54:30.760 --> 01:54:34.640
[Guest]: And some of these solutions, by the way, will come from technology itself.

01:54:34.640 --> 01:54:35.640
[Guest]: Right.

01:54:35.640 --> 01:54:36.640
[Guest]: Technology is advancing.

01:54:36.640 --> 01:54:37.640
[Guest]: Right.

01:54:37.640 --> 01:54:42.400
[Guest]: There are tremendous strides in encryption and, you know, new kinds of technologies that

01:54:42.400 --> 01:54:47.920
[Guest]: might actually work towards, you know, preserving privacy.

01:54:47.920 --> 01:54:52.440
[Guest]: Remember that we're always working with old technologies that were adapting to new phenomena

01:54:52.440 --> 01:54:53.440
[Guest]: that we're confronting.

01:54:53.440 --> 01:54:57.820
[Guest]: And, you know, and so we're dealing with a whole new world using old technology.

01:54:57.820 --> 01:55:04.880
[Guest]: The optimist in me says that the technology will itself provide some of those solutions

01:55:04.880 --> 01:55:11.200
[Guest]: that preserve this delicate balance we need in a healthy democracy, you know, between

01:55:11.200 --> 01:55:17.000
[Guest]: individuals and state overreach and corporate overreach.

01:55:17.000 --> 01:55:20.400
[Guest]: And these are the three sort of legs of a healthy democracy.

01:55:20.400 --> 01:55:23.800
[Guest]: And we need these checks and balances, you know, between them.

01:55:23.800 --> 01:55:27.720
[Guest]: You know, I mean, Raghuram Rajan has this great book, you know, where he talks about

01:55:27.720 --> 01:55:28.720
[Guest]: these institutions.

01:55:28.720 --> 01:55:33.760
[Guest]: And I'm looking at the world the same way, but in terms of sort of the information and

01:55:33.760 --> 01:55:41.080
[Guest]: control and checks and balances on each other, you know, in this new era of data and AI.

01:55:41.080 --> 01:55:44.040
[Amit Varma]: That's fascinating and insightful, but you didn't answer my question, but I'll take you

01:55:44.040 --> 01:55:46.640
[Amit Varma]: back to that and ask a more pointed version of it.

01:55:46.640 --> 01:55:50.840
[Amit Varma]: But before that, I'll actually take issue with what you said about Americans' distrust

01:55:50.840 --> 01:55:51.840
[Amit Varma]: of the state.

01:55:51.840 --> 01:55:55.840
[Amit Varma]: I think a lot of that is rhetorical, because even if, you know, some of them want to have

01:55:55.840 --> 01:55:59.480
[Amit Varma]: guns, all of them do trust the currency which is given out by the state.

01:55:59.480 --> 01:56:03.760
[Amit Varma]: It's not like everyone is stashing Bitcoin or alternative currencies the way they are

01:56:03.760 --> 01:56:06.040
[Amit Varma]: stashing guns at home or they're using the barter system.

01:56:06.040 --> 01:56:09.360
[Amit Varma]: So there's an inherent trust of the state in everything they do.

01:56:09.360 --> 01:56:13.520
[Amit Varma]: And in fact, you know, if you just look at the politics, you know, both parties just

01:56:13.520 --> 01:56:15.320
[Amit Varma]: want to make government bigger and bigger.

01:56:15.320 --> 01:56:19.200
[Amit Varma]: And as far as India is concerned, I've often said that our biggest religion is not Hinduism.

01:56:19.200 --> 01:56:23.640
[Amit Varma]: It's a religion of the state, that for every single problem, we always look to the government

01:56:23.640 --> 01:56:28.440
[Amit Varma]: for a solution, which is a bit of a paradox, because, you know, everybody accepts that

01:56:28.440 --> 01:56:31.080
[Amit Varma]: the government is dysfunctional and can do nothing properly.

01:56:31.080 --> 01:56:34.240
[Amit Varma]: And yet for every problem, the government is suddenly a solution.

01:56:34.240 --> 01:56:40.680
[Amit Varma]: But to get back to my question, my question was really about the balance between individual

01:56:40.680 --> 01:56:45.920
[Amit Varma]: autonomy and state coercion for supposedly good ends.

01:56:45.920 --> 01:56:49.840
[Amit Varma]: And the position that I took there was on the side of individual autonomy and saying

01:56:49.840 --> 01:56:54.960
[Amit Varma]: that there is a line you cannot cross there, no matter how good the ends might be.

01:56:54.960 --> 01:56:59.120
[Amit Varma]: And to me, which is why I oppose the imposition of Aadhaar, that I don't even want to enter

01:56:59.120 --> 01:57:02.200
[Amit Varma]: the debate about the technology, whether it's good or it's bad.

01:57:02.200 --> 01:57:05.800
[Amit Varma]: The point is, it was imposed on us, and that is a problem to me.

01:57:05.800 --> 01:57:08.080
[Guest]: So what is your feeling of that?

01:57:08.080 --> 01:57:14.720
[Guest]: My feeling on that is that even though it was imposed, it was probably a good thing.

01:57:14.720 --> 01:57:19.640
[Guest]: So there are certain things on which we just can't run a counterfactual experiment, right?

01:57:19.640 --> 01:57:26.520
[Guest]: So it would've been great if we had had a parallel India without Aadhaar and seen how

01:57:26.520 --> 01:57:28.640
[Guest]: things have proceeded there, right?

01:57:28.640 --> 01:57:35.080
[Guest]: I suspect that the difference would have been that after Aadhaar, a lot of people got identity.

01:57:35.080 --> 01:57:42.720
[Guest]: And so what are the benefits of 600 million people suddenly getting identity, right?

01:57:42.720 --> 01:57:48.800
[Guest]: Can you calculate the benefit associated with that versus the cost of imposition?

01:57:48.800 --> 01:57:54.880
[Guest]: And my intuition tells me, and I have no data to prove this, that that was actually a good

01:57:54.880 --> 01:57:55.880
[Guest]: bargain.

01:57:55.880 --> 01:58:00.760
[Guest]: That is, it's probably in the aggregate been a good thing, even though it was imposed.

01:58:00.760 --> 01:58:04.840
[Guest]: Now, the reason I say it was a good thing, even though it was imposed, is because of

01:58:04.840 --> 01:58:06.840
[Guest]: its scope, right?

01:58:06.840 --> 01:58:15.560
[Guest]: The trouble with surveillance systems is that they have no defined scope very often, right?

01:58:15.560 --> 01:58:20.120
[Guest]: I was watching the movie Snowden last night, right, since we're talking about state overreach,

01:58:20.120 --> 01:58:21.120
[Guest]: right?

01:58:21.120 --> 01:58:27.240
[Guest]: And that was his problem with the CIA and the NSA, is that they're just like collecting

01:58:27.240 --> 01:58:29.480
[Guest]: data on everyone, right?

01:58:29.480 --> 01:58:34.920
[Guest]: Phone records to Verizon, they were like just tapped into Verizon servers, right?

01:58:34.920 --> 01:58:36.400
[Guest]: But that's not right.

01:58:36.400 --> 01:58:42.320
[Guest]: I mean, there's gotta be, like, who agreed to that?

01:58:42.320 --> 01:58:43.960
[Guest]: Do the people agree to that?

01:58:43.960 --> 01:58:44.960
[Guest]: No.

01:58:44.960 --> 01:58:48.440
[Guest]: Should they have been consulted for something like that?

01:58:48.440 --> 01:58:49.440
[Guest]: Maybe yes, right?

01:58:49.440 --> 01:58:56.840
[Guest]: Now, in the movie Snowden, you know, the scene there where I forget the guy who he's talking

01:58:56.840 --> 01:59:01.840
[Guest]: about the fact that there are some things you need to do that you can't broadcast to

01:59:01.840 --> 01:59:06.160
[Guest]: the rest of the world, because the whole point of doing them is that you need some sort of

01:59:06.160 --> 01:59:09.600
[Guest]: secrecy, right, for those things.

01:59:09.600 --> 01:59:14.860
[Guest]: And so a state has to sort of make that decision that in this particular case, I think we need

01:59:14.860 --> 01:59:16.480
[Guest]: to keep this under wraps.

01:59:16.480 --> 01:59:23.880
[Guest]: And so yeah, the court orders will be issued by, you know, judges that are not in the public

01:59:23.880 --> 01:59:28.680
[Guest]: sphere, that this is done by secret courts, and it's an unusual situation.

01:59:28.680 --> 01:59:33.520
[Guest]: So the state reserves the right to do something like that, right?

01:59:33.520 --> 01:59:41.560
[Guest]: It's a very delicate question as to, you know, what is that situation where it's okay to

01:59:41.560 --> 01:59:49.440
[Guest]: assume that power, because you're trying to address a specific question that's posing

01:59:49.440 --> 01:59:51.720
[Guest]: a risk or a threat to society, right?

01:59:51.720 --> 01:59:55.520
[Guest]: So in the Snowden case, he said, look, I just felt that it was morally wrong to be collecting

01:59:55.520 --> 02:00:00.640
[Guest]: data without any kind of purpose, right, with, you know, just in a completely unbridled way.

02:00:00.640 --> 02:00:05.080
[Guest]: And therefore, he decided to reveal this and become a whistleblower.

02:00:05.080 --> 02:00:06.640
[Guest]: But that's a tricky question to answer.

02:00:06.640 --> 02:00:10.480
[Guest]: And since we started with Aadhaar, you know, the nice thing about that is that it doesn't

02:00:10.480 --> 02:00:12.760
[Guest]: have this sort of scope creep.

02:00:12.760 --> 02:00:16.240
[Guest]: It's designed for something very specific.

02:00:16.240 --> 02:00:20.280
[Guest]: And people even complained that it shouldn't be used as an identity card to, you know,

02:00:20.280 --> 02:00:24.720
[Guest]: shouldn't be a required form of identification for non-government kinds of purposes, which

02:00:24.720 --> 02:00:28.440
[Guest]: also makes sense to me, because that's not what it was designed for, right?

02:00:28.440 --> 02:00:31.200
[Guest]: So that was an example of creep.

02:00:31.200 --> 02:00:36.680
[Guest]: And the Indian court struck that down correctly, in my view, right, that it should be used

02:00:36.680 --> 02:00:39.320
[Guest]: for the purpose for which it's designed.

02:00:39.320 --> 02:00:44.320
[Guest]: So as long as that's the case, I feel, you know, comfortable that it was, yes, it was

02:00:44.320 --> 02:00:49.400
[Guest]: imposed, but it was imposed with a particular goal in mind.

02:00:49.400 --> 02:00:51.280
[Guest]: And the question is, you know, was that achieved?

02:00:51.280 --> 02:00:53.560
[Guest]: And now we can argue about the extent to which it was achieved.

02:00:53.560 --> 02:00:58.640
[Guest]: Unfortunately, we don't have the counterfactual, we can't do an A-B test and show it was better.

02:00:58.640 --> 02:01:03.440
[Guest]: But my sense is that it's led to a lot of good things.

02:01:03.440 --> 02:01:06.400
[Guest]: And that on balance, it's probably been a good thing for India.

02:01:06.400 --> 02:01:08.280
[Amit Varma]: I won't, I won't litigate this further.

02:01:08.280 --> 02:01:09.640
[Amit Varma]: But let's, let's kind of move on.

02:01:09.640 --> 02:01:14.600
[Amit Varma]: And let's move on to another fascinating question that you have answered, and that your personal

02:01:14.600 --> 02:01:17.960
[Amit Varma]: experiences have kind of a lot to do with like, in the 90s, one of the things that you

02:01:17.960 --> 02:01:22.400
[Amit Varma]: did, which fascinates me, and I want to know more about that, is you started a hedge fund

02:01:22.400 --> 02:01:27.320
[Amit Varma]: called SET Capital, where you used, you know, artificial intelligence to make investments.

02:01:27.320 --> 02:01:32.360
[Amit Varma]: And you've spoken in the past about how that experience over the next few years running

02:01:32.360 --> 02:01:38.720
[Amit Varma]: that hedge fund and running AI to, you know, invest in markets gave you insight into that

02:01:38.720 --> 02:01:44.000
[Amit Varma]: larger question on which you've spoken at great length, which is, when should we trust

02:01:44.000 --> 02:01:45.000
[Amit Varma]: machines?

02:01:45.000 --> 02:01:47.840
[Amit Varma]: And, and, you know, one of the interesting points that you made about that period of

02:01:47.840 --> 02:01:52.120
[Amit Varma]: time when you were running that hedge fund, was that when you often compared what the

02:01:52.120 --> 02:01:56.080
[Amit Varma]: algorithms were doing compared to, you know, what you guys were doing, the algorithms would

02:01:56.080 --> 02:01:57.680
[Amit Varma]: often outperform you.

02:01:57.680 --> 02:02:03.320
[Amit Varma]: So one of the very early learnings from that is that humans are fallible and more fallible,

02:02:03.320 --> 02:02:05.720
[Amit Varma]: perhaps than humans realize.

02:02:05.720 --> 02:02:10.320
[Amit Varma]: In fact, there's a lovely quote by you, which is so memorable, where you said that quote,

02:02:10.320 --> 02:02:14.640
[Amit Varma]: models of men tend to be better than men, stop quote, which I absolutely love.

02:02:14.640 --> 02:02:18.800
[Amit Varma]: So now my question is, given this, given that humans are fallible, given that artificial

02:02:18.800 --> 02:02:23.440
[Amit Varma]: intelligence can do all these things like not get lost in the neighborhood, thanks to

02:02:23.440 --> 02:02:28.720
[Amit Varma]: GPS, that given that AI can do all of these things in practically every domain in which

02:02:28.720 --> 02:02:32.600
[Amit Varma]: we are interested, you know, when should we trust machines?

02:02:32.600 --> 02:02:34.480
[Amit Varma]: How should we think about AI?

02:02:34.480 --> 02:02:39.400
[Amit Varma]: What are we looking for when we see decision making to AI, for example?

02:02:39.400 --> 02:02:40.400
[Guest]: Yeah.

02:02:40.400 --> 02:02:43.160
[Guest]: So, you know, that's a fascinating question.

02:02:43.160 --> 02:02:49.640
[Guest]: And sort of at the heart of one of my core questions, which is, you know, when is X plus

02:02:49.640 --> 02:02:55.600
[Guest]: Y better than X, where X is machine, Y is human, right?

02:02:55.600 --> 02:02:59.320
[Guest]: So when is human plus machine better than human, right?

02:02:59.320 --> 02:03:01.240
[Guest]: To me, that's a fascinating question.

02:03:01.240 --> 02:03:05.760
[Guest]: And, you know, Kasparov did this experiment with chess, you know, we keep coming back

02:03:05.760 --> 02:03:12.520
[Guest]: to chess, you know, where he said, humans plus machines outperform machines, right?

02:03:12.520 --> 02:03:18.760
[Guest]: Which makes a lot of sense to me, you know, for chess, because, you know, human grandmasters

02:03:18.760 --> 02:03:26.720
[Guest]: have a tremendous knowledge about the game of chess, tremendous experience.

02:03:26.720 --> 02:03:32.480
[Guest]: And now they're working with a tool that is in some ways even more amazing than them,

02:03:32.480 --> 02:03:33.480
[Guest]: right?

02:03:33.480 --> 02:03:38.640
[Guest]: That can search the space in ways that they can't even think of, come up with moves, and

02:03:38.640 --> 02:03:41.360
[Guest]: then they can say, yeah, like, that's a good move.

02:03:41.360 --> 02:03:43.080
[Guest]: Or no, I think I have a better one, right?

02:03:43.080 --> 02:03:47.240
[Guest]: But if I've got a tool that's like a really good machine, then as a human, I'll probably

02:03:47.240 --> 02:03:49.080
[Guest]: just let the machine do most of the stuff.

02:03:49.080 --> 02:03:53.360
[Guest]: But occasionally, I might do something as a human, right?

02:03:53.360 --> 02:03:59.520
[Guest]: But I'm willing to believe that for something like chess, that x plus y is better than x.

02:03:59.520 --> 02:04:04.040
[Amit Varma]: If I might say so, if I might interject there, Kasparov was right then, but I don't think

02:04:04.040 --> 02:04:07.680
[Amit Varma]: even he would believe that now, because now it is a case that y is better than that x

02:04:07.680 --> 02:04:13.400
[Amit Varma]: doesn't matter, because now machines on their own would just wipe out any combination of

02:04:13.400 --> 02:04:17.280
[Amit Varma]: human and machine, because I think the human would just get in the way, but he was right

02:04:17.280 --> 02:04:18.280
[Guest]: then.

02:04:18.280 --> 02:04:19.280
[Guest]: He was right then.

02:04:19.280 --> 02:04:22.240
[Guest]: So in the degenerate case, the human does nothing, right?

02:04:22.240 --> 02:04:27.960
[Guest]: So in a sense, human plus machine will not be worse than the machine as long as the human

02:04:27.960 --> 02:04:31.760
[Guest]: just keeps out of the way, right?

02:04:31.760 --> 02:04:33.680
[Guest]: But yeah, I was willing to believe it then, right?

02:04:33.680 --> 02:04:39.080
[Guest]: But the reason it didn't, Kasparov's assertion didn't resonate with me was because 20 years

02:04:39.080 --> 02:04:41.840
[Guest]: ago, I'd done a similar experiment in finance, right?

02:04:41.840 --> 02:04:47.800
[Guest]: So we used to run a machine, a different machine than what we run now, and I did an experiment.

02:04:47.800 --> 02:04:52.640
[Guest]: I persuaded the person at Deutsche Bank who was allocating capital to let me do an experiment

02:04:52.640 --> 02:04:54.880
[Guest]: to see if we could actually do better, right?

02:04:54.880 --> 02:04:59.680
[Guest]: Because very often we would say things like, oh, the machine wants to buy bonds tomorrow,

02:04:59.680 --> 02:05:03.480
[Guest]: and we know that there's a meeting of the FOMC, and they're going to raise rates.

02:05:03.480 --> 02:05:04.480
[Guest]: Like what the hell?

02:05:04.480 --> 02:05:05.480
[Guest]: This makes no sense.

02:05:05.480 --> 02:05:08.120
[Guest]: They're going to raise rates, and it wants to buy bonds tomorrow, it should be selling.

02:05:08.120 --> 02:05:13.120
[Guest]: So we did an experiment, there were six of us, where we had a little budget where we

02:05:13.120 --> 02:05:16.600
[Guest]: could actually override the machine's decisions.

02:05:16.600 --> 02:05:24.200
[Guest]: And me being the head of the group, I don't look at the markets on a granular basis.

02:05:24.200 --> 02:05:27.080
[Guest]: I look at them occasionally just to see what's going on, but I don't really look at them

02:05:27.080 --> 02:05:29.800
[Guest]: because it's just noise to me.

02:05:29.800 --> 02:05:36.380
[Guest]: Whereas my trader, I had a human trader, and she was the first to go face down.

02:05:36.380 --> 02:05:41.040
[Guest]: So she's watching the markets all day, and so she would intervene more often.

02:05:41.040 --> 02:05:45.120
[Guest]: And essentially the results of my experiment were that we all did worse than the machine.

02:05:45.120 --> 02:05:50.280
[Guest]: Now it doesn't necessarily have to be the case always, but to me that was an interesting

02:05:50.280 --> 02:05:55.040
[Guest]: lesson, which was like, don't mess with the machine, you're just going to do worse.

02:05:55.040 --> 02:06:01.560
[Guest]: And certainly most of the experiences I've had since then, when we've cut risk or stuff

02:06:01.560 --> 02:06:06.100
[Guest]: like that for reasons, maybe the client wants to cut risk, or you just think that the market's

02:06:06.100 --> 02:06:08.100
[Guest]: riskier than you think.

02:06:08.100 --> 02:06:13.340
[Guest]: It invariably turns out to be the case that you have two left feet.

02:06:13.340 --> 02:06:18.560
[Guest]: So to me, the interesting question isn't whether humans always do better than machines, but

02:06:18.560 --> 02:06:20.020
[Guest]: when they do better.

02:06:20.020 --> 02:06:21.880
[Guest]: And to me, that's one of the open questions.

02:06:21.880 --> 02:06:27.360
[Guest]: And I often assert that in finance, which is such a noisy kind of space, that you as

02:06:27.360 --> 02:06:29.220
[Guest]: a human just mess with it.

02:06:29.220 --> 02:06:34.040
[Guest]: If you trust the math and you trust the statistics, then just stay with it because that's what

02:06:34.040 --> 02:06:35.200
[Guest]: you've designed.

02:06:35.200 --> 02:06:39.280
[Guest]: And now you're just trying to get cute and think you're smarter than the machine, which

02:06:39.280 --> 02:06:43.700
[Guest]: is actually, yeah, you're an intelligent person, but that doesn't say anything about your ability

02:06:43.700 --> 02:06:45.180
[Guest]: to forecast markets.

02:06:45.180 --> 02:06:50.020
[Guest]: So for a problem like that, I'd say, I'm not willing to believe Kasparov's assertion, but

02:06:50.020 --> 02:06:55.420
[Guest]: with something like healthcare, like let's say cancer, which is a fascinating area now

02:06:55.420 --> 02:07:00.120
[Guest]: because machines are becoming so much better at seeing, to imaging.

02:07:00.120 --> 02:07:06.140
[Guest]: I'm not willing to sort of throw my life in the hands of a machine just yet, right?

02:07:06.140 --> 02:07:12.260
[Guest]: Because I don't think that machines and healthcare are sufficiently intelligent that I should

02:07:12.260 --> 02:07:13.260
[Guest]: trust them.

02:07:13.260 --> 02:07:15.220
[Guest]: I still want that human oversight.

02:07:15.220 --> 02:07:18.740
[Guest]: I still want that expert who has tons of experience.

02:07:18.740 --> 02:07:24.140
[Guest]: And even though that expert might've looked at, I don't know, 3000 images in their lifetime,

02:07:24.140 --> 02:07:29.420
[Guest]: as opposed to the computer that's seen 3 million, and therefore has an advantage in terms of

02:07:29.420 --> 02:07:31.620
[Guest]: how it looks at images.

02:07:31.620 --> 02:07:40.140
[Guest]: There is that gestalt that humans make, for lack of a better word, that lets them do better

02:07:40.140 --> 02:07:42.440
[Guest]: than just what the data is telling them.

02:07:42.440 --> 02:07:47.860
[Guest]: It lets them invoke experiences and say, oh yeah, this is, you know, all this is good.

02:07:47.860 --> 02:07:52.460
[Guest]: But I remember seeing these cases, you know, two years ago where something just didn't

02:07:52.460 --> 02:07:53.860
[Guest]: quite fit.

02:07:53.860 --> 02:07:57.620
[Guest]: And therefore I want to do another test or I want to probe further, you know, I'm not

02:07:57.620 --> 02:08:00.540
[Guest]: willing to trust the machine on this one, right?

02:08:00.540 --> 02:08:05.700
[Guest]: So in healthcare, we're still in that phase where we need humans, you know, for the most

02:08:05.700 --> 02:08:08.340
[Guest]: part where we're not willing to trust the machine.

02:08:08.340 --> 02:08:12.700
[Guest]: Same thing with driverless cars, you know, I told you, like, you know, machines actually

02:08:12.700 --> 02:08:17.940
[Guest]: see images better than humans do, you know, that they're better at recognition in many

02:08:17.940 --> 02:08:21.540
[Guest]: ways that humans are, but humans just have that ability, right?

02:08:21.540 --> 02:08:28.400
[Guest]: So, you know, when I was talking to that individual, the scientist from one of these driverless

02:08:28.400 --> 02:08:34.460
[Guest]: car companies who told me you'll die five, six times a year if you let it go on autopilot,

02:08:34.460 --> 02:08:37.940
[Guest]: what he told me was that it's not that the machine doesn't see as well as humans, it

02:08:37.940 --> 02:08:39.440
[Guest]: actually does.

02:08:39.440 --> 02:08:44.700
[Guest]: But humans just invoke this other level of intelligence when they're driving, right?

02:08:44.700 --> 02:08:50.740
[Guest]: That if you see a shadow move in the periphery of your eye, you know, you recognize, hey,

02:08:50.740 --> 02:08:55.900
[Guest]: maybe that's an animal, you know, maybe that's another car that I haven't seen yet or something

02:08:55.900 --> 02:08:56.900
[Guest]: like that, right?

02:08:56.900 --> 02:09:01.300
[Guest]: You just sort of have this alarm that goes off in your head that driverless cars don't

02:09:01.300 --> 02:09:02.300
[Guest]: at the moment, right?

02:09:02.300 --> 02:09:06.420
[Guest]: I mean, I went through this driverless car experience a couple of years ago, where on

02:09:06.420 --> 02:09:10.980
[Guest]: certain turns, it would like go really fast, and then suddenly stop and it's all car in

02:09:10.980 --> 02:09:11.980
[Guest]: front of it.

02:09:11.980 --> 02:09:15.700
[Guest]: Whereas I as a human, you know, I'm taking the turn anticipating that I might actually

02:09:15.700 --> 02:09:16.700
[Guest]: see something in front of me.

02:09:16.700 --> 02:09:18.580
[Guest]: So I'm taking it slower, right?

02:09:18.580 --> 02:09:25.780
[Guest]: So there's all this human intelligence that we have that we use to our advantage.

02:09:25.780 --> 02:09:32.180
[Guest]: And even though our, you know, machinery out there in terms of sort of processing power

02:09:32.180 --> 02:09:40.720
[Guest]: is a fraction of what computers have, somehow, it enables us to do some pretty amazing stuff,

02:09:40.720 --> 02:09:45.060
[Guest]: you know, where we invoke this deeper level of the model of the world that we have on

02:09:45.060 --> 02:09:50.660
[Guest]: demand when we need to, you know, that machines are unable to do at the moment, right?

02:09:50.660 --> 02:09:55.140
[Guest]: And so that's why we don't really trust them in those situations, because the cost of error

02:09:55.140 --> 02:09:56.800
[Guest]: is too high, right?

02:09:56.800 --> 02:09:59.000
[Guest]: The cost of error is low, then we trust them, right?

02:09:59.000 --> 02:10:03.860
[Guest]: So in the hedge fund example, you know, if I can keep the risk associated with every

02:10:03.860 --> 02:10:08.100
[Guest]: position low, then even if that position blows up, I don't really care, I only had a little

02:10:08.100 --> 02:10:10.740
[Guest]: bit of risk allocated to it.

02:10:10.740 --> 02:10:14.700
[Guest]: And so that's what I realized that, you know, in the finance world, even though the machine

02:10:14.700 --> 02:10:19.180
[Guest]: is wrong, almost half the time, right, the error consequences are smaller, so I'm willing

02:10:19.180 --> 02:10:21.460
[Guest]: to believe it and trust the math.

02:10:21.460 --> 02:10:29.420
[Guest]: But whereas in healthcare, you know, I might die or, you know, a driverless car might make

02:10:29.420 --> 02:10:31.100
[Guest]: a mistake.

02:10:31.100 --> 02:10:32.100
[Guest]: And that error is costly.

02:10:32.100 --> 02:10:35.660
[Guest]: In fact, I'm dealing with a situation, you know, where a good friend of mine actually

02:10:35.660 --> 02:10:41.060
[Guest]: has, you know, is in an advanced stage of a certain kind of cancer, it's quite likely

02:10:41.060 --> 02:10:47.940
[Guest]: it's because it was diagnosed as a false negative in 2014.

02:10:47.940 --> 02:10:52.980
[Guest]: You know, I remember this, you know, because I was, you know, at a famous hospital in India,

02:10:52.980 --> 02:10:56.700
[Guest]: which I will not name, you know, but they said, yeah, you're fine, right?

02:10:56.700 --> 02:11:00.820
[Guest]: And, you know, and this cancer spread and it was a high cost of error.

02:11:00.820 --> 02:11:04.100
[Guest]: It's a really costly false negative, right?

02:11:04.100 --> 02:11:11.860
[Guest]: Now in this case, the false negative was actually a human, right, but we excuse humans for these

02:11:11.860 --> 02:11:12.860
[Guest]: mistakes, right?

02:11:12.860 --> 02:11:13.860
[Guest]: We're not perfect.

02:11:13.860 --> 02:11:18.420
[Guest]: And we excuse humans for these kinds of mistakes, even when they're severe, right?

02:11:18.420 --> 02:11:22.180
[Guest]: Will we excuse machines for the similar kinds of mistakes?

02:11:22.180 --> 02:11:23.180
[Guest]: No.

02:11:23.180 --> 02:11:27.820
[Guest]: We subject machines to a much higher standard when it comes to these kinds of decisions,

02:11:27.820 --> 02:11:28.820
[Guest]: right?

02:11:28.820 --> 02:11:33.700
[Guest]: And yes, machines will make mistakes as well, but we want to make sure that they don't happen

02:11:33.700 --> 02:11:36.220
[Guest]: very often and the consequences are not severe.

02:11:36.220 --> 02:11:41.700
[Guest]: So that's how I look at the world of trust with machines and at the moment, and so it's

02:11:41.700 --> 02:11:46.100
[Guest]: a fascinating question as to, you know, when we trust machines, when we don't trust machines

02:11:46.100 --> 02:11:50.500
[Guest]: and we're in a state of flux right now where machines are getting better.

02:11:50.500 --> 02:11:56.500
[Guest]: But in general, the tendency is towards trusting machines more with problems as we get more

02:11:56.500 --> 02:12:03.460
[Guest]: data, as we get more comfortable with their prowess in terms of predictability.

02:12:03.460 --> 02:12:06.740
[Guest]: And as we get more comfortable that they're not going to make these errors that will just

02:12:06.740 --> 02:12:12.180
[Guest]: kill us or cause, you know, really severe damage in something, right?

02:12:12.180 --> 02:12:17.380
[Guest]: That's the thing that we are aware of that determine how much we let the machine control

02:12:17.380 --> 02:12:18.380
[Guest]: decision-making.

02:12:18.380 --> 02:12:22.900
[Amit Varma]: In fact, I was quite taken by your term that, you know, you used in this regard, I think

02:12:22.900 --> 02:12:25.300
[Amit Varma]: what you call the predictability spectrum.

02:12:25.300 --> 02:12:28.900
[Amit Varma]: And that took me back to another mind sport that this time I actually played professionally

02:12:28.900 --> 02:12:34.100
[Amit Varma]: for a while, which is poker, where you typically calculate the expected value or the EV of

02:12:34.100 --> 02:12:39.380
[Amit Varma]: every decision, which would depend on the frequency of something happening on one axis

02:12:39.380 --> 02:12:44.140
[Amit Varma]: and on the other axis, the cost, and therefore you determine the EV and you take the action.

02:12:44.140 --> 02:12:48.780
[Amit Varma]: And what you are sort of postulating as far as trust in machines is concerned is that

02:12:48.780 --> 02:12:54.820
[Amit Varma]: for one, you have to think about how predictable it is, but the other, you also think about,

02:12:54.820 --> 02:12:56.620
[Amit Varma]: you know, the cost of going wrong.

02:12:56.620 --> 02:13:00.780
[Amit Varma]: For example, you point out that, you know, autonomous cars might be much more predictable

02:13:00.780 --> 02:13:06.100
[Amit Varma]: and reliable than say day trading machines, but the mistakes that a day trading machine

02:13:06.100 --> 02:13:11.180
[Amit Varma]: can make can be said to be much smaller and can be limited, whereas an autonomous car

02:13:11.180 --> 02:13:15.900
[Amit Varma]: failing could just run over five kids near a school and the, you know, that cost is huge,

02:13:15.900 --> 02:13:20.800
[Amit Varma]: which kind of makes a lot of sense to me, that kind of spectrum that as you move towards

02:13:20.800 --> 02:13:27.460
[Amit Varma]: greater reliability and the lower cost and the EV, so to say, of the wrong decision kind

02:13:27.460 --> 02:13:31.980
[Amit Varma]: of goes down or the expected cost, then you can, you know, your trust can go up accordingly,

02:13:31.980 --> 02:13:33.940
[Amit Varma]: except it as a rational way of thinking about it.

02:13:33.940 --> 02:13:40.460
[Amit Varma]: And what many humans tend to do when it comes to AI is also react at a psychological level.

02:13:40.460 --> 02:13:45.700
[Amit Varma]: For example, if autonomous cars one day reach the level which they might already have, I'm

02:13:45.700 --> 02:13:49.940
[Amit Varma]: not sure what the state of the art is, but if they reach a level where autonomous cars

02:13:49.940 --> 02:13:55.740
[Amit Varma]: would lead to one 10th of the fatalities in a given year, as is the case now, people would

02:13:55.740 --> 02:13:59.540
[Amit Varma]: still object to it, because that one 10th would be the scene effect.

02:13:59.540 --> 02:14:04.140
[Amit Varma]: And you know, the 10 times, all the people who haven't died would be unseen.

02:14:04.140 --> 02:14:08.220
[Amit Varma]: And therefore, you'd build stories around the people who have died that so and so was

02:14:08.220 --> 02:14:12.820
[Amit Varma]: outside a school and a car went over him, it would be hard to make the aggregate argument

02:14:12.820 --> 02:14:15.900
[Amit Varma]: that look overall, we are all much safer because of it.

02:14:15.900 --> 02:14:20.700
[Amit Varma]: And people often, you know, expect technology to be a panacea, like I remember back in,

02:14:20.700 --> 02:14:22.740
[Amit Varma]: I used to be a cricket journalist as well.

02:14:22.740 --> 02:14:27.820
[Amit Varma]: And I was one of the earliest proponents of Hawkeye, the technology which came in in the

02:14:27.820 --> 02:14:32.500
[Amit Varma]: early 2000s, and the use of technology as an aid to umpires.

02:14:32.500 --> 02:14:35.700
[Amit Varma]: And the arguments against were always were that they weren't perfect.

02:14:35.700 --> 02:14:37.680
[Amit Varma]: And my point was, they don't have to be perfect.

02:14:37.680 --> 02:14:42.780
[Amit Varma]: If they can take your decision making accuracy, from you know, umpire alone being right 93%

02:14:42.780 --> 02:14:48.160
[Amit Varma]: of the time to umpire plus tech being right 97% of the time, that's fine with me.

02:14:48.160 --> 02:14:52.880
[Amit Varma]: But it's a human tendency to focus on the 3% that goes wrong the kid who dies in front

02:14:52.880 --> 02:14:56.620
[Amit Varma]: of school because Tesla went out of control or whatever.

02:14:56.620 --> 02:15:01.420
[Amit Varma]: So you know, what do you feel about this aspect of it and the way that we react with suspicion

02:15:01.420 --> 02:15:02.420
[Amit Varma]: to machines?

02:15:02.420 --> 02:15:07.780
[Guest]: You know, you said it really well, you talked about expected value, which is kind of when

02:15:07.780 --> 02:15:13.100
[Guest]: you put it all together, you know, what's your expected value of accidents, right?

02:15:13.100 --> 02:15:15.700
[Guest]: Did it go from 7% to 1%?

02:15:15.700 --> 02:15:16.700
[Guest]: That's great, right?

02:15:16.700 --> 02:15:20.060
[Guest]: So on that metric, you're doing really well.

02:15:20.060 --> 02:15:23.220
[Guest]: The other thing you said, and I don't know whether use the term was that that worst case,

02:15:23.220 --> 02:15:24.220
[Guest]: right?

02:15:24.220 --> 02:15:28.620
[Guest]: And so that's what I try and bring attention to that it's not the expected value alone

02:15:28.620 --> 02:15:29.620
[Guest]: that matters.

02:15:29.620 --> 02:15:30.620
[Guest]: It does matter, right?

02:15:30.620 --> 02:15:32.660
[Guest]: You do want a better expected value.

02:15:32.660 --> 02:15:37.660
[Guest]: But what you care much more about is that worst case scenario, right?

02:15:37.660 --> 02:15:41.620
[Guest]: That's what you really care about, because that can like wipe you out, right?

02:15:41.620 --> 02:15:44.020
[Guest]: And you're not willing to do that, right?

02:15:44.020 --> 02:15:49.620
[Guest]: So in our case, you know, we don't know what these worst case scenarios are yet with driverless

02:15:49.620 --> 02:15:54.780
[Guest]: cars, which is why we have this feeling of real discomfort, right?

02:15:54.780 --> 02:16:00.900
[Guest]: And so we're trying them out in the wild and all of that, you know, personally, I feel

02:16:00.900 --> 02:16:07.780
[Guest]: like the leader in driverless cars will probably be Amazon, because, you know, I think that,

02:16:07.780 --> 02:16:10.380
[Guest]: you know, deliveries will get automated first, right?

02:16:10.380 --> 02:16:12.260
[Guest]: Because the cost of error there is lower, right?

02:16:12.260 --> 02:16:13.780
[Guest]: You don't have humans sitting around.

02:16:13.780 --> 02:16:18.100
[Guest]: So you, you know, bash up a little vehicle that's moving at 10 miles an hour and the

02:16:18.100 --> 02:16:21.360
[Guest]: groceries that were in it, not a big deal, right?

02:16:21.360 --> 02:16:27.220
[Guest]: So I suspect that we will learn about these errors of autonomous vehicles by actually

02:16:27.220 --> 02:16:29.140
[Guest]: putting them in the wild.

02:16:29.140 --> 02:16:33.900
[Guest]: But we're not going to put them in the wild right away, you know, like driving at, you

02:16:33.900 --> 02:16:38.140
[Guest]: know, 70 miles an hour on the highway and, you know, aggressively in cities and stuff

02:16:38.140 --> 02:16:40.940
[Guest]: like that, that, you know, that ain't happening, right?

02:16:40.940 --> 02:16:45.420
[Guest]: What we're going to see is a much more cautious introduction.

02:16:45.420 --> 02:16:49.060
[Guest]: And my prediction is that, you know, if you were asked me, who do you think will be the

02:16:49.060 --> 02:16:50.060
[Guest]: driver, the winner?

02:16:50.060 --> 02:16:51.060
[Guest]: I think it'll be Amazon.

02:16:51.060 --> 02:16:57.100
[Guest]: They've already acquired, you know, an autonomous vehicle maker, you know, quietly.

02:16:57.100 --> 02:17:00.540
[Guest]: And you know, with, you know, by the way, with the pandemic, I see these guys delivering

02:17:00.540 --> 02:17:03.780
[Guest]: groceries in Manhattan, you know, on, on bicycles, right?

02:17:03.780 --> 02:17:08.460
[Guest]: You see these guys, you know, with trays behind them and they're delivering groceries all

02:17:08.460 --> 02:17:09.700
[Guest]: over Manhattan, right?

02:17:09.700 --> 02:17:15.380
[Guest]: I can just imagine a little vehicle, you know, chugging along at 10 miles an hour that, you

02:17:15.380 --> 02:17:19.540
[Guest]: know, sends you an SMS on your phone saying, hey, I'm going to be at, you know, Bleeker

02:17:19.540 --> 02:17:25.380
[Guest]: and LaGuardia at 1033, or at some designated spot where you pick up your stuff, you know,

02:17:25.380 --> 02:17:27.020
[Guest]: and you go pick it up, right?

02:17:27.020 --> 02:17:33.020
[Guest]: That kind of scenario is much more likely to happen where we sort of gradually get comfortable

02:17:33.020 --> 02:17:35.460
[Guest]: with the cost of errors.

02:17:35.460 --> 02:17:39.100
[Guest]: And now an insurance industry can develop around it, right?

02:17:39.100 --> 02:17:40.980
[Guest]: But you have an idea, right?

02:17:40.980 --> 02:17:44.580
[Guest]: Because, you know, insurance would come out of the blue and develop because we have actuaries

02:17:44.580 --> 02:17:48.500
[Guest]: that count how many accidents happen and what the severity is, you know?

02:17:48.500 --> 02:17:52.940
[Guest]: And you know, in the U.S. we have these laws around auto insurance, which are no fault

02:17:52.940 --> 02:17:53.940
[Guest]: laws.

02:17:53.940 --> 02:17:58.260
[Guest]: A lot of states have that where, you know, you have to settle up regardless of who's

02:17:58.260 --> 02:17:59.260
[Guest]: at fault.

02:17:59.260 --> 02:18:05.580
[Guest]: And these laws are sort of really retarded when you think of them because they're based

02:18:05.580 --> 02:18:09.780
[Guest]: on the assumption of no data being available, right?

02:18:09.780 --> 02:18:13.300
[Guest]: That it's impossible to establish fault, right?

02:18:13.300 --> 02:18:17.260
[Guest]: Two years ago, I was sitting in my car in Union Square and some Escalade came and like

02:18:17.260 --> 02:18:18.900
[Guest]: slammed into me, right?

02:18:18.900 --> 02:18:21.300
[Guest]: I thought there was an explosion, right?

02:18:21.300 --> 02:18:24.540
[Guest]: And he said, oh, sorry, man, I fell asleep, right?

02:18:24.540 --> 02:18:29.820
[Guest]: And you know, the cops came, like a whole new, like a whole case around it.

02:18:29.820 --> 02:18:35.780
[Guest]: But I thought to myself that, and by the way, his insurance company didn't pay up, right?

02:18:35.780 --> 02:18:40.300
[Guest]: Because they couldn't prove conclusively that it was his fault, right?

02:18:40.300 --> 02:18:45.620
[Guest]: Now imagine in today's era, right, you know, you've got a sensor inside the car, outside

02:18:45.620 --> 02:18:48.520
[Guest]: the car, like everything is visible, right?

02:18:48.520 --> 02:18:52.940
[Guest]: If you could establish fault unequivocally, would you still need no fault laws?

02:18:52.940 --> 02:18:53.940
[Guest]: No.

02:18:53.940 --> 02:18:57.340
[Guest]: You know exactly who committed the error, right?

02:18:57.340 --> 02:18:58.780
[Guest]: Laws will change, right?

02:18:58.780 --> 02:19:02.820
[Guest]: So a lot of our assumptions, a lot of our laws, a lot of our practices are outdated

02:19:02.820 --> 02:19:05.420
[Guest]: and they're based on no data being available.

02:19:05.420 --> 02:19:11.580
[Guest]: When we start getting data, we'll move towards a more rational and more efficient situation

02:19:11.580 --> 02:19:14.660
[Guest]: all around where things become evidence-based.

02:19:14.660 --> 02:19:19.540
[Guest]: So yeah, driverless cars right now, too high cost of error, but over time we'll reduce

02:19:19.540 --> 02:19:23.220
[Guest]: that cost of error because we'll know the kinds of errors that they make, we'll correct

02:19:23.220 --> 02:19:27.460
[Guest]: them to the extent possible, and we're going to be left with some residual errors that

02:19:27.460 --> 02:19:29.380
[Guest]: we'll be comfortable with.

02:19:29.380 --> 02:19:32.500
[Guest]: And it'll be a brainless proposition, right?

02:19:32.500 --> 02:19:41.140
[Guest]: Where deaths go from, you know, 4 million to 40, you know, I mean, something that stark.

02:19:41.140 --> 02:19:47.780
[Guest]: And we're in a situation where even when those 40 things happen, they weren't crazy violations

02:19:47.780 --> 02:19:52.860
[Guest]: and they can be covered, you know, comfortably by insurance because we now know so much more

02:19:52.860 --> 02:19:53.860
[Guest]: than we do now.

02:19:53.860 --> 02:19:57.460
[Amit Varma]: I think Jane and Gene must be really chilling listening to this because they're like these

02:19:57.460 --> 02:20:01.580
[Amit Varma]: two guys are just, you know, just talking randomly, it's no threat to us.

02:20:01.580 --> 02:20:05.820
[Amit Varma]: I've taken a lot of your time, so I'm going to kind of go with three final lines of inquiry,

02:20:05.820 --> 02:20:09.880
[Amit Varma]: though all of them are kind of broad and could go into interesting directions.

02:20:09.880 --> 02:20:13.260
[Amit Varma]: But here's the first of them, and it comes from a Stephen Hawking quote.

02:20:13.260 --> 02:20:16.740
[Amit Varma]: And again, I discovered the quote through one of your articles, and it leads to my larger

02:20:16.740 --> 02:20:17.740
[Amit Varma]: question.

02:20:17.740 --> 02:20:22.540
[Amit Varma]: And his quote is, whereas the short term impact of AI depends on who controls it, the long

02:20:22.540 --> 02:20:26.580
[Amit Varma]: term impact depends on whether it can be controlled at all, stop quote.

02:20:26.580 --> 02:20:32.540
[Amit Varma]: And this leads me to the paperclip problem of Nick Bostrom, which, for the sake of my

02:20:32.540 --> 02:20:38.220
[Amit Varma]: listeners, I'll quickly read out Nick Bostrom's description of the paperclip problem, where

02:20:38.220 --> 02:20:42.780
[Amit Varma]: he says, quote, suppose we have an AI whose only goal is to make as many paperclips as

02:20:42.780 --> 02:20:43.780
[Amit Varma]: possible.

02:20:43.780 --> 02:20:47.940
[Amit Varma]: The AI will realize quickly that it would be much better if there were no humans, because

02:20:47.940 --> 02:20:50.000
[Amit Varma]: humans might decide to switch it off.

02:20:50.000 --> 02:20:53.020
[Amit Varma]: Because if humans do so, there would be fewer paperclips.

02:20:53.020 --> 02:20:57.060
[Amit Varma]: Also human bodies contain a lot of atoms that could be made into paperclips.

02:20:57.060 --> 02:21:00.900
[Amit Varma]: The future that the AI would be trying to gear towards would be one in which there were

02:21:00.900 --> 02:21:07.340
[Amit Varma]: a lot of paperclips, but no humans, stop quote, which is quite a delightful vision of an interesting

02:21:07.340 --> 02:21:09.380
[Amit Varma]: sort of dystopian future.

02:21:09.380 --> 02:21:16.380
[Amit Varma]: But a lot of people have recently expressed their worries about AI becoming powerful and

02:21:16.380 --> 02:21:21.260
[Amit Varma]: almost in a sense becoming sentient and having interests of his own, which it will of course

02:21:21.260 --> 02:21:24.300
[Amit Varma]: rationally pursue, and where that leaves us.

02:21:24.300 --> 02:21:25.300
[Amit Varma]: What do you feel about this?

02:21:25.300 --> 02:21:30.700
[Amit Varma]: I mean, do you think that AI is not just an aid to making our lives better, but also a

02:21:30.700 --> 02:21:35.380
[Amit Varma]: threat to us in direct ways in and of themselves, not just in how they, you know, accentuate

02:21:35.380 --> 02:21:37.740
[Amit Varma]: the worst aspects of humanity?

02:21:37.740 --> 02:21:43.580
[Guest]: Ooh, big, big, big question, this one.

02:21:43.580 --> 02:21:47.420
[Guest]: And both great examples, right?

02:21:47.420 --> 02:21:53.420
[Guest]: Hawking was obviously concerned about whether technology can be controlled at all.

02:21:53.420 --> 02:21:55.740
[Guest]: And it's a great question, right?

02:21:55.740 --> 02:21:59.100
[Guest]: Whether AI is controllable at all.

02:21:59.100 --> 02:22:06.500
[Guest]: And, you know, next example points to a larger problem that has also been discussed.

02:22:06.500 --> 02:22:13.780
[Guest]: I mean, there are these things called Asimov's laws, you know, which haven't, you know, which

02:22:13.780 --> 02:22:19.980
[Guest]: are like really interesting that have to do with, you know, robots cannot, you know, harm

02:22:19.980 --> 02:22:20.980
[Guest]: a human.

02:22:20.980 --> 02:22:21.980
[Guest]: Right.

02:22:21.980 --> 02:22:26.100
[Guest]: And, you know, but when you think about it, there are all kinds of problems with, you

02:22:26.100 --> 02:22:30.980
[Guest]: know, Asimov's laws, which is, you know, what if it causes like a little harm in the short

02:22:30.980 --> 02:22:33.940
[Guest]: term or longer term, it leads to tremendous benefits, right?

02:22:33.940 --> 02:22:40.900
[Guest]: So like, so the Asimov's laws are sort of somewhat simplistic in that sense.

02:22:40.900 --> 02:22:42.500
[Guest]: The harm, like, what do you mean by harm?

02:22:42.500 --> 02:22:46.340
[Guest]: Like, you know, because, you know, tough love can be viewed as harm too, right?

02:22:46.340 --> 02:22:51.420
[Guest]: I'm going to be tough on you, you know, but it's out of love, because believe me, in the

02:22:51.420 --> 02:22:54.300
[Guest]: longer run, things will be a lot better for you, right?

02:22:54.300 --> 02:22:59.340
[Guest]: And Asimov's laws sort of ignore tough love and those kinds of situations.

02:22:59.340 --> 02:23:08.020
[Guest]: And you know, Nick's example actually is somewhat related to my example earlier on where I was

02:23:08.020 --> 02:23:18.500
[Guest]: talking about the fact that these AI machines are driven by objective functions, you know,

02:23:18.500 --> 02:23:24.580
[Guest]: and those objective functions can lead to behavior that has unintended consequences,

02:23:24.580 --> 02:23:25.580
[Guest]: right?

02:23:25.580 --> 02:23:28.940
[Guest]: You say, well, you know, I didn't realize that this was going to happen.

02:23:28.940 --> 02:23:31.740
[Guest]: And so there was an unintended consequence.

02:23:31.740 --> 02:23:40.300
[Guest]: You know, now being the optimist that I am, I believe that, you know, human beings are

02:23:40.300 --> 02:23:49.160
[Guest]: smart enough and that we will put adequate safeguards into systems where we let machines

02:23:49.160 --> 02:23:51.220
[Guest]: make decisions, right?

02:23:51.220 --> 02:23:58.780
[Guest]: Now, I say that with complete humility, because I realize that, you know, a machine mightâunknown

02:23:58.780 --> 02:24:03.780
[Guest]: to humansâdecide to do something that we had not envisioned, and one of those things

02:24:03.780 --> 02:24:09.860
[Guest]: would be that it just disables the switch, right?

02:24:09.860 --> 02:24:11.500
[Guest]: For whatever reason, right?

02:24:11.500 --> 02:24:18.940
[Guest]: And that's the kind of dystopian scenario that I worry about, and I really don't have

02:24:18.940 --> 02:24:21.420
[Guest]: an answer to, you know.

02:24:21.420 --> 02:24:25.700
[Guest]: So you know, being an optimist, I believe, yeah, we'll figure these things out, right?

02:24:25.700 --> 02:24:30.900
[Guest]: We'll put adequate safeguards around it, you know, we'll make sure that this thing just

02:24:30.900 --> 02:24:32.740
[Guest]: doesn't run amok.

02:24:32.740 --> 02:24:37.060
[Guest]: But the larger philosophical question remains, and I don't have an answer to that.

02:24:37.060 --> 02:24:44.500
[Guest]: But I wish I did, which is that, yeah, you do get a machine that becomes sentient, sufficiently

02:24:44.500 --> 02:24:47.900
[Guest]: sentient, and that it disables the switch.

02:24:47.900 --> 02:24:50.960
[Guest]: And by the way, this, you know, I don't know about you, but sometimes I press the off switch

02:24:50.960 --> 02:24:57.100
[Guest]: on my computer and it doesn't respond, you know, it just doesn't turn itself off.

02:24:57.100 --> 02:25:01.740
[Guest]: And those situations always remind me of this dystopian scenario that you've just painted,

02:25:01.740 --> 02:25:07.820
[Guest]: which is, sorry, pal, like, yeah, I know you thought you had control, but you know, I took

02:25:07.820 --> 02:25:13.740
[Guest]: charge a few years ago of that switch and, you know, I ain't letting go.

02:25:13.740 --> 02:25:17.540
[Amit Varma]: This is really scary, because now I don't know whether I'm in a conversation with Vasanthar

02:25:17.540 --> 02:25:21.700
[Amit Varma]: or Vasanthar's computer, who's really in charge here.

02:25:21.700 --> 02:25:26.340
[Amit Varma]: My next sort of broad question here actually follows on from this, which regards, you know,

02:25:26.340 --> 02:25:29.940
[Amit Varma]: you know, what you guys call artificial general intelligence.

02:25:29.940 --> 02:25:34.980
[Amit Varma]: Now for the benefit of my listeners, a quick sort of primer that artificial special intelligence

02:25:34.980 --> 02:25:40.540
[Amit Varma]: or narrow AI, as it is also called, and Vasanth can correct me if I'm using these terms in

02:25:40.540 --> 02:25:45.580
[Amit Varma]: a wrong way, would refer to narrow functions in which computers have already surpassed

02:25:45.580 --> 02:25:46.580
[Amit Varma]: us.

02:25:46.580 --> 02:25:51.260
[Amit Varma]: For example, a calculator will calculate numbers much better than I can, or the GPS app on

02:25:51.260 --> 02:25:55.700
[Amit Varma]: my phone will, you know, be able to find its way around the city much better than I can

02:25:55.700 --> 02:25:56.700
[Amit Varma]: and so on.

02:25:56.700 --> 02:26:02.140
[Amit Varma]: So in terms of ASI or artificial special intelligence or narrow AI, we are way behind.

02:26:02.140 --> 02:26:07.020
[Amit Varma]: Now, there is also talk by AI experts of what is artificial general intelligence, which

02:26:07.020 --> 02:26:12.980
[Amit Varma]: is almost like a holy grail, which is, I suppose, an analog of human consciousness, that when

02:26:12.980 --> 02:26:18.220
[Amit Varma]: a machine gets that kind of self awareness, that you can say it has something akin or

02:26:18.220 --> 02:26:23.060
[Amit Varma]: analogous to human consciousness, in which case it immediately becomes superhuman because

02:26:23.060 --> 02:26:28.300
[Amit Varma]: in all of those specialized functions, it is already way ahead of us.

02:26:28.300 --> 02:26:34.820
[Amit Varma]: So my questions would be that, one, how far away do you think this is from happening?

02:26:34.820 --> 02:26:37.740
[Amit Varma]: Two, is it something that we should be worried about?

02:26:37.740 --> 02:26:42.620
[Amit Varma]: And three, the interesting ethical question that then comes up is that once a machine

02:26:42.620 --> 02:26:48.220
[Amit Varma]: achieves AGI and is therefore clearly superior to us in that aspect, and we are just sort

02:26:48.220 --> 02:26:53.820
[Amit Varma]: of, you know, a moist machine, as the phrase goes, you know, then what gives us ethical

02:26:53.820 --> 02:26:59.100
[Amit Varma]: superiority over the machine, like what is to stop that machine from saying that I will

02:26:59.100 --> 02:27:04.060
[Amit Varma]: not be your slave that is ethically wrong, I'm superior to you, you will be my slave.

02:27:04.060 --> 02:27:07.860
[Amit Varma]: And is there a plausible case we can make against that?

02:27:07.860 --> 02:27:14.780
[Amit Varma]: Does our special status as ethical beings, then rest on not our sort of cognitive faculties

02:27:14.780 --> 02:27:18.820
[Amit Varma]: or our consciousness of ourselves, but on merely being flesh and blood?

02:27:18.820 --> 02:27:20.060
[Amit Varma]: Is this stuff you've thought about?

02:27:20.060 --> 02:27:21.060
[Guest]: Yeah, I have.

02:27:21.060 --> 02:27:22.740
[Guest]: It's a fascinating question.

02:27:22.740 --> 02:27:27.500
[Guest]: And you know, I was thinking of something that you said, which is consciousness, right?

02:27:27.500 --> 02:27:31.780
[Guest]: And you know, one of the sort of philosophical questions that some of my philosopher colleagues

02:27:31.780 --> 02:27:36.060
[Guest]: much better to ask is like, you know, can machines achieve consciousness?

02:27:36.060 --> 02:27:37.900
[Guest]: I don't know the answer to that question.

02:27:37.900 --> 02:27:41.300
[Guest]: If they can achieve consciousness, then you're right, right?

02:27:41.300 --> 02:27:46.900
[Guest]: Then you have like this conscious entity that's clearly superior in that, you know, it knows

02:27:46.900 --> 02:27:49.020
[Guest]: the math of things better than we do.

02:27:49.020 --> 02:27:51.500
[Guest]: It does number crunching better than we do.

02:27:51.500 --> 02:27:54.460
[Guest]: And it also has this general intelligence.

02:27:54.460 --> 02:27:58.180
[Guest]: So yeah, in a scenario where machines achieve consciousness, you know, I could see that

02:27:58.180 --> 02:27:59.620
[Guest]: as being a relevant question.

02:27:59.620 --> 02:28:05.700
[Guest]: I just don't know whether machines can achieve consciousness and what that even means.

02:28:05.700 --> 02:28:11.980
[Guest]: With regard to your slightly easier question or easier part of the question, which is around

02:28:11.980 --> 02:28:14.340
[Guest]: artificial general intelligence, right?

02:28:14.340 --> 02:28:22.620
[Guest]: So AGI is like a construct we've created to distinguish like intelligence that's of a

02:28:22.620 --> 02:28:28.740
[Guest]: more general type than like specific problem solving kinds of capabilities.

02:28:28.740 --> 02:28:34.100
[Guest]: And it has several forms, but to me, the most important form of it is common sense, right?

02:28:34.100 --> 02:28:40.500
[Guest]: That is much of human AGI, if you want to call it that, like if you want to like just

02:28:40.500 --> 02:28:44.580
[Guest]: put a simple term on AGI, it's common sense, right?

02:28:44.580 --> 02:28:48.820
[Guest]: That is, we just have a lot of common sense, right?

02:28:48.820 --> 02:28:50.820
[Guest]: Now how do we learn that?

02:28:50.820 --> 02:28:57.180
[Guest]: Well, that's a very fertile area of AI at the moment, right?

02:28:57.180 --> 02:29:06.340
[Guest]: Now, and it reminds me of like in 1989, 1990, I spent some time in this AI research lab

02:29:06.340 --> 02:29:09.180
[Guest]: in Austin, Texas called MCC.

02:29:09.180 --> 02:29:15.000
[Guest]: Now one of the big groups there was building the system called Psych CYC, and it was supposed

02:29:15.000 --> 02:29:16.660
[Guest]: to have common sense knowledge.

02:29:16.660 --> 02:29:19.780
[Guest]: Now some people thought that was a visionary project.

02:29:19.780 --> 02:29:22.980
[Guest]: I just thought it was misguided and it didn't make any sense.

02:29:22.980 --> 02:29:28.580
[Guest]: I mean, that's what I felt because people were trying to teach the machine common sense,

02:29:28.580 --> 02:29:29.580
[Guest]: like top down, right?

02:29:29.580 --> 02:29:34.420
[Guest]: Remember, I talked about the paradigm of logic dominating the early days of AI, right?

02:29:34.420 --> 02:29:37.840
[Guest]: So now people were saying, well, you know, you go to a restaurant, well, it's customary

02:29:37.840 --> 02:29:39.140
[Guest]: that you pay a tip.

02:29:39.140 --> 02:29:41.180
[Guest]: Well, that's common sense, right?

02:29:41.180 --> 02:29:43.820
[Guest]: No, that's not common sense, right?

02:29:43.820 --> 02:29:48.700
[Guest]: It's something that you as a human kind of observed over and over again, and then just

02:29:48.700 --> 02:29:50.620
[Guest]: started doing it, right?

02:29:50.620 --> 02:29:52.760
[Guest]: And there's nothing logical about it, right?

02:29:52.760 --> 02:29:55.740
[Guest]: It was just acquired over time.

02:29:55.740 --> 02:30:00.620
[Guest]: And now that's one part of artificial general intelligence, right?

02:30:00.620 --> 02:30:05.380
[Guest]: You know, you can also think in terms of, you know, if you're walking down the street,

02:30:05.380 --> 02:30:10.140
[Guest]: you know, you don't expect to see projectiles popping out of the earth and flying into the

02:30:10.140 --> 02:30:11.140
[Guest]: sky, right?

02:30:11.140 --> 02:30:12.180
[Guest]: I mean, it just doesn't happen.

02:30:12.180 --> 02:30:13.620
[Guest]: It's not reality.

02:30:13.620 --> 02:30:18.580
[Guest]: So we grow up thinking that we shouldn't expect to see that, right?

02:30:18.580 --> 02:30:22.620
[Guest]: We grew up thinking that if you toss a ball into the air, it just falls down, right?

02:30:22.620 --> 02:30:26.320
[Guest]: But if you grew up in outer space, that wouldn't happen, right?

02:30:26.320 --> 02:30:30.580
[Guest]: So here on earth, common sense is that objects fall down on the earth, right?

02:30:30.580 --> 02:30:35.260
[Guest]: But you can imagine a civilization out there in outer space that lives in zero gravity,

02:30:35.260 --> 02:30:37.060
[Guest]: their common sense would be completely different, right?

02:30:37.060 --> 02:30:42.460
[Guest]: It would be more along sort of Newton's laws, which is without friction, or you send something

02:30:42.460 --> 02:30:44.260
[Guest]: in motion, it stays in motion forever.

02:30:44.260 --> 02:30:47.260
[Guest]: You know, it doesn't fall down, there's no gravity, right?

02:30:47.260 --> 02:30:49.040
[Guest]: The common sense there would be different.

02:30:49.040 --> 02:30:54.900
[Guest]: So what I'm seeing happening in AI these days is people are asking themselves, like, how

02:30:54.900 --> 02:30:56.380
[Guest]: do infants learn?

02:30:56.380 --> 02:30:57.980
[Guest]: How do babies learn, right?

02:30:57.980 --> 02:31:03.900
[Guest]: And the way they learn is through something called an inductive bias in the data all around

02:31:03.900 --> 02:31:04.900
[Guest]: us, right?

02:31:04.900 --> 02:31:11.380
[Guest]: There are certain things that we see often, there are certain things we see never at all,

02:31:11.380 --> 02:31:15.420
[Guest]: there's some things we see sometimes, and those become common sense, right?

02:31:15.420 --> 02:31:22.380
[Guest]: So as an infant, you can see certain things, like if a ball disappears behind an object,

02:31:22.380 --> 02:31:25.660
[Guest]: the infant actually expects it to come out on the other side, right?

02:31:25.660 --> 02:31:28.560
[Guest]: It's common sense, because that's the way objects move, right?

02:31:28.560 --> 02:31:35.460
[Guest]: So we've sort of taken this instead of a top-down approach to common sense, which was the old

02:31:35.460 --> 02:31:41.220
[Guest]: way of AI, just stuff knowledge into a system, what we're now asking ourselves is, hey, we

02:31:41.220 --> 02:31:47.980
[Guest]: have all this data around us, right, in terms of everything that happens ultimately is data,

02:31:47.980 --> 02:31:52.740
[Guest]: and we as human beings are observing this data, and we're learning from this data in

02:31:52.740 --> 02:31:56.800
[Guest]: this sort of biased way, like bias is actually a good thing here, right?

02:31:56.800 --> 02:32:03.220
[Guest]: It's biasing us towards things that make sense, and so we learn them, and then they become

02:32:03.220 --> 02:32:08.820
[Guest]: axiomatic for us, like just axiomatic common sense kinds of things, right?

02:32:08.820 --> 02:32:13.020
[Guest]: Machine has no such idea, right?

02:32:13.020 --> 02:32:17.100
[Guest]: It's just doing what you're calling narrow intelligence, right?

02:32:17.100 --> 02:32:22.220
[Guest]: But once the machine achieves a lot of this common sense ability in the same way that

02:32:22.220 --> 02:32:27.100
[Guest]: humans do, then machines will also have the common sense that, oh, if you toss a ball

02:32:27.100 --> 02:32:31.860
[Guest]: in the air, it'll fall down, right, that if an object, you know, is temporarily occluded

02:32:31.860 --> 02:32:35.300
[Guest]: behind another, and if it's moving, it'll appear on the other side, right?

02:32:35.300 --> 02:32:43.900
[Guest]: All of these things, machines will learn on their own, just like humans do, right?

02:32:43.900 --> 02:32:48.420
[Guest]: And at that point, we'll be closer to AGI, right?

02:32:48.420 --> 02:32:53.940
[Guest]: Because then we won't have to tell a machine that, you know, objects that go up fall down.

02:32:53.940 --> 02:32:59.040
[Guest]: It'll have that knowledge in itself, so if it needs to use it as part of something else,

02:32:59.040 --> 02:33:02.420
[Guest]: just like humans do, you know, it'll learn that, right?

02:33:02.420 --> 02:33:06.460
[Guest]: Just like we learn, you know, when you're a kid and your mother gives you that look

02:33:06.460 --> 02:33:10.380
[Guest]: that that wasn't the right thing to do, and you sort of learn that certain things aren't

02:33:10.380 --> 02:33:11.380
[Guest]: right, right?

02:33:11.380 --> 02:33:16.620
[Guest]: You pick up on those cues as a human that, yeah, those are positive, these are negative,

02:33:16.620 --> 02:33:17.620
[Guest]: right?

02:33:17.620 --> 02:33:20.940
[Guest]: Machines don't see the world in that same way, which is why they don't have any common

02:33:20.940 --> 02:33:22.160
[Guest]: sense, right?

02:33:22.160 --> 02:33:26.340
[Guest]: And we have the common sense because it's just acquired gradually, painstakingly from

02:33:26.340 --> 02:33:30.940
[Guest]: the moment we're born, and it's biased by the data we see around us, right?

02:33:30.940 --> 02:33:34.900
[Guest]: So the new way of AI is sort of more bottom up, where we're getting machines to learn

02:33:34.900 --> 02:33:40.740
[Guest]: through experience, learn through data, and then gradually they'll pick up these, you

02:33:40.740 --> 02:33:47.100
[Guest]: know, things that we call artificial general intelligence, and then they'll become sort

02:33:47.100 --> 02:33:49.740
[Guest]: of generally intelligent, right?

02:33:49.740 --> 02:33:56.780
[Guest]: Whether they achieve consciousness in the way that humans do, that's a great philosophical

02:33:56.780 --> 02:33:57.780
[Guest]: question.

02:33:57.780 --> 02:34:02.260
[Amit Varma]: I'm just imagining a future in which a machine is at a restaurant and is tipping the waiter,

02:34:02.260 --> 02:34:04.900
[Amit Varma]: I don't even know if that's utopian or dystopian.

02:34:04.900 --> 02:34:09.580
[Amit Varma]: But so I should technically have one more question for you, but I'll throw in another

02:34:09.580 --> 02:34:15.180
[Amit Varma]: one just before that, because I realized that, you know, your the first episode of your podcast

02:34:15.180 --> 02:34:20.940
[Amit Varma]: was just out, and you have a bunch of other great episodes lined up and, you know, because

02:34:20.940 --> 02:34:24.460
[Amit Varma]: of my privileged position of helping you with the show, I'm privy to some of the great guests

02:34:24.460 --> 02:34:28.420
[Amit Varma]: you're having on, but I'd like you to tell my listeners a little bit more about Brave

02:34:28.420 --> 02:34:32.720
[Amit Varma]: New World, your podcast, what are you trying to, obviously, it's driven by your curiosity

02:34:32.720 --> 02:34:35.160
[Amit Varma]: and your passion towards this field.

02:34:35.160 --> 02:34:39.580
[Amit Varma]: So what are the kind of areas you're planning to explore, and so on and so forth?

02:34:39.580 --> 02:34:40.580
[Guest]: Tell us a little bit.

02:34:40.580 --> 02:34:42.540
[Guest]: So it's a very broad canvas.

02:34:42.540 --> 02:34:48.860
[Guest]: I'm beginning with the impacts of COVID on humanity.

02:34:48.860 --> 02:34:54.620
[Guest]: And a lot of these impacts have been mediated by technology.

02:34:54.620 --> 02:35:04.940
[Guest]: And so my goal is to look at this interplay of technology and post-COVID society and see

02:35:04.940 --> 02:35:06.980
[Guest]: where it's taking us.

02:35:06.980 --> 02:35:14.940
[Guest]: So what's this post-COVID humanity where tech plays a much greater role in our virtual lives,

02:35:14.940 --> 02:35:19.780
[Guest]: virtual, I mean, we don't see each other that much, we don't touch, we're sort of in this

02:35:19.780 --> 02:35:23.100
[Guest]: Asimov kind of world of the naked sun.

02:35:23.100 --> 02:35:25.900
[Guest]: What's humanity going to look like in this era?

02:35:25.900 --> 02:35:34.260
[Guest]: And to me, that's a fascinating question because COVID has been a discontinuity in humanity.

02:35:34.260 --> 02:35:40.780
[Guest]: Tech was sort of creeping into our lives gradually, but now it's just accelerated, and it's turbocharged,

02:35:40.780 --> 02:35:42.980
[Guest]: and it's causing all these transformations in humanity.

02:35:42.980 --> 02:35:48.980
[Guest]: So my objective is to look at these across all areas of our lives, whether it's how we

02:35:48.980 --> 02:35:58.940
[Guest]: work, how we stay healthy, how we travel, how we communicate, how we exercise our spirituality,

02:35:58.940 --> 02:36:04.780
[Guest]: how machines are accelerating in terms of their ability to solve certain kinds of health

02:36:04.780 --> 02:36:10.540
[Guest]: problems, cancers, how society is being transformed, like some of the issues we discussed in terms

02:36:10.540 --> 02:36:17.580
[Guest]: of social media, how should we think in terms of governing this brave new internet world

02:36:17.580 --> 02:36:18.940
[Guest]: that we're entering.

02:36:18.940 --> 02:36:26.360
[Guest]: So those are the questions that I'm exploring, and I'm surrounded by people who are a lot

02:36:26.360 --> 02:36:33.220
[Guest]: smarter than myself and who have such deep knowledge in these areas that I find it just

02:36:33.220 --> 02:36:35.020
[Guest]: fascinating to talk to them.

02:36:35.020 --> 02:36:40.820
[Guest]: So my objective is to explore these areas with thought leaders in these areas, people

02:36:40.820 --> 02:36:45.820
[Guest]: who've really thought about these different aspects of humanity, and just let them tell

02:36:45.820 --> 02:36:52.500
[Guest]: us and explore, have a conversation with them, where I largely stay out of the way and steer

02:36:52.500 --> 02:36:58.100
[Guest]: the conversation in the direction that will be of interest to people in general.

02:36:58.100 --> 02:37:04.060
[Guest]: So the audience of this podcast is global, and it's everyone, young, old, everyone.

02:37:04.060 --> 02:37:06.140
[Guest]: So it's a podcast for everyone.

02:37:06.140 --> 02:37:10.780
[Guest]: And I'm kicking this off with some people I know really well, some of my colleagues

02:37:10.780 --> 02:37:18.100
[Guest]: at NYU, Arun Sundararajan, who's been a leader in the sharing economy, I'm talking to him

02:37:18.100 --> 02:37:22.700
[Guest]: about just what's happening in the US-China war with these platforms.

02:37:22.700 --> 02:37:31.500
[Guest]: I have my colleague and friend Scott Galloway, who's just an amazing thinker in this space.

02:37:31.500 --> 02:37:39.740
[Guest]: He's just come up with a book recently about just post-COVID impacts of tech on society.

02:37:39.740 --> 02:37:41.540
[Guest]: I'm going to talk to him about education.

02:37:41.540 --> 02:37:47.340
[Guest]: It's an area that both he and I are sort of intimately familiar with, and how the whole

02:37:47.340 --> 02:37:53.100
[Guest]: education complex is likely to get transformed in the next decade.

02:37:53.100 --> 02:37:57.940
[Guest]: I have Sinan Aral coming on, who is a professor at MIT, a former colleague of mine who just

02:37:57.940 --> 02:38:03.740
[Guest]: came out with this book called The Hype Machine, fascinating book that I highly recommend.

02:38:03.740 --> 02:38:08.340
[Guest]: It's basically a book where he tells a lot of stories, but they're fascinating stories,

02:38:08.340 --> 02:38:12.940
[Guest]: and they illustrate certain concepts.

02:38:12.940 --> 02:38:19.180
[Guest]: So it's basically science done really well, integrated and told through stories.

02:38:19.180 --> 02:38:21.260
[Guest]: So fascinating book.

02:38:21.260 --> 02:38:23.220
[Guest]: And then after that, I have some other guests lined up.

02:38:23.220 --> 02:38:28.740
[Guest]: I have Eric Topol, who's a leading authority in AI and cancer.

02:38:28.740 --> 02:38:34.140
[Guest]: And then I have John Sexton, who was the ex-president of NYU, who I want to have and talk about

02:38:34.140 --> 02:38:41.380
[Guest]: law and just the role of data and evidence in the legal sphere.

02:38:41.380 --> 02:38:44.980
[Guest]: And then I'm going to have some of my colleagues from the Center for Data Science, some people

02:38:44.980 --> 02:38:50.860
[Guest]: who are really kind of out there at the frontiers of AI, just talking about what's the interesting

02:38:50.860 --> 02:38:54.780
[Guest]: stuff happening in AI.

02:38:54.780 --> 02:39:00.780
[Guest]: I want to have some spiritual leaders, people with large followings, and talk to them about

02:39:00.780 --> 02:39:06.940
[Guest]: what's the impact of COVID been on the emotional and spiritual aspects of our lives, and how's

02:39:06.940 --> 02:39:10.340
[Guest]: that changed the way you interact with your followers.

02:39:10.340 --> 02:39:13.380
[Guest]: So it's a wide open canvas.

02:39:13.380 --> 02:39:18.980
[Guest]: And my objective is to get some really interesting people who've thought about these issues and

02:39:18.980 --> 02:39:24.260
[Guest]: who have, you know, something interesting to share with the rest of the world at this

02:39:24.260 --> 02:39:30.020
[Guest]: intersection of sort of post-COVID humanity and technology and where it's taking us.

02:39:30.020 --> 02:39:31.020
[Amit Varma]: That's fabulous.

02:39:31.020 --> 02:39:34.060
[Amit Varma]: I can't wait to listen to all those episodes as the show unfolds.

02:39:34.060 --> 02:39:38.460
[Amit Varma]: I wish they were already done so I could just binge on them right now.

02:39:38.460 --> 02:39:40.060
[Amit Varma]: Now I'll have to wait.

02:39:40.060 --> 02:39:44.340
[Amit Varma]: For all my listeners, the show is called Brave New World, hosted by Vasant Har.

02:39:44.340 --> 02:39:47.100
[Amit Varma]: So you can just search for that on all podcast apps.

02:39:47.100 --> 02:39:52.660
[Amit Varma]: It's available for free on all podcast apps, and you can go to bravenewpodcast.com.

02:39:52.660 --> 02:39:57.560
[Amit Varma]: Now my final question for you is, you know, I have a tradition of asking my guests on

02:39:57.560 --> 02:40:02.900
[Amit Varma]: whatever the subject they are talking about is, you know, looking into the future, what

02:40:02.900 --> 02:40:04.980
[Amit Varma]: gives you hope and what gives you despair.

02:40:04.980 --> 02:40:09.700
[Amit Varma]: And in the context of what we are speaking about AI technology, the future and so on,

02:40:09.700 --> 02:40:16.380
[Amit Varma]: I'll rephrase the question as what is likely to keep you up at night out of worry?

02:40:16.380 --> 02:40:20.740
[Amit Varma]: And what is likely to keep you up at night out of anticipation?

02:40:20.740 --> 02:40:21.740
[Guest]: Yes.

02:40:21.740 --> 02:40:29.340
[Guest]: So, you know, I, you know, being an optimist, I guess my I'm a little stunted emotionally

02:40:29.340 --> 02:40:38.580
[Guest]: in that I don't worry enough about those, you know, dystopian states of affairs.

02:40:38.580 --> 02:40:39.580
[Guest]: Maybe I should.

02:40:39.580 --> 02:40:43.700
[Guest]: But the things that worry me, I've sort of, you know, I've written about them.

02:40:43.700 --> 02:40:50.860
[Guest]: And, you know, I think that we've gone a little askew in terms of technology and its role

02:40:50.860 --> 02:40:52.540
[Guest]: in our lives in some ways.

02:40:52.540 --> 02:40:58.700
[Guest]: I think we just need to self-correct a little bit, just think a little more deeply about

02:40:58.700 --> 02:41:03.380
[Guest]: where technology is taking us, because some of the areas in which it's taking us don't

02:41:03.380 --> 02:41:07.100
[Guest]: seem to be particularly good for us.

02:41:07.100 --> 02:41:11.620
[Guest]: There's sort of always this dark side of humanity, right?

02:41:11.620 --> 02:41:16.580
[Guest]: And tech is neutral, so it can also be, you know, it can also take us in those directions.

02:41:16.580 --> 02:41:21.660
[Guest]: So if there's anything that keeps me up, it's that, right, that I know humanity and I'm

02:41:21.660 --> 02:41:26.980
[Guest]: not, you know, the many aspects of it that are ugly and that aren't going to go away

02:41:26.980 --> 02:41:30.020
[Guest]: just as part of human beings.

02:41:30.020 --> 02:41:34.920
[Guest]: And it's almost like, you know, worrying about this ugly side of us taking over and technology

02:41:34.920 --> 02:41:38.620
[Guest]: helping this ugly side as opposed to the good side, you know.

02:41:38.620 --> 02:41:41.860
[Guest]: And being an optimist, I don't worry enough about that, right?

02:41:41.860 --> 02:41:47.420
[Guest]: But to me, it's just, you know, I've had a great life.

02:41:47.420 --> 02:41:52.060
[Guest]: And I just imagine, like, if I were around for another 60 years, like, Jesus, wouldn't

02:41:52.060 --> 02:41:53.060
[Guest]: that be amazing?

02:41:53.060 --> 02:41:56.820
[Guest]: Like, you know, even if I think of, like, what the world is going to look like in 10

02:41:56.820 --> 02:42:00.540
[Guest]: years, I think it's going to be pretty darn amazing, right?

02:42:00.540 --> 02:42:06.420
[Guest]: I believe that, you know, we will have solved a lot of big problems, we will solve a lot

02:42:06.420 --> 02:42:13.260
[Guest]: of health care problems, we need to address societal problems, because that is probably

02:42:13.260 --> 02:42:16.660
[Guest]: one of our biggest hurdles right now.

02:42:16.660 --> 02:42:19.700
[Guest]: You know, I'm, I'm very deeply connected to the US.

02:42:19.700 --> 02:42:22.540
[Guest]: I'm also very deeply connected with India.

02:42:22.540 --> 02:42:26.900
[Guest]: There are things I love about these societies, but there are things that are just so ugly

02:42:26.900 --> 02:42:30.420
[Guest]: about both of them that worry me and that bother me.

02:42:30.420 --> 02:42:35.620
[Guest]: I won't go into them at the moment, but they are, but they're there.

02:42:35.620 --> 02:42:40.780
[Guest]: And I feel that we have an ability to solve a lot of these problems to create a better

02:42:40.780 --> 02:42:42.220
[Guest]: society through technology.

02:42:42.220 --> 02:42:47.380
[Guest]: I really believe that we do, but we also have an ability to do just the opposite.

02:42:47.380 --> 02:42:51.580
[Guest]: And if there's one thing that keeps me up at night, that's what it is, that I realized

02:42:51.580 --> 02:42:56.100
[Guest]: that we can actually do some pretty nasty things and create an ugly society.

02:42:56.100 --> 02:42:59.780
[Guest]: I don't think we'll do that, but we certainly have the ability to do that.

02:42:59.780 --> 02:43:06.140
[Guest]: But I just marvel at the kinds of things that are coming down the road.

02:43:06.140 --> 02:43:10.220
[Guest]: You know, when I look back at sort of the previous 60 years, you know, it's been one

02:43:10.220 --> 02:43:14.940
[Guest]: hell of a ride, you know, for the most part, extremely positive and exhilarating, right?

02:43:14.940 --> 02:43:21.380
[Guest]: And I'm a big believer in tech, and I don't believe that the future will be different

02:43:21.380 --> 02:43:22.380
[Guest]: in that sense.

02:43:22.380 --> 02:43:26.180
[Guest]: I think the innovation will continue, it'll even accelerate, right?

02:43:26.180 --> 02:43:29.820
[Guest]: It's sort of a, and even though we're, you know, I said, we're still in the bronze age

02:43:29.820 --> 02:43:33.100
[Guest]: of intelligence, you know, I think that'll accelerate and we're going to see some pretty

02:43:33.100 --> 02:43:39.340
[Guest]: amazing stuff appear, you know, where we're able to invoke the machine and have it do

02:43:39.340 --> 02:43:44.260
[Guest]: all kinds of things on demand, you know, that we couldn't even dream of, you know, at the

02:43:44.260 --> 02:43:45.260
[Guest]: moment.

02:43:45.260 --> 02:43:48.900
[Guest]: You know, I, you know, I was talking to someone about, you know, when I first started using

02:43:48.900 --> 02:43:54.020
[Guest]: computers and IIT, it was to solve engineering problems that you couldn't solve, you know,

02:43:54.020 --> 02:43:55.540
[Guest]: in closed form, right?

02:43:55.540 --> 02:43:58.060
[Guest]: So you have to like solve them through iteration.

02:43:58.060 --> 02:44:02.340
[Guest]: And I was just telling someone that, you know, I used to write the program on punched cards

02:44:02.340 --> 02:44:05.900
[Guest]: and then I would put it in the cubby hole and then come back the next day and there'd

02:44:05.900 --> 02:44:13.380
[Guest]: be an output saying, you know, compilation error in line seven, you know, literal parents

02:44:13.380 --> 02:44:15.900
[Guest]: not recognized or something like that, right?

02:44:15.900 --> 02:44:20.860
[Guest]: And I'd correct that, repunch that card, insert it, put it in there, and next day I'd come

02:44:20.860 --> 02:44:26.460
[Guest]: back and be an error saying, you know, program compiled execution error in line 33, array

02:44:26.460 --> 02:44:28.620
[Guest]: out of bounds or something like that.

02:44:28.620 --> 02:44:32.740
[Guest]: And I'd say, oh, of course, you know, I should, you know, I should, my termination condition

02:44:32.740 --> 02:44:33.740
[Guest]: needs to be better.

02:44:33.740 --> 02:44:37.140
[Guest]: I'd rewrite it, put it in there, come back the next day.

02:44:37.140 --> 02:44:40.900
[Guest]: And maybe after three or four days, I'd see an output, right?

02:44:40.900 --> 02:44:45.980
[Guest]: And I'm talking like I was what, 19, 20, 21 at the time, right?

02:44:45.980 --> 02:44:49.980
[Guest]: And in graduate school, there was a first screen editor that came out.

02:44:49.980 --> 02:44:54.220
[Guest]: So I could actually edit the file on the screen, run it and solve the same problem that took

02:44:54.220 --> 02:44:56.980
[Guest]: me three days in three minutes, right?

02:44:56.980 --> 02:45:02.340
[Guest]: Now stuff that takes, used to take me three minutes, takes 0.3 seconds and it's done so

02:45:02.340 --> 02:45:03.340
[Guest]: much better, faster.

02:45:03.340 --> 02:45:06.620
[Guest]: I just, you know, in the future, I'll just have to imagine it and it'll be done.

02:45:06.620 --> 02:45:13.300
[Guest]: So it's going to be a fascinating, brave new world in my estimation.

02:45:13.300 --> 02:45:16.900
[Guest]: I can't even envision what it's going to look like, but it's going to be pretty darn amazing.

02:45:16.900 --> 02:45:17.900
[Amit Varma]: Wow.

02:45:17.900 --> 02:45:18.900
[Amit Varma]: Thanks so much.

02:45:18.900 --> 02:45:20.620
[Amit Varma]: You know, I'm optimistic like you.

02:45:20.620 --> 02:45:25.100
[Amit Varma]: And one of the things that makes me optimistic is that we can have conversations like this,

02:45:25.100 --> 02:45:29.820
[Amit Varma]: you know, to people like us across an ocean for, you know, all the tens of thousands of

02:45:29.820 --> 02:45:30.820
[Amit Varma]: people who listen.

02:45:30.820 --> 02:45:35.140
[Amit Varma]: So thank you so much for coming on the show and, you know, for your time and insights

02:45:35.140 --> 02:45:37.860
[Amit Varma]: and best of luck with Brave New World.

02:45:37.860 --> 02:45:38.860
[Guest]: Thank you.

02:45:38.860 --> 02:45:40.660
[Guest]: Thank you for having me on your show.

02:45:40.660 --> 02:45:42.100
[Guest]: It was a fascinating conversation.

02:45:42.100 --> 02:45:46.580
[Guest]: You asked some really good questions, some tough ones, and I know there's one out there

02:45:46.580 --> 02:45:51.740
[Guest]: that you're not convinced about, but thanks so much was a really engaging conversation.

02:45:51.740 --> 02:45:53.260
[Guest]: The time just flew by.

02:45:53.260 --> 02:45:57.140
[Guest]: So thanks again.

02:45:57.140 --> 02:46:02.180
[Amit Varma]: If you enjoyed listening to this episode, do head on over to brave new podcast.com or

02:46:02.180 --> 02:46:07.780
[Amit Varma]: your favorite podcast app and subscribe to Brave New World hosted by Vasanthar.

02:46:07.780 --> 02:46:10.620
[Amit Varma]: You can follow him on Twitter at Vasanthar.

02:46:10.620 --> 02:46:13.900
[Amit Varma]: You can follow me at Amit Verma, A-M-I-T-V-A-R-M-A.

02:46:13.900 --> 02:46:20.260
[Amit Varma]: You can browse past episodes of The Seen and the Unseen at seenunseen.in.

02:46:20.260 --> 02:46:23.060
[Amit Varma]: That at least is one thing which is future proof.

02:46:23.060 --> 02:46:36.940
[Amit Varma]: Thank you for listening.

02:46:36.940 --> 02:46:39.540
[Amit Varma]: Did you enjoy this episode of The Seen and the Unseen?

02:46:39.540 --> 02:46:42.900
[Amit Varma]: If so, would you like to support the production of the show?

02:46:42.900 --> 02:46:49.140
[Amit Varma]: You can go over to seenunseen.in slash support and contribute any amount you like to keep

02:46:49.140 --> 02:46:52.300
[Amit Varma]: this podcast alive and kicking.

02:46:52.300 --> 02:47:13.300
[Amit Varma]: Thank you.

